v
@misc{baayen_celex_1993,
	address = {Philadelphia: Linguistic Data Consortium, University of Pennsylvania.},
	title = {The {CELEX} lexical database [cd-rom]},
	author = {Baayen, R. Harald and Piepenbrock, R and Van Rijn, H},
	year = {1993},
}

@article{brysbaert_moving_2009,
	title = {Moving beyond {Kučera} and {Francis}: {A} critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for {American} {English}},
	volume = {41},
	shorttitle = {Moving beyond {Ku}{\textbackslash}vcera and {Francis}},
	url = {http://www.springerlink.com/index/6557541785743M18.pdf},
	number = {4},
	urldate = {2012-10-04},
	journal = {Behavior Research Methods},
	author = {Brysbaert, M. and New, B.},
	year = {2009},
	pages = {977--990},
}

@book{baayen_analyzing_2008,
	address = {Cambridge},
	title = {Analyzing linguistic data: {A} practical introduction to statistics using {R}},
	isbn = {0-521-70918-0},
	shorttitle = {Analyzing {Linguistic} {Data}},
	publisher = {Cambridge University Press},
	author = {Baayen, R. H.},
	year = {2008},
}

@article{murakami_l1_2016,
	title = {L1 influence on the acquisition order of {English} grammatical morphemes: {A} learner corpus study},
	volume = {38},
	issn = {0272-2631, 1470-1545},
	shorttitle = {L1 {INFLUENCE} {ON} {THE} {ACQUISITION} {ORDER} {OF} {ENGLISH} {GRAMMATICAL} {MORPHEMES}},
	url = {https://www.cambridge.org/core/journals/studies-in-second-language-acquisition/article/l1-influence-on-the-acquisition-order-of-english-grammatical-morphemes/3263C3E82ECA4A7EB19D8F50E45FA1C3},
	doi = {10.1017/S0272263115000352},
	abstract = {We revisit morpheme studies to evaluate the long-standing claim for a universal order of acquisition. We investigate the L2 acquisition order of six English grammatical morphemes by learners from seven L1 groups across five proficiency levels. Data are drawn from approximately 10,000 written exam scripts from the Cambridge Learner Corpus. The study establishes clear L1 influence on the absolute accuracy of morphemes and their acquisition order, therefore challenging the widely held view that there is a universal order of acquisition of L2 morphemes. Moreover, we find that L1 influence is morpheme specific, with morphemes encoding language-specific concepts most vulnerable to L1 influence.},
	language = {en},
	number = {3},
	urldate = {2018-11-27},
	journal = {Studies in Second Language Acquisition},
	author = {Murakami, Akira and Alexopoulou, Theodora},
	month = sep,
	year = {2016},
	pages = {365--401},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\4X5GUIEU\\Murakami and Alexopoulou - 2016 - L1 INFLUENCE ON THE ACQUISITION ORDER OF ENGLISH G.pdf:application/pdf;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\E9VQDFR9\\3263C3E82ECA4A7EB19D8F50E45FA1C3.html:text/html},
}

@article{weiss_analyzing_2021,
	title = {Analyzing the linguistic complexity of {German} learner language in a reading comprehension task: {Using} proficiency classification to investigate short answer data, cross-data generalizability, and the impact of linguistic analysis quality},
	volume = {7},
	issn = {2215-1478, 2215-1486},
	shorttitle = {Analyzing the linguistic complexity of {German} learner language in a reading comprehension task},
	url = {https://www.jbe-platform.com/content/journals/10.1075/ijlcr.20006.wei},
	doi = {10.1075/ijlcr.20006.wei},
	abstract = {Abstract While traditionally linguistic complexity analysis of learner language is mostly based on essays, there is increasing interest in other task types. This is crucial for obtaining a broader empirical basis for characterizing language proficiency and highlights the need to advance our understanding of how task and learner properties interact in shaping the linguistic complexity of learner productions. It also makes it important to determine which complexity measures generalize well across which tasks. In this paper, we investigate the linguistic complexity of answers to reading comprehension questions written by foreign language learners of German at the college level. Analyzing the corpus with computational linguistic methods identifying a wide range of complexity features, we explore which linguistic complexity analyses can successfully be performed for such short answers, how learner proficiency impacts the results, how generalizable they are across different contexts, and how the quality of the underlying analysis impacts the results.},
	language = {en},
	number = {1},
	urldate = {2021-07-05},
	journal = {International Journal of Learner Corpus Research},
	author = {Weiss, Zarah and Meurers, Detmar},
	month = mar,
	year = {2021},
	note = {00000 
Publisher: John Benjamins},
	pages = {83--130},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\YRYGEUPF\\ijlcr.20006.html:text/html},
}

@misc{ludewig_what_2022,
	title = {What {Text} {Features} {Make} {Reading} {Comprehension} {Difficult} {Across} {Elementary} {School}? {Investigating} {Difficulty} and {Changes} in {Difficulty}},
	shorttitle = {What {Text} {Features} {Make} {Reading} {Comprehension} {Difficult} {Across} {Elementary} {School}?},
	url = {https://psyarxiv.com/axkbv/},
	doi = {10.31234/osf.io/axkbv},
	abstract = {Beginning readers benefit in most situations from reading activities that are neither too difficult nor too easy. This study investigated which text features make reading comprehension difficult for third and fourth grade elementary school students. Specifically, 145 multiple-choice items from a reading comprehension test used in several cross-sectional studies (G3: N = 1387; G4: N = 868) and a longitudinal sub-study (N = 195) were analyzed using explanatory item response models to explain item difficulty and changes in item difficulty across grades. A multi-step feature selection procedure controlling for seven task features led to the selection of eight text features from a total of 268 linguistic text features examined. The results showed that lexical and syntactic features and text genre were the most relevant features and that the importance of specific text features changes from third to fourth grade. Expository text were more difficult on average than narrative texts. This difference was only partially explained by lexical and syntactic features in third grade, but almost completely in fourth grade. The results suggest that text features have a dynamic effect on reading comprehension difficulty throughout third to fourth grade; this is especially true of text genre. Our results can help to select more appropriate texts for elementary students and to improve our understanding of the complex interaction between reader, text and activity as it develops over time.},
	language = {en-us},
	urldate = {2023-03-22},
	publisher = {PsyArXiv},
	author = {Ludewig, Ulrich and Trendtel, Matthias and Weiss, Zarah and Meurers, Detmar and McElvany, Nele},
	month = jun,
	year = {2022},
	keywords = {Linguistic complexity, Social and Behavioral Sciences, Educational Psychology, Elementary school, Learner, Reading comprehension, Text genre},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\XANBBENZ\\Ludewig et al. - 2022 - What Text Features Make Reading Comprehension Diff.pdf:application/pdf},
}

@inproceedings{liesenfeld_building_2022,
	address = {Marseille, France},
	title = {Building and curating conversational corpora for diversity-aware language science and technology},
	url = {https://aclanthology.org/2022.lrec-1.126},
	abstract = {We present an analysis pipeline and best practice guidelines for building and curating corpora of everyday conversation in diverse languages. Surveying language documentation corpora and other resources that cover 67 languages and varieties from 28 phyla, we describe the compilation and curation process, specify minimal properties of a unified format for interactional data, and develop methods for quality control that take into account turn-taking and timing. Two case studies show the broad utility of conversational data for (i) charting human interactional infrastructure and (ii) tracing challenges and opportunities for current ASR solutions. Linguistically diverse conversational corpora can provide new insights for the language sciences and stronger empirical foundations for language technology.},
	urldate = {2023-03-23},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Liesenfeld, Andreas and Dingemanse, Mark},
	month = jun,
	year = {2022},
	pages = {1178--1192},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\CPGDHB3Y\\Liesenfeld and Dingemanse - 2022 - Building and curating conversational corpora for d.pdf:application/pdf},
}

@misc{lehner_mehr_2023,
	title = {Mehr als {ChatGPT}: {EU} will {KI} die {Grenzen} aufzeigen},
	shorttitle = {Mehr als {ChatGPT}},
	url = {https://orf.at/stories/3308939/},
	abstract = {Der Chatbot ChatGPT hat künstliche Intelligenz (KI) und ihre Risiken in den Fokus der Öffentlichkeit gerückt. Dabei wird KI längst in vielen Bereichen eingesetzt. Brüssel will der Technologie deshalb Grenzen aufzeigen. Doch die Debatten über ein geplantes Gesetz zur Regulierung der KI wollen nicht abreißen. Während die einen ein Zuviel an Regulierung fürchten, warnen die anderen vor einer zu löchrigen Gesetzgebung.},
	language = {de},
	urldate = {2023-03-23},
	journal = {news.ORF.at},
	author = {Lehner, Katja and Brüssel, aus and {ORF.at}},
	month = mar,
	year = {2023},
}

@article{shiffrin_probing_2023,
	title = {Probing the psychology of {AI} models},
	volume = {120},
	url = {https://www.pnas.org/doi/10.1073/pnas.2300963120},
	doi = {10.1073/pnas.2300963120},
	number = {10},
	urldate = {2023-03-28},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Shiffrin, Richard and Mitchell, Melanie},
	month = mar,
	year = {2023},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2300963120},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\QTYT43WQ\\Shiffrin and Mitchell - 2023 - Probing the psychology of AI models.pdf:application/pdf},
}

@article{binz_using_2023,
	title = {Using cognitive psychology to understand {GPT}-3},
	volume = {120},
	url = {https://www.pnas.org/doi/10.1073/pnas.2218523120},
	doi = {10.1073/pnas.2218523120},
	abstract = {We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3’s decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3’s behavior is impressive: It solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multiarmed bandit task, and shows signatures of model-based reinforcement learning. Yet, we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. Taken together, these results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.},
	number = {6},
	urldate = {2023-03-28},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Binz, Marcel and Schulz, Eric},
	month = feb,
	year = {2023},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2218523120},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\YQG2HT4A\\Binz and Schulz - 2023 - Using cognitive psychology to understand GPT-3.pdf:application/pdf},
}

@misc{piantadosi_modern_2023,
	title = {Modern language models refute {Chomsky}’s approach to language},
	url = {https://lingbuzz.net/lingbuzz/007180},
	abstract = {The rise and success of large language models undermines virtually every strong claim for the innateness of language that has been proposed by generative linguistics. Modern machine learning has subverted and bypassed the entire theoretical framework of Chomsky's approach, including its core claims to particular insights, principles, structures, and processes. I describe the sense in which modern language models implement genuine theories of language, including representations of syntactic and semantic structure. I highlight the relationship between contemporary models and prior approaches in linguistics, namely those based on gradient computations and memorized constructions. I also respond to several critiques of large language models, including claims that they can't answer ``why'' questions, and skepticism that they are informative about real life acquisition. Most notably, large language models have attained remarkable success at discovering grammar without using any of the methods that some in linguistics insisted were necessary for a science of language to progress.},
	urldate = {2023-03-28},
	publisher = {LingBuzz},
	author = {Piantadosi, Steven},
	month = mar,
	year = {2023},
	note = {LingBuzz Published In:},
	keywords = {syntax, computational modeling, cognitive science, chomsky, emergent, generative syntax, large language model, minimalism, statistical learning},
	file = {LingBuzz Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\3CHCU857\\Piantadosi - 2023 - Modern language models refute Chomsky’s approach t.pdf:application/pdf;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\E4KV7QTS\\007180.html:text/html},
}

@article{koch_traceback_2022,
	title = {The traceback method and the early constructicon: theoretical and methodological considerations},
	volume = {18},
	issn = {1613-7035},
	shorttitle = {The traceback method and the early constructicon},
	url = {https://www.degruyter.com/document/doi/10.1515/cllt-2020-0045/html?lang=de},
	doi = {10.1515/cllt-2020-0045},
	abstract = {Usage-based approaches assume that children’s early utterances are item-based. This has been demonstrated in a number of studies using the traceback method. In this approach, a small amount of “target utterances” from a child language corpus is “traced back” to earlier utterances. Drawing on a case study of German, this paper provides a critical evaluation of the method from a usage-based perspective. In particular, we check how factors inherent to corpus data as well as methodological choices influence the results of traceback studies. To this end, we present four case studies in which we change thresholds and the composition of the main corpus, use a cross-corpus approach tracing one child’s utterances back to another child’s corpus, and reverse and randomize the target utterances. Overall, the results show that the method can provide interesting insights—particularly regarding different pathways of language acquisition—but they also show the limitations of the method.},
	language = {en},
	number = {3},
	urldate = {2023-04-17},
	journal = {Corpus Linguistics and Linguistic Theory},
	author = {Koch, Nikolas and Hartmann, Stefan and Quick, Antje Endesfelder},
	month = oct,
	year = {2022},
	note = {Publisher: De Gruyter Mouton},
	keywords = {language acquisition, construction grammar, traceback, usage-based model},
	pages = {477--504},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\BGWX8C8L\\Koch et al. - 2022 - The traceback method and the early constructicon .pdf:application/pdf},
}

@article{behrens_inputoutput_2006,
	title = {The input–output relationship in first language acquisition},
	volume = {21},
	number = {1-3},
	journal = {Language and cognitive processes},
	author = {Behrens, Heike},
	year = {2006},
	note = {Publisher: Taylor \& Francis},
	pages = {2--24},
}

@article{monster_word_2022,
	title = {Word {Properties} {Predicting} {Children}’s {Word} {Recognition}},
	volume = {26},
	issn = {1088-8438},
	url = {https://doi.org/10.1080/10888438.2021.2020795},
	doi = {10.1080/10888438.2021.2020795},
	abstract = {We examined whether word recognition accuracy and latency of words children encounter during primary school across the upper primary school grades can be predicted from word form (word length, mean Levenshtein distance, and mean frequency of neighbors), word meaning (free association network markers) and word exposure (corpus frequency and contextual diversity). As a measure of word recognition, 1454 children (M = 10.1 years, SD = 11.8 months, 52.4\% girls) in grade 3, 4 and 5 of Dutch regular primary schools completed a lexical decision task. Confirmatory factor analyses showed that word characteristics could be reduced to latent constructs of form, meaning, and exposure. Structural equation models indicated that word form and exposure predicted word recognition accuracy, and that word recognition accuracy, word form, and word meaning predicted word recognition latency. The present study provided empirical evidence that word form, word meaning, and word exposure differentially predict word recognition accuracy and latency of words children encounter during primary school across the upper primary grades.},
	number = {5},
	urldate = {2023-04-18},
	journal = {Scientific Studies of Reading},
	author = {Monster, Iris and Tellings, Agnes and Burk, William J. and Keuning, Jos and Segers, Eliane and Verhoeven, Ludo},
	month = sep,
	year = {2022},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/10888438.2021.2020795},
	pages = {373--389},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\RVGKGLSE\\Monster et al. - 2022 - Word Properties Predicting Children’s Word Recogni.pdf:application/pdf},
}

@article{van_den_boer_lexical_2012,
	title = {Lexical decision in children: {Sublexical} processing or lexical search?},
	volume = {65},
	shorttitle = {Lexical decision in children},
	number = {6},
	journal = {Quarterly Journal of Experimental Psychology},
	author = {van den Boer, Madelon and de Jong, Peter F. and Haentjens-van Meeteren, Marleen M.},
	year = {2012},
	note = {Publisher: SAGE Publications Sage UK: London, England},
	pages = {1214--1228},
}

@article{davies_reading_2017,
	title = {Reading through the life span: {Individual} differences in psycholinguistic effects},
	volume = {43},
	issn = {1939-1285},
	shorttitle = {Reading through the life span},
	doi = {10.1037/xlm0000366},
	abstract = {The effects of psycholinguistic variables are critical to the evaluation of theories about the cognitive reading system. However, reading research has tended to focus on the impact of key variables on average performance. We report the first investigation examining variation in psycholinguistic effects across the life span, from childhood into old age. We analyzed the performance of a sample of 535 readers, aged 8–83 years in lexical decision and pronunciation tasks. Our findings show that the effects on reading of two key variables, frequency and AoA, decrease in size with increasing age over the life span. We observed the systematic modulation by age and reading ability of these and other psycholinguistic effects alongside a global U-shaped effect of age. Diffusion model analyses suggest that developmental speed-up in decision responses can be attributed to the increasing quality of evidence accumulation in reaction to words, while the ageing-related slowing can be attributed to decreasing efficiency of stimulus encoding or response execution processes. An analysis of spoken response durations furnishes a consistent picture in which the slowing of pronunciation responses with age can be attributed to slowing articulatory processes. We think our findings can be explained by theoretical accounts that incorporate learning as the basis for the development of structure in the reading system. However, an adequate theory shall have to include assumptions about both developmental learning and later ageing. Our results warrant a life span theory of reading. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
	journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Davies, Rob A. I. and Arnell, Ruth and Birchenough, Julia M. H. and Grimmond, Debbie and Houlson, Sam},
	year = {2017},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Aging, Psycholinguistics, Childhood Development, Life Span, Individual Differences, Reading Development, Word Recognition},
	pages = {1298--1338},
	file = {Accepted Version:C\:\\Users\\Job Schepens\\Zotero\\storage\\TBVLMHSP\\Davies et al. - 2017 - Reading through the life span Individual differen.pdf:application/pdf},
}

@article{tellings_basilex_2014,
	title = {{BasiLex}: {An} 11.5 million words corpus of {Dutch} texts written for children},
	volume = {4},
	shorttitle = {{BasiLex}},
	journal = {Computational Linguistics in the Netherlands Journal},
	author = {Tellings, Agnes and Hulsbosch, Micha and Vermeer, Anne and Van den Bosch, Antal},
	year = {2014},
	pages = {191--208},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\PXG89IYC\\Tellings et al. - BasiLex an 11.5 million words corpus of Dutch tex.pdf:application/pdf;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\HUDSI43P\\50.html:text/html},
}

@misc{pellert_ai_2022,
	title = {{AI} {Psychometrics}: {Using} psychometric inventories to obtain psychological profiles of large language models},
	shorttitle = {{AI} {Psychometrics}},
	url = {https://psyarxiv.com/jv5dt/},
	doi = {10.31234/osf.io/jv5dt},
	abstract = {In this perspective article, we argue that it is possible that systems built on large language models exhibit psychological traits that have so far been studied only in humans. Whereas we do not aim to anthropomorphize artificial intelligence, we argue that because large language models are trained on vast corpora of text that often contain statements about human values, attitudes, beliefs, and personality traits, such models will have learned a set of psychological characteristics that ultimately gives a unique "psychological" makeup to every such model. This psychological makeup can manifest in the model's outputs. Therefore, it should be possible to assess these characteristics by applying psychometric assessments to these models. In a series of demonstrations, we provide various models with psychometric questionnaire items as input and "ask" them to choose an answer as output. Their responses open a pathway to exploring potential biases ingrained in large language models in a rich way, and ultimately may help to avoid the development of large language models that induce harm when deployed in broader societal applications. We conclude by arguing that our investigations give rise to a new interdisciplinary field of research that we would refer to as 'AI Psychometrics'. We propose that AI Psychometrics should focus on tackling the manifold research opportunities and challenges that emerge when deploying psychometric tests to large language models.},
	language = {en-us},
	urldate = {2023-04-25},
	publisher = {PsyArXiv},
	author = {Pellert, Max and Lechner, Clemens and Wagner, Claudia and Rammstedt, Beatrice and Strohmaier, Markus},
	month = dec,
	year = {2022},
	keywords = {Psychology, NLP, Psychometrics, Social and Behavioral Sciences, AI, Gender/Sex Diversity Beliefs, Large Language Models, LLM, Moral Foundations, Natural Language Inference, Natural Language Processing, NLI, other, Personality, Quantitative Methods, Values},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\FKXPLUCR\\Pellert et al. - 2022 - AI Psychometrics Using psychometric inventories t.pdf:application/pdf},
}

@misc{binz_meta-learned_2023,
	title = {Meta-{Learned} {Models} of {Cognition}},
	url = {http://arxiv.org/abs/2304.06729},
	doi = {10.48550/arXiv.2304.06729},
	abstract = {Meta-learning is a framework for learning learning algorithms through repeated interactions with an environment as opposed to designing them by hand. In recent years, this framework has established itself as a promising tool for building models of human cognition. Yet, a coherent research program around meta-learned models of cognition is still missing. The purpose of this article is to synthesize previous work in this field and establish such a research program. We rely on three key pillars to accomplish this goal. We first point out that meta-learning can be used to construct Bayes-optimal learning algorithms. This result not only implies that any behavioral phenomenon that can be explained by a Bayesian model can also be explained by a meta-learned model but also allows us to draw strong connections to the rational analysis of cognition. We then discuss several advantages of the meta-learning framework over traditional Bayesian methods. In particular, we argue that meta-learning can be applied to situations where Bayesian inference is impossible and that it enables us to make rational models of cognition more realistic, either by incorporating limited computational resources or neuroscientific knowledge. Finally, we reexamine prior studies from psychology and neuroscience that have applied meta-learning and put them into the context of these new insights. In summary, our work highlights that meta-learning considerably extends the scope of rational analysis and thereby of cognitive theories more generally.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Binz, Marcel and Dasgupta, Ishita and Jagadish, Akshay and Botvinick, Matthew and Wang, Jane X. and Schulz, Eric},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06729 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\5GR85A3W\\Binz et al. - 2023 - Meta-Learned Models of Cognition.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\ZYCQ6X28\\2304.html:text/html},
}

@misc{park_correct_2023,
	title = {"{Correct} answers" from the psychology of artificial intelligence},
	url = {http://arxiv.org/abs/2302.07267},
	doi = {10.48550/arXiv.2302.07267},
	abstract = {We re-replicate 14 psychology studies from the Many Labs 2 replication project (Klein et al., 2018) with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. Among the eight studies we could analyse, our GPT sample replicated 37.5\% of the original results and 37.5\% of the Many Labs 2 results. We could not analyse the remaining six studies, due to an unexpected phenomenon we call the "correct answer" effect. Different runs of GPT3.5 answered nuanced questions probing political orientation, economic preference, judgement, and moral philosophy with zero or near-zero variation in responses: with the supposedly "correct answer." Most but not all of these "correct answers" were robust to changing the order of answer choices. One exception occurred in the Moral Foundations Theory survey (Graham et al., 2009), for which GPT3.5 almost always identified as a conservative in the original condition (N=1,030, 99.6\%) and as a liberal in the reverse-order condition (N=1,030, 99.3\%). GPT3.5's responses to subsequent questions revealed post-hoc rationalisation; there was a relative bias in the direction of its previously reported political orientation. But both self-reported GPT conservatives and self-reported GPT liberals revealed right-leaning Moral Foundations, although the right-leaning bias of self-reported GPT liberals was weaker. We hypothesise that this pattern was learned from a conservative bias in the model's largely Internet-based training data. Since AI models of the future may be trained on much of the same Internet data as GPT3.5, our results raise concerns that a hypothetical AI-led future may be subject to a diminished diversity of thought.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Park, Peter S. and Schoenegger, Philipp and Zhu, Chongyang},
	month = apr,
	year = {2023},
	note = {arXiv:2302.07267 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, 68T50, Computer Science - Human-Computer Interaction, I.2.7},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\FW3Y23QQ\\Park et al. - 2023 - Correct answers from the psychology of artificia.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\SYRNT83Y\\2302.html:text/html},
}

@misc{zhao_survey_2023,
	title = {A {Survey} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2303.18223},
	doi = {10.48550/arXiv.2303.18223},
	abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	month = apr,
	year = {2023},
	note = {arXiv:2303.18223 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\Y5Y6T8N9\\Zhao et al. - 2023 - A Survey of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\3KZAPXC9\\2303.html:text/html},
}

@misc{chang_language_2023,
	title = {Language {Model} {Behavior}: {A} {Comprehensive} {Survey}},
	shorttitle = {Language {Model} {Behavior}},
	url = {http://arxiv.org/abs/2303.11504},
	doi = {10.48550/arXiv.2303.11504},
	abstract = {Transformer language models have received widespread public attention, yet their generated text is often surprising even to NLP researchers. In this survey, we discuss over 250 recent studies of English language model behavior before task-specific fine-tuning. Language models possess basic capabilities in syntax, semantics, pragmatics, world knowledge, and reasoning, but these capabilities are sensitive to specific inputs and surface features. Despite dramatic increases in generated text quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases. Many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text. We synthesize recent results to highlight what is currently known about what large language models can and cannot do.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Chang, Tyler A. and Bergen, Benjamin K.},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11504 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\G8UIB2AE\\Chang and Bergen - 2023 - Language Model Behavior A Comprehensive Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\WW5UHF9C\\2303.html:text/html},
}

@misc{cai_does_2023,
	title = {Does {ChatGPT} resemble humans in language use?},
	url = {http://arxiv.org/abs/2303.08014},
	doi = {10.48550/arXiv.2303.08014},
	abstract = {Large language models (LLMs) and LLM-driven chatbots such as ChatGPT have shown remarkable capacities in comprehending and producing language. However, their internal workings remain a black box in cognitive terms, and it is unclear whether LLMs and chatbots can develop humanlike characteristics in language use. Cognitive scientists have devised many experiments that probe, and have made great progress in explaining, how people process language. We subjected ChatGPT to 12 of these experiments, pre-registered and with 1,000 runs per experiment. In 10 of them, ChatGPT replicated the human pattern of language use. It associated unfamiliar words with different meanings depending on their forms, continued to access recently encountered meanings of ambiguous words, reused recent sentence structures, reinterpreted implausible sentences that were likely to have been corrupted by noise, glossed over errors, drew reasonable inferences, associated causality with different discourse entities according to verb semantics, and accessed different meanings and retrieved different words depending on the identity of its interlocutor. However, unlike humans, it did not prefer using shorter words to convey less informative content and it did not use context to disambiguate syntactic ambiguities. We discuss how these convergences and divergences may occur in the transformer architecture. Overall, these experiments demonstrate that LLM-driven chatbots like ChatGPT are capable of mimicking human language processing to a great extent, and that they have the potential to provide insights into how people learn and use language.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Cai, Zhenguang G. and Haslett, David A. and Duan, Xufeng and Wang, Shuqi and Pickering, Martin J.},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08014 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\ZM86I5LL\\Cai et al. - 2023 - Does ChatGPT resemble humans in language use.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\4INAV3HN\\2303.html:text/html},
}

@article{barredo_arrieta_explainable_2020,
	title = {Explainable {Artificial} {Intelligence} ({XAI}): {Concepts}, taxonomies, opportunities and challenges toward responsible {AI}},
	volume = {58},
	issn = {1566-2535},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
	doi = {10.1016/j.inffus.2019.12.012},
	abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
	language = {en},
	urldate = {2023-04-27},
	journal = {Information Fusion},
	author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
	month = jun,
	year = {2020},
	keywords = {Accountability, Comprehensibility, Data Fusion, Deep Learning, Explainable Artificial Intelligence, Fairness, Interpretability, Machine Learning, Privacy, Responsible Artificial Intelligence, Transparency},
	pages = {82--115},
	file = {Accepted Version:C\:\\Users\\Job Schepens\\Zotero\\storage\\MZQB7RG8\\Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concept.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\PKMMAGQV\\S1566253519308103.html:text/html},
}

@article{ziems_can_nodate,
	title = {Can large language models transform computational social science?},
	url = {https://arxiv.org/abs/2305.03514},
	author = {Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\K5NLTRSK\\Ziems et al. - Can Large Language Models Transform Computational .pdf:application/pdf},
}

@article{adelman_contextual_2006,
	title = {Contextual {Diversity}, {Not} {Word} {Frequency}, {Determines} {Word}-{Naming} and {Lexical} {Decision} {Times}},
	volume = {17},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1111/j.1467-9280.2006.01787.x},
	doi = {10.1111/j.1467-9280.2006.01787.x},
	abstract = {Word frequency is an important predictor of word-naming and lexical decision times. It is, however, confounded with contextual diversity, the number of contexts in which a word has been seen. In a study using a normative, corpus-based measure of contextual diversity, word-frequency effects were eliminated when effects of contextual diversity were taken into account (but not vice versa) across three naming and three lexical decision data sets; the same pattern of results was obtained regardless of which of three corpora was used to derive the frequency and contextual-diversity values. The results are incompatible with existing models of visual word recognition, which attribute frequency effects directly to frequency, and are particularly problematic for accounts in which frequency effects reflect learning. We argue that the results reflect the importance of likely need in memory processes, and that the continuity between reading and memory suggests using principles from memory research to inform theories of reading.},
	language = {en},
	number = {9},
	urldate = {2023-05-16},
	journal = {Psychological Science},
	author = {Adelman, James S. and Brown, Gordon D.A. and Quesada, José F.},
	month = sep,
	year = {2006},
	pages = {814--823},
	file = {Submitted Version:C\:\\Users\\Job Schepens\\Zotero\\storage\\ZBD6GEY4\\Adelman et al. - 2006 - Contextual Diversity, Not Word Frequency, Determin.pdf:application/pdf},
}

@article{mcdonald_rethinking_2001,
	title = {Rethinking the {Word} {Frequency} {Effect}: {The} {Neglected} {Role} of {Distributional} {Information} in {Lexical} {Processing}},
	volume = {44},
	issn = {0023-8309, 1756-6053},
	shorttitle = {Rethinking the {Word} {Frequency} {Effect}},
	url = {http://journals.sagepub.com/doi/10.1177/00238309010440030101},
	doi = {10.1177/00238309010440030101},
	abstract = {Attempts to quantify lexical variation have produced a large number of theoretical and empirical constructs, such as Word Frequency, Concreteness, and Ambiguity, which have been claimed to predict between-word differences in lexical processing behavior. Models of word recognition that have been developed to account for the effects of these variables have typically lacked adequate semantic representations, and have dealt with words as if they exist in isolation from their environment. We present a new dimension of lexical variation that is addressed to this concern. Contextual Distinctiveness(CD), a corpus-derived summary measure of the frequency distribution of the contexts in which a word occurs, is naturally compatible with contextual theories of semantic representation and meaning. Experiment 1 demonstrates that CD is a significantly better predictor of lexical decision latencies than occurrence frequency, suggesting that CD is the more psychologically relevant variable. We additionally explore the relationship between CD and six subjectively-defined measures: Concreteness, Context Availability, Number of Contexts, Ambiguity, Age of Acquisition and Familiarity and find CD to be reliably related to Ambiguity only. We argue for the priority of immediate context in determining the representation and processing of language.},
	language = {en},
	number = {3},
	urldate = {2023-05-16},
	journal = {Language and Speech},
	author = {McDonald, Scott A. and Shillcock, Richard C.},
	month = sep,
	year = {2001},
	pages = {295--322},
}

@article{hofmann_individual_2020,
	title = {Individual corpora predict fast memory retrieval during reading},
	url = {https://www.semanticscholar.org/paper/Individual-corpora-predict-fast-memory-retrieval-Hofmann-Muller/c03d1820afdbd9cc5208ba3e73329c26958589d3},
	abstract = {The corpus, from which a predictive language model is trained, can be considered the experience of a semantic system. We recorded everyday reading of two participants for two months on a tablet, generating individual corpus samples of 300/500K tokens. Then we trained word2vec models from individual corpora and a 70 million-sentence newspaper corpus to obtain individual and norm-based long-term memory structure. To test whether individual corpora can make better predictions for a cognitive task of long-term memory retrieval, we generated stimulus materials consisting of 134 sentences with uncorrelated individual and norm-based word probabilities. For the subsequent eye tracking study 1-2 months later, our regression analyses revealed that individual, but not norm-corpus-based word probabilities can account for first-fixation duration and first-pass gaze duration. Word length additionally affected gaze duration and total viewing duration. The results suggest that corpora representative for an individual’s long-term memory structure can better explain reading performance than a norm corpus, and that recently acquired information is lexically accessed rapidly.},
	urldate = {2023-05-16},
	journal = {ArXiv},
	author = {Hofmann, M. and Muller, Lara and Rolke, A. and Radach, R. and Biemann, Chris},
	month = oct,
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\TLU9DB87\\Hofmann et al. - 2020 - Individual corpora predict fast memory retrieval d.pdf:application/pdf},
}

@article{hofmann_language_2022,
	title = {Language {Models} {Explain} {Word} {Reading} {Times} {Better} {Than} {Empirical} {Predictability}},
	volume = {4},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/articles/10.3389/frai.2021.730570/full},
	doi = {10.3389/frai.2021.730570},
	abstract = {Though there is a strong consensus that word length and frequency are the most important single-word features determining visual-orthographic access to the mental lexicon, there is less agreement as how to best capture syntactic and semantic factors. The traditional approach in cognitive reading research assumes that word predictability from sentence context is best captured by cloze completion probability (CCP) derived from human performance data. We review recent research suggesting that probabilistic language models provide deeper explanations for syntactic and semantic effects than CCP. Then we compare CCP with three probabilistic language models for predicting word viewing times in an English and a German eye tracking sample: (1) Symbolic n-gram models consolidate syntactic and semantic short-range relations by computing the probability of a word to occur, given two preceding words. (2) Topic models rely on subsymbolic representations to capture long-range semantic similarity by word co-occurrence counts in documents. (3) In recurrent neural networks (RNNs), the subsymbolic units are trained to predict the next word, given all preceding words in the sentences. To examine lexical retrieval, these models were used to predict single fixation durations and gaze durations to capture rapidly successful and standard lexical access, and total viewing time to capture late semantic integration. The linear item-level analyses showed greater correlations of all language models with all eye-movement measures than CCP. Then we examined non-linear relations between the different types of predictability and the reading times using generalized additive models. N-gram and RNN probabilities of the present word more consistently predicted reading performance compared with topic models or CCP. For the effects of last-word probability on current-word viewing times, we obtained the best results with n-gram models. Such count-based models seem to best capture short-range access that is still underway when the eyes move on to the subsequent word. The prediction-trained RNN models, in contrast, better predicted early preprocessing of the next word. In sum, our results demonstrate that the different language models account for differential cognitive processes during reading. We discuss these algorithmically concrete blueprints of lexical consolidation as theoretically deep explanations for human reading.},
	urldate = {2023-05-16},
	journal = {Frontiers in Artificial Intelligence},
	author = {Hofmann, Markus J. and Remus, Steffen and Biemann, Chris and Radach, Ralph and Kuchinke, Lars},
	month = feb,
	year = {2022},
	pages = {730570},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\W7QNG7HE\\Hofmann et al. - 2022 - Language Models Explain Word Reading Times Better .pdf:application/pdf},
}

@article{baayen_demythologizing_2010,
	title = {Demythologizing the word frequency effect: {A} discriminative learning perspective},
	volume = {5},
	issn = {1871-1340, 1871-1375},
	shorttitle = {Demythologizing the word frequency effect},
	url = {http://www.jbe-platform.com/content/journals/10.1075/ml.5.3.10baa},
	doi = {10.1075/ml.5.3.10baa},
	abstract = {This study starts from the hypothesis, first advanced by McDonald and Shillcock (2001), that the word frequency effect for a large part reflects local syntactic co-occurrence. It is shown that indeed the word frequency effect in the sense of pure repeated exposure accounts for only a small proportion of the variance in lexical decision, and that local syntactic and morphological co-occurrence probabilities are what makes word frequency a powerful predictor for lexical decision latencies. A comparison of two computational models, the cascaded dual route model (Coltheart, Rastle, Perry, Langdon, \& Ziegler, 2001) and the Naive Discriminative Reader (Baayen, Milin, Filipovic Durdjevic, Hendrix, \& Marelli, 2010), indicates that only the latter model properly captures the quantitative weight of the latent dimensions of lexical variation as predictors of response times. Computational models that account for frequency of occurrence by some mechanism equivalent to a counter in the head therefore run the risk of overestimating the role of frequency as repetition, of overestimating the importance of words’ form properties, and of underestimating the importance of contextual learning during past experience in proficient reading.},
	language = {en},
	number = {3},
	urldate = {2023-05-16},
	journal = {The Mental Lexicon},
	author = {Baayen, R. H.},
	month = dec,
	year = {2010},
	pages = {436--461},
}

@article{brysbaert_word_2018,
	title = {The word frequency effect in word processing: {An} updated review},
	volume = {27},
	shorttitle = {The word frequency effect in word processing},
	number = {1},
	journal = {Current Directions in Psychological Science},
	author = {Brysbaert, Marc and Mandera, Pawe{\textbackslash}l and Keuleers, Emmanuel},
	year = {2018},
	note = {Publisher: Sage Publications Sage CA: Los Angeles, CA},
	pages = {45--50},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\KG6RDQQD\\Brysbaert et al. - 2018 - The word frequency effect in word processing An u.pdf:application/pdf},
}

@article{keuleers_british_2010,
	title = {The {British} {Lexicon} {Project}: {Lexical} decision data for 28,730 monosyllabic and disyllabic {English} words},
	volume = {1},
	shorttitle = {The {British} {Lexicon} {Project}},
	journal = {Psychology},
	author = {Keuleers, Emmanuel and Lacey, Paula and Rastle, Kathleen and Brysbaert, Marc},
	year = {2010},
	note = {Publisher: Citeseer},
	pages = {174},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\CEUJA8DZ\\Keuleers et al. - 2010 - The British Lexicon Project Lexical decision data.pdf:application/pdf},
}

@article{ferrand_french_2010,
	title = {The {French} {Lexicon} {Project}: {Lexical} decision data for 38,840 {French} words and 38,840 pseudowords},
	volume = {42},
	shorttitle = {The {French} {Lexicon} {Project}},
	journal = {Behavior research methods},
	author = {Ferrand, Ludovic and New, Boris and Brysbaert, Marc and Keuleers, Emmanuel and Bonin, Patrick and Méot, Alain and Augustinova, Maria and Pallier, Christophe},
	year = {2010},
	note = {Publisher: Springer},
	pages = {488--496},
}

@article{van_heuven_subtlex-uk_2014,
	title = {{SUBTLEX}-{UK}: {A} new and improved word frequency database for {British} {English}},
	volume = {67},
	shorttitle = {{SUBTLEX}-{UK}},
	number = {6},
	journal = {Quarterly journal of experimental psychology},
	author = {Van Heuven, Walter JB and Mandera, Pawel and Keuleers, Emmanuel and Brysbaert, Marc},
	year = {2014},
	note = {Publisher: SAGE Publications Sage UK: London, England},
	pages = {1176--1190},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\PRC4EAAE\\Van Heuven et al. - 2014 - SUBTLEX-UK A new and improved word frequency data.pdf:application/pdf},
}

@misc{wild_ratte_nodate,
	title = {Ratte. {Regensburger} {Analysetool} für {Texte}. {Version} 2.0.},
	url = {https://www.uni-regensburg.de/sprache-literatur-kultur/germanistik-did/downloads/ratte/index.html},
	urldate = {2023-05-05},
	author = {Wild, J and Pissarek, M},
	file = {Ratte - Universität Regensburg:C\:\\Users\\Job Schepens\\Zotero\\storage\\49ZDE5UW\\index.html:text/html},
}

@article{hasenacker_comparing_2019,
	title = {Comparing length and frequency effects in children across modalities},
	volume = {72},
	issn = {1747-0218},
	url = {https://doi.org/10.1177/1747021818805063},
	doi = {10.1177/1747021818805063},
	abstract = {Although it is well established that beginning readers rely heavily on phonological decoding, the overlap of the phonological pathways used in visual and auditory word recognition is not clear. Especially in transparent languages, phonological reading could use the same pathways as spoken word processing. In the present study, we report a direct comparison of lexical decision performance in the visual and auditory modality in beginning readers of a transparent language. Using lexical decision, we examine how marker effects of length and frequency differ in the two modalities and how these differences are modulated by reading ability. The results show that both frequency and length effects are stronger in the visual modality, and the differences in length effects between modalities are more pronounced for poorer readers than for better readers. This suggests that visual word recognition in beginning readers of a transparent language initially is based on phonological decoding and subsequent matching in the phonological lexicon, especially for poor readers. However, some orthographic processing seems to be involved already. We claim that the relative contribution of the phonological and orthographic route in beginning readers can be measured by the differences in marker effects between auditory and visual lexical decision.},
	language = {en},
	number = {7},
	urldate = {2023-05-04},
	journal = {Quarterly Journal of Experimental Psychology},
	author = {Hasenäcker, Jana and Verra, Luianta and Schroeder, Sascha},
	month = jul,
	year = {2019},
	note = {Publisher: SAGE Publications},
	pages = {1682--1691},
}

@article{silge_tidytext_2016,
	title = {tidytext: {Text} {Mining} and {Analysis} {Using} {Tidy} {Data} {Principles} in {R}},
	volume = {1},
	url = {http://dx.doi.org/10.21105/joss.00037},
	doi = {10.21105/joss.00037},
	number = {3},
	journal = {JOSS},
	author = {Silge, Julia and Robinson, David},
	year = {2016},
	note = {Publisher: The Open Journal},
}

@inproceedings{evert_zipfr_2007,
	title = {{zipfR}: {Word} frequency distributions in {R}},
	shorttitle = {{zipfR}},
	booktitle = {Proceedings of the 45th annual meeting of the {ACL} on interactive poster and demonstration sessions},
	author = {Evert, Stefan and Baroni, Marco},
	year = {2007},
	pages = {29--32},
}

@book{wild_learning_2016,
	address = {Cham},
	title = {Learning {Analytics} in {R} with {SNA}, {LSA}, and {MPIA}},
	isbn = {978-3-319-28789-8 978-3-319-28791-1},
	url = {http://link.springer.com/10.1007/978-3-319-28791-1},
	language = {en},
	urldate = {2023-07-06},
	publisher = {Springer International Publishing},
	author = {Wild, Fridolin},
	year = {2016},
	doi = {10.1007/978-3-319-28791-1},
	keywords = {Learning, Latent Semantic Analysis, Learning Analytics, Linear Algebra, Social Network Analysis},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\CEZC7HLL\\Wild - 2016 - Learning Analytics in R with SNA, LSA, and MPIA.pdf:application/pdf},
}

@misc{noauthor_ud_german-gsd_nodate,
	title = {{UD}\_German-{GSD}},
	url = {https://universaldependencies.org/treebanks/de_gsd/index.html},
	urldate = {2023-07-06},
	file = {UD_German-GSD:C\:\\Users\\Job Schepens\\Zotero\\storage\\5IWC7EBH\\index.html:text/html},
}

@article{nivre_universal_2017,
	title = {Universal {Dependencies} 2.0},
	copyright = {Licence Universal Dependencies v2.0},
	url = {https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1983},
	abstract = {Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). 
 
This release is special in that the treebanks will be used as training/development data in the CoNLL 2017 shared task (http://universaldependencies.org/conll17/). Test data are not released, except for the few treebanks that do not take part in the shared task. 64 treebanks will be in the shared task, and they correspond to the following 45 languages: Ancient Greek, Arabic, Basque, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Gothic, Greek, Hebrew, Hindi, Hungarian, Indonesian, Irish, Italian, Japanese, Kazakh, Korean, Latin, Latvian, Norwegian, Old Church Slavonic, Persian, Polish, Portuguese, Romanian, Russian, Slovak, Slovenian, Spanish, Swedish, Turkish, Ukrainian, Urdu, Uyghur and Vietnamese. 
 
This release fixes a bug in http://hdl.handle.net/11234/1-1976. Changed files: ud-tools-v2.0.tgz (conllu\_to\_text.pl, conllu\_to\_conllx.pl; added text\_without\_spaces.pl), ud-treebanks-conll2017.tgz (fi\_ftb-ud-train.txt, he-ud-train.txt, it-ud-train.txt, pt\_br-ud-train.txt, es-ud-train.txt) and ud-treebanks-v2.0.tgz (fi\_ftb-ud-train.txt, he-ud-train.txt, it-ud-train.txt, pt\_br-ud-train.txt, es-ud-train.txt, ar\_nyuad-ud-dev.txt, ar\_nyuad-ud-test.txt, ar\_nyuad-ud-train.txt, cop-ud-dev.txt, cop-ud-test.txt, cop-ud-train.txt, sa-ud-dev.txt, sa-ud-test.txt, sa-ud-train.txt).},
	language = {grc},
	urldate = {2023-07-06},
	journal = {http://universaldependencies.org/},
	author = {Nivre, Joakim and Agić, Željko and Ahrenberg, Lars and Aranzabe, Maria Jesus and Asahara, Masayuki and Atutxa, Aitziber and Ballesteros, Miguel and Bauer, John and Bengoetxea, Kepa and Bhat, Riyaz Ahmad and Bick, Eckhard and Bosco, Cristina and Bouma, Gosse and Bowman, Sam and Candito, Marie and Cebiroğlu Eryiğit, Gülşen and Celano, Giuseppe G. A. and Chalub, Fabricio and Choi, Jinho and Çöltekin, Çağrı and Connor, Miriam and Davidson, Elizabeth and de Marneffe, Marie-Catherine and de Paiva, Valeria and Diaz de Ilarraza, Arantza and Dobrovoljc, Kaja and Dozat, Timothy and Droganova, Kira and Dwivedi, Puneet and Eli, Marhaba and Erjavec, Tomaž and Farkas, Richárd and Foster, Jennifer and Freitas, Cláudia and Gajdošová, Katarína and Galbraith, Daniel and Garcia, Marcos and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and Gökırmak, Memduh and Goldberg, Yoav and Gómez Guinovart, Xavier and Gonzáles Saavedra, Berta and Grioni, Matias and Grūzītis, Normunds and Guillaume, Bruno and Habash, Nizar and Hajič, Jan and Hà Mỹ, Linh and Haug, Dag and Hladká, Barbora and Hohle, Petter and Ion, Radu and Irimia, Elena and Johannsen, Anders and Jørgensen, Fredrik and Kaşıkara, Hüner and Kanayama, Hiroshi and Kanerva, Jenna and Kotsyba, Natalia and Krek, Simon and Laippala, Veronika and Lê Hồng, Phương and Lenci, Alessandro and Ljubešić, Nikola and Lyashevskaya, Olga and Lynn, Teresa and Makazhanov, Aibek and Manning, Christopher and Mărănduc, Cătălina and Mareček, David and Martínez Alonso, Héctor and Martins, André and Mašek, Jan and Matsumoto, Yuji and McDonald, Ryan and Missilä, Anna and Mititelu, Verginica and Miyao, Yusuke and Montemagni, Simonetta and More, Amir and Mori, Shunsuke and Moskalevskyi, Bohdan and Muischnek, Kadri and Mustafina, Nina and Müürisep, Kaili and Nguyễn Thị, Lương and Nguyễn Thị Minh, Huyền and Nikolaev, Vitaly and Nurmi, Hanna and Ojala, Stina and Osenova, Petya and Øvrelid, Lilja and Pascual, Elena and Passarotti, Marco and Perez, Cenel-Augusto and Perrier, Guy and Petrov, Slav and Piitulainen, Jussi and Plank, Barbara and Popel, Martin and Pretkalniņa, Lauma and Prokopidis, Prokopis and Puolakainen, Tiina and Pyysalo, Sampo and Rademaker, Alexandre and Ramasamy, Loganathan and Real, Livy and Rituma, Laura and Rosa, Rudolf and Saleh, Shadi and Sanguinetti, Manuela and Saulīte, Baiba and Schuster, Sebastian and Seddah, Djamé and Seeker, Wolfgang and Seraji, Mojgan and Shakurova, Lena and Shen, Mo and Sichinava, Dmitry and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simkó, Katalin and Šimková, Mária and Simov, Kiril and Smith, Aaron and Suhr, Alane and Sulubacak, Umut and Szántó, Zsolt and Taji, Dima and Tanaka, Takaaki and Tsarfaty, Reut and Tyers, Francis and Uematsu, Sumire and Uria, Larraitz and van Noord, Gertjan and Varga, Viktor and Vincze, Veronika and Washington, Jonathan North and Žabokrtský, Zdeněk and Zeldes, Amir and Zeman, Daniel and Zhu, Hanzhi},
	month = mar,
	year = {2017},
	note = {Accepted: 2017-03-14T13:16:19Z
Publisher: Universal Dependencies Consortium},
}

@inproceedings{straka_tokenizing_2017,
	address = {Vancouver, Canada},
	title = {Tokenizing, {POS} {Tagging}, {Lemmatizing} and {Parsing} {UD} 2.0 with {UDPipe}},
	url = {http://www.aclweb.org/anthology/K/K17/K17-3009.pdf},
	booktitle = {Proceedings of the {CoNLL} 2017 {Shared} {Task}: {Multilingual} {Parsing} from {Raw} {Text} to {Universal} {Dependencies}},
	publisher = {Association for Computational Linguistics},
	author = {Straka, Milan and Straková, Jana},
	month = aug,
	year = {2017},
	pages = {88--99},
}

@misc{wijffels_udpipe_2023,
	title = {udpipe: {Tokenization}, {Parts} of {Speech} {Tagging}, {Lemmatization} and {Dependency} {Parsing} with the '{UDPipe}' '{NLP}' {Toolkit}},
	copyright = {MPL-2.0},
	shorttitle = {udpipe},
	url = {https://cran.r-project.org/web/packages/udpipe/index.html},
	abstract = {This natural language processing toolkit provides language-agnostic 'tokenization', 'parts of speech tagging', 'lemmatization' and 'dependency parsing' of raw text. Next to text parsing, the package also allows you to train annotation models based on data of 'treebanks' in 'CoNLL-U' format as provided at {\textless}https://universaldependencies.org/format.html{\textgreater}. The techniques are explained in detail in the paper: 'Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe', available at {\textless}doi:10.18653/v1/K17-3009{\textgreater}. The toolkit also contains functionalities for commonly used data manipulations on texts which are enriched with the output of the parser. Namely functionalities and algorithms for collocations, token co-occurrence, document term matrix handling, term frequency inverse document frequency calculations, information retrieval metrics (Okapi BM25), handling of multi-word expressions, keyword detection (Rapid Automatic Keyword Extraction, noun phrase extraction, syntactical patterns) sentiment scoring and semantic similarity analysis.},
	urldate = {2023-07-06},
	author = {Wijffels, Jan and BNOSAC and Linguistics, Institute of Formal {and} Applied and Physics, Faculty of Mathematics and and Prague, Charles University in and Republic, Czech and Straka, Milan and Straková, Jana},
	month = jan,
	year = {2023},
	keywords = {NaturalLanguageProcessing},
}

@article{feinerer_text_2008,
	title = {Text {Mining} {Infrastructure} in {R}},
	volume = {25},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v25/i05/},
	doi = {10.18637/jss.v025.i05},
	language = {en},
	number = {5},
	urldate = {2023-07-06},
	journal = {Journal of Statistical Software},
	author = {Feinerer, Ingo and Hornik, Kurt and Meyer, David},
	year = {2008},
	file = {Accepted Version:C\:\\Users\\Job Schepens\\Zotero\\storage\\ZIEL98MJ\\Feinerer et al. - 2008 - Text Mining Infrastructure in R.pdf:application/pdf},
}

@misc{diamond_genlangs_2023,
	title = {"{Genlangs}" and {Zipf}'s {Law}: {Do} languages generated by {ChatGPT} statistically look human?},
	shorttitle = {"{Genlangs}" and {Zipf}'s {Law}},
	url = {http://arxiv.org/abs/2304.12191},
	doi = {10.48550/arXiv.2304.12191},
	abstract = {OpenAI's GPT-4 is a Large Language Model (LLM) that can generate coherent constructed languages, or "conlangs," which we propose be called "genlangs" when generated by Artificial Intelligence (AI). The genlangs created by ChatGPT for this research (Voxphera, Vivenzia, and Lumivoxa) each have unique features, appear facially coherent, and plausibly "translate" into English. This study investigates whether genlangs created by ChatGPT follow Zipf's law. Zipf's law approximately holds across all natural and artificially constructed human languages. According to Zipf's law, the word frequencies in a text corpus are inversely proportional to their rank in the frequency table. This means that the most frequent word appears about twice as often as the second most frequent word, three times as often as the third most frequent word, and so on. We hypothesize that Zipf's law will hold for genlangs because (1) genlangs created by ChatGPT fundamentally operate in the same way as human language with respect to the semantic usefulness of certain tokens, and (2) ChatGPT has been trained on a corpora of text that includes many different languages, all of which exhibit Zipf's law to varying degrees. Through statistical linguistics, we aim to understand if LLM-based languages statistically look human. Our findings indicate that genlangs adhere closely to Zipf's law, supporting the hypothesis that genlangs created by ChatGPT exhibit similar statistical properties to natural and artificial human languages. We also conclude that with human assistance, AI is already capable of creating the world's first fully-functional genlang, and we call for its development.},
	urldate = {2023-06-29},
	publisher = {arXiv},
	author = {Diamond, Justin},
	month = mar,
	year = {2023},
	note = {arXiv:2304.12191 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\P4FMCX4G\\Diamond - 2023 - Genlangs and Zipf's Law Do languages generated .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\TSKCW8BS\\2304.html:text/html},
}

@article{jurish_word_2013,
	title = {Word and sentence tokenization with {Hidden} {Markov} {Models}},
	volume = {28},
	number = {2},
	journal = {Journal for Language Technology and Computational Linguistics},
	author = {Jurish, Bryan and Würzner, Kay-Michael},
	year = {2013},
	pages = {61--83},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\XKHS6DVT\\Jurish and Würzner - 2013 - Word and sentence tokenization with Hidden Markov .pdf:application/pdf},
}

@article{graf_faktorenanalyse_2005,
	title = {Faktorenanalyse von 57 {Variablen} der visuellen {Worterkennung}},
	volume = {213},
	number = {4},
	journal = {Zeitschrift für Psychologie/Journal of Psychology},
	author = {Graf, Ralf and Nagler, Markus and Jacobs, Arthur M.},
	year = {2005},
	note = {Publisher: Hogrefe Verlag Göttingen},
	pages = {205--218},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\APGZQ9K8\\0044-3409.213.4.html:text/html},
}

@misc{ignat_phd_2023,
	title = {A {PhD} {Student}'s {Perspective} on {Research} in {NLP} in the {Era} of {Very} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.12544},
	doi = {10.48550/arXiv.2305.12544},
	abstract = {Recent progress in large language models has enabled the deployment of many generative NLP applications. At the same time, it has also led to a misleading public discourse that ``it's all been solved.'' Not surprisingly, this has in turn made many NLP researchers -- especially those at the beginning of their career -- wonder about what NLP research area they should focus on. This document is a compilation of NLP research directions that are rich for exploration, reflecting the views of a diverse group of PhD students in an academic research lab. While we identify many research areas, many others exist; we do not cover those areas that are currently addressed by LLMs but where LLMs lag behind in performance, or those focused on LLM development. We welcome suggestions for other research directions to include: https://bit.ly/nlp-era-llm},
	urldate = {2023-06-05},
	publisher = {arXiv},
	author = {Ignat, Oana and Jin, Zhijing and Abzaliev, Artem and Biester, Laura and Castro, Santiago and Deng, Naihao and Gao, Xinyi and Gunal, Aylin and He, Jacky and Kazemi, Ashkan and Khalifa, Muhammad and Koh, Namho and Lee, Andrew and Liu, Siyang and Min, Do June and Mori, Shinka and Nwatu, Joan and Perez-Rosas, Veronica and Shen, Siqi and Wang, Zekun and Wu, Winston and Mihalcea, Rada},
	month = may,
	year = {2023},
	note = {arXiv:2305.12544 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\N8VTBY6V\\Ignat et al. - 2023 - A PhD Student's Perspective on Research in NLP in .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\452QL8L3\\2305.html:text/html},
}

@article{noauthor_empfehlungen_nodate,
	title = {Empfehlungen zu datentechnischen {Standards} und {Tools} bei der {Erhebung} von {Sprachkorpora}},
	language = {de},
	file = {Empfehlungen zu datentechnischen Standards und Too.pdf:C\:\\Users\\Job Schepens\\Zotero\\storage\\5I9EJTRC\\Empfehlungen zu datentechnischen Standards und Too.pdf:application/pdf},
}

@book{kuhn_tidy_nodate,
	title = {Tidy {Modeling} with {R}},
	url = {https://www.tmwr.org/},
	abstract = {The tidymodels framework is a collection of R packages for modeling and machine learning using tidyverse principles. This book provides a thorough introduction to how to use tidymodels, and an outline of good methodology and statistical practice for phases of the modeling process.},
	urldate = {2023-05-22},
	author = {Kuhn, Max and Silge, Julia},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\JME4THKM\\www.tmwr.org.html:text/html},
}

@book{silge_chapter_nodate,
	title = {Chapter 2 {Tokenization} {\textbar} {Supervised} {Machine} {Learning} for {Text} {Analysis} in {R}},
	url = {https://smltar.com/tokenization.html},
	abstract = {Chapter 2 Tokenization {\textbar} Supervised Machine Learning for Text Analysis in R},
	urldate = {2023-05-22},
	author = {Silge, Emil Hvitfeldt {and} Julia},
}

@book{hvitfeldt_supervised_2021,
	title = {Supervised machine learning for text analysis in {R}},
	publisher = {CRC Press},
	author = {Hvitfeldt, Emil and Silge, Julia},
	year = {2021},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\L3TDCN9Z\\books.html:text/html},
}

@misc{lebryk_introduction_2021,
	title = {Introduction to the {Structural} {Topic} {Model} ({STM})},
	url = {https://towardsdatascience.com/introduction-to-the-structural-topic-model-stm-34ec4bd5383},
	abstract = {A unique way to use topic modelling for social science research},
	language = {en},
	urldate = {2023-05-22},
	journal = {Medium},
	author = {Lebryk, Theo},
	month = apr,
	year = {2021},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\NWXYIW9A\\introduction-to-the-structural-topic-model-stm-34ec4bd5383.html:text/html},
}

@book{grimmer_text_2022,
	title = {Text as data: {A} new framework for machine learning and the social sciences},
	shorttitle = {Text as data},
	publisher = {Princeton University Press},
	author = {Grimmer, Justin and Roberts, Margaret E. and Stewart, Brandon M.},
	year = {2022},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\AP8UUV29\\books.html:text/html},
}

@book{silge_text_2017,
	title = {Text mining with {R}: {A} tidy approach},
	shorttitle = {Text mining with {R}},
	publisher = {" O'Reilly Media, Inc."},
	author = {Silge, Julia and Robinson, David},
	year = {2017},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\NF3RUNRZ\\books.html:text/html},
}

@article{gregorova_access_2023,
	title = {Access to meaning from visual input: {Object} and word frequency effects in categorization behavior.},
	shorttitle = {Access to meaning from visual input},
	journal = {Journal of Experimental Psychology: General},
	author = {Gregorova, Klara and Turini, Jacopo and Gagl, Benjamin and Võ, Melissa Le-Hoa},
	year = {2023},
	note = {Publisher: American Psychological Association},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\L6XDK6TC\\2023-69316-001.html:text/html},
}

@article{keuleers_subtlex-nl_2010,
	title = {{SUBTLEX}-{NL}: {A} new measure for {Dutch} word frequency based on film subtitles},
	volume = {42},
	shorttitle = {{SUBTLEX}-{NL}},
	number = {3},
	journal = {Behavior Research Methods},
	author = {Keuleers, EMMANUEL and Brysbaert, MARC and New, BORIS},
	year = {2010},
	pages = {643--650},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\4SX3BSQF\\KEULEERS et al. - 2010 - SUBTLEX-NL A new measure for Dutch word frequency.pdf:application/pdf},
}

@article{slobin_origins_1997,
	title = {The origins of grammaticizable notions: {Beyond} the individual mind},
	volume = {5},
	shorttitle = {The origins of grammaticizable notions},
	journal = {The crosslinguistic study of language acquisition. Expanding the contexts},
	author = {Slobin, Dan I.},
	year = {1997},
	note = {Publisher: Lawrence Erlbaum Associates},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\EYI3GBWN\\1573387449547652480.html:text/html},
}

@article{slobin_learning_1991,
	title = {Learning to think for speaking: {Native} language, cognition, and rhetorical style},
	volume = {1},
	shorttitle = {Learning to think for speaking},
	number = {1},
	journal = {Pragmatics. Quarterly Publication of the International Pragmatics Association (IPrA)},
	author = {Slobin, Dan I.},
	year = {1991},
	note = {Publisher: John Benjamins Publishing Company Amsterdam/Philadephia},
	pages = {7--25},
}

@article{slobin_learning_nodate,
	title = {{LEARNING} {TO} {THINK} {FOR} {SPEAKING}: {NATTVE} {LANGUAGE}, {COGNITION}, {AND} {RHETORICAL} {STYLB}},
	shorttitle = {{LEARNING} {TO} {THINK} {FOR} {SPEAKING}},
	author = {Slobin, Dan I.},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\DRSN8772\\Slobin - LEARNING TO THINK FOR SPEAKING NATTVE LANGUAGE, C.pdf:application/pdf},
}

@article{brown_language_2020,
	title = {Language models are few-shot learners},
	volume = {33},
	journal = {Advances in neural information processing systems},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D. and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda},
	year = {2020},
	pages = {1877--1901},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\ZHWNW425\\1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html:text/html},
}

@book{hart_meaningful_1995,
	title = {Meaningful differences in the everyday experience of young {American} children.},
	publisher = {Paul H Brookes Publishing},
	author = {Hart, Betty and Risley, Todd R.},
	year = {1995},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\WNP6XKWK\\1995-98021-000.html:text/html},
}

@article{stokes_neighborhood_2010,
	title = {Neighborhood {Density} and {Word} {Frequency} {Predict} {Vocabulary} {Size} in {Toddlers}},
	volume = {53},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/1092-4388%282009/08-0254%29},
	doi = {10.1044/1092-4388(2009/08-0254)},
	abstract = {Purpose
              To document the lexical characteristics of neighborhood density (ND) and word frequency (WF) in the lexicons of a large sample of English-speaking toddlers.
            
            
              Method
              
                Parents of 222 British-English–speaking children aged 27(±3) months completed a British adaptation of the MacArthur–Bates Communicative Development Inventory: Words and Sentences (MCDI; Klee \& Harrison, 2001). Child words were coded for ND and WF, and the relationships among vocabulary, ND, and WF were examined. A cut-point of −1
                SD
                below the mean on the MCDI classified children into one of two groups: low or high vocabulary size. Group differences on ND and WF were examined using nonparametric statistics.
              
            
            
              Results
              In a hierarchical regression, ND and WF accounted for 47\% and 14\% of unique variance in MCDI scores, respectively. Low-vocabulary children scored significantly higher on ND and significantly lower on WF than did high-vocabulary children, but there was more variability in ND and WF for children at the lowest points of the vocabulary continuum.
            
            
              Conclusion
              Children at the lowest points of a continuum of vocabulary size may be extracting statistical properties of the input language in a manner quite different from their more able age peers.},
	language = {en},
	number = {3},
	urldate = {2023-05-16},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Stokes, Stephanie F.},
	month = jun,
	year = {2010},
	pages = {670--683},
}

@article{brysbaert_impact_2016,
	title = {The impact of word prevalence on lexical decision times: {Evidence} from the {Dutch} {Lexicon} {Project} 2.},
	volume = {42},
	issn = {1939-1277, 0096-1523},
	shorttitle = {The impact of word prevalence on lexical decision times},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xhp0000159},
	doi = {10.1037/xhp0000159},
	abstract = {Keuleers, Stevens, Mandera, and Brysbaert (2015) presented a new variable, word prevalence, defined as word knowledge in the population. Some words are known to more people than other. This is particularly true for low-frequency words (e.g., screenshot vs. scourage). In the present study, we examined the impact of the measure by collecting lexical decision times for 30,000 Dutch word lemmas of various lengths (the Dutch Lexicon Project 2). Word prevalence had the second highest correlation with lexical decision times (after word frequency): Words known by everyone in the population were responded to 100 ms faster than words known to only half of the population, even after controlling for word frequency, word length, age of acquisition, similarity to other words, and concreteness. Because word prevalence has rather low correlations with the existing measures (including word frequency), the unique variance it contributes to lexical decision times is higher than that of the other variables. We consider the reasons why word prevalence has an impact on word processing times and we argue that it is likely to be the most important new variable protecting researchers against experimenter bias in selecting stimulus materials.},
	language = {en},
	number = {3},
	urldate = {2023-05-16},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Brysbaert, Marc and Stevens, Michaël and Mandera, Paweł and Keuleers, Emmanuel},
	year = {2016},
	pages = {441--458},
}

@article{hallin_effects_2018,
	title = {Effects of frequency and morphosyntactic structure on error detection, correction, and repetition in {Swedish}-speaking children},
	volume = {39},
	issn = {0142-7164, 1469-1817},
	url = {https://www.cambridge.org/core/product/identifier/S0142716418000280/type/journal_article},
	doi = {10.1017/S0142716418000280},
	abstract = {ABSTRACT
            Grammatical error detection and correction are often used to test explicit language knowledge. This study investigated effects of token frequency and error type in error detection, correction, and repetition, and performance on the three tasks were compared and related to models of metalinguistic awareness and development. Thirty Swedish-speaking 10-year-olds with typical language development participated in the study, which focused on four morphosyntactic errors: the infinitive instead of past tense for regular and irregular verbs, and the omission of the obligatory indefinite article in common and neuter gender noun phrases. Target verbs and nouns were of high or low frequency. Results showed significant effects of verb frequency in all tasks, and effects of noun gender for error detection, but not for correction and repetition. Children detected significantly more past-tense errors than they accurately corrected, but the opposite result was seen for noun phrase errors. The patterns of results both within and across tasks imply that implicit language knowledge affects performance, and that lexical frequency, even of familiar words, needs to be controlled when designing tasks for measuring grammatical knowledge. The particular challenge of the Swedish neuter noun phrase in language development and language processing needs to be further investigated.},
	language = {en},
	number = {6},
	urldate = {2023-05-16},
	journal = {Applied Psycholinguistics},
	author = {Hallin, Anna Eva and Reuterskiöld, Christina},
	month = nov,
	year = {2018},
	pages = {1189--1220},
}

@article{lieven_input_2010,
	title = {Input and first language acquisition: {Evaluating} the role of frequency},
	volume = {120},
	issn = {00243841},
	shorttitle = {Input and first language acquisition},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0024384110001658},
	doi = {10.1016/j.lingua.2010.06.005},
	abstract = {Semantic Scholar extracted view of "Input and first language acquisition: Evaluating the role of frequency" by E. Lieven},
	language = {en},
	number = {11},
	urldate = {2023-05-16},
	journal = {Lingua},
	author = {Lieven, Elena},
	month = nov,
	year = {2010},
	pages = {2546--2556},
}

@article{laarmann-quante_litkey_2019,
	title = {The {Litkey} {Corpus}: {A} richly annotated longitudinal corpus of {German} texts written by primary school children},
	volume = {51},
	issn = {1554-3528},
	shorttitle = {The {Litkey} {Corpus}},
	url = {http://link.springer.com/10.3758/s13428-019-01261-x},
	doi = {10.3758/s13428-019-01261-x},
	language = {en},
	number = {4},
	urldate = {2023-09-19},
	journal = {Behavior Research Methods},
	author = {Laarmann-Quante, Ronja and Ortmann, Katrin and Ehlert, Anna and Masloch, Simon and Scholz, Doreen and Belke, Eva and Dipper, Stefanie},
	month = aug,
	year = {2019},
	pages = {1889--1918},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\R2G3I9YN\\Laarmann-Quante et al. - 2019 - The Litkey Corpus A richly annotated longitudinal.pdf:application/pdf},
}

@article{rouder_lognormal_2015,
	title = {The {Lognormal} {Race}: {A} {Cognitive}-{Process} {Model} of {Choice} and {Latency} with {Desirable} {Psychometric} {Properties}},
	volume = {80},
	issn = {1860-0980},
	shorttitle = {The {Lognormal} {Race}},
	url = {https://doi.org/10.1007/s11336-013-9396-3},
	doi = {10.1007/s11336-013-9396-3},
	abstract = {We present a cognitive process model of response choice and response time performance data that has excellent psychometric properties and may be used in a wide variety of contexts. In the model there is an accumulator associated with each response option. These accumulators have bounds, and the first accumulator to reach its bound determines the response time and response choice. The times at which accumulator reaches its bound is assumed to be lognormally distributed, hence the model is race or minima process among lognormal variables. A key property of the model is that it is relatively straightforward to place a wide variety of models on the logarithm of these finishing times including linear models, structural equation models, autoregressive models, growth-curve models, etc. Consequently, the model has excellent statistical and psychometric properties and can be used in a wide range of contexts, from laboratory experiments to high-stakes testing, to assess performance. We provide a Bayesian hierarchical analysis of the model, and illustrate its flexibility with an application in testing and one in lexical decision making, a reading skill.},
	language = {en},
	number = {2},
	urldate = {2023-09-15},
	journal = {Psychometrika},
	author = {Rouder, Jeffrey N. and Province, Jordan M. and Morey, Richard D. and Gomez, Pablo and Heathcote, Andrew},
	month = jun,
	year = {2015},
	keywords = {cognitive psychometrics, race models, response-times models},
	pages = {491--513},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\EBBWVWI2\\Rouder et al. - 2015 - The Lognormal Race A Cognitive-Process Model of C.pdf:application/pdf},
}

@article{shatz_potential_2023,
	title = {The potential influence of crosslinguistic lexical similarity on lexical diversity in {L2} {English} writing},
	journal = {Corpora},
	author = {Shatz, Itamar and Alexopoulou, Theodora and Murakami, Akira},
	year = {2023},
	note = {Publisher: Edinburgh University Press},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\578P74QS\\Shatz et al. - 2023 - The potential influence of crosslinguistic lexical.pdf:application/pdf;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\QJQ8GG2E\\the-potential-influence-of-crosslinguistic-lexical-similarity-on-.html:text/html},
}

@misc{mahowald_dissociating_2023,
	title = {Dissociating language and thought in large language models: a cognitive perspective},
	shorttitle = {Dissociating language and thought in large language models},
	url = {http://arxiv.org/abs/2301.06627},
	doi = {10.48550/arXiv.2301.06627},
	abstract = {Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become -- "thinking machines", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.},
	urldate = {2023-09-14},
	publisher = {arXiv},
	author = {Mahowald, Kyle and Ivanova, Anna A. and Blank, Idan A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
	month = jan,
	year = {2023},
	note = {arXiv:2301.06627 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\NXF3YNVV\\Mahowald et al. - 2023 - Dissociating language and thought in large languag.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\8KUECEXD\\2301.html:text/html},
}

@article{elhaik_principal_2022,
	title = {Principal {Component} {Analyses} ({PCA})-based findings in population genetic studies are highly biased and must be reevaluated},
	volume = {12},
	issn = {2045-2322},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9424212/},
	doi = {10.1038/s41598-022-14395-4},
	abstract = {Principal Component Analysis (PCA) is a multivariate analysis that reduces the complexity of datasets while preserving data covariance. The outcome can be visualized on colorful scatterplots, ideally with only a minimal loss of information. PCA applications, implemented in well-cited packages like EIGENSOFT and PLINK, are extensively used as the foremost analyses in population genetics and related fields (e.g., animal and plant or medical genetics). PCA outcomes are used to shape study design, identify, and characterize individuals and populations, and draw historical and ethnobiological conclusions on origins, evolution, dispersion, and relatedness. The replicability crisis in science has prompted us to evaluate whether PCA results are reliable, robust, and replicable. We analyzed twelve common test cases using an intuitive color-based model alongside human population data. We demonstrate that PCA results can be artifacts of the data and can be easily manipulated to generate desired outcomes. PCA adjustment also yielded unfavorable outcomes in association studies. PCA results may not be reliable, robust, or replicable as the field assumes. Our findings raise concerns about the validity of results reported in the population genetics literature and related fields that place a disproportionate reliance upon PCA outcomes and the insights derived from them. We conclude that PCA may have a biasing role in genetic investigations and that 32,000-216,000 genetic studies should be reevaluated. An alternative mixed-admixture population genetic model is discussed.},
	urldate = {2023-09-07},
	journal = {Scientific Reports},
	author = {Elhaik, Eran},
	month = aug,
	year = {2022},
	pmid = {36038559},
	pmcid = {PMC9424212},
	pages = {14683},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\IYV767W2\\Elhaik - 2022 - Principal Component Analyses (PCA)-based findings .pdf:application/pdf},
}

@article{segbers_how_2017,
	title = {How many words do children know? {A} corpus-based estimation of children’s total vocabulary size},
	volume = {34},
	shorttitle = {How many words do children know?},
	number = {3},
	journal = {Language Testing},
	author = {Segbers, Jutta and Schroeder, Sascha},
	year = {2017},
	note = {Publisher: Sage Publications Sage UK: London, England},
	pages = {297--320},
}

@article{cunningham_vocabulary_2005,
	title = {Vocabulary growth through independent reading and reading aloud to children},
	journal = {Teaching and learning vocabulary: Bringing research to practice},
	author = {Cunningham, Anne E.},
	year = {2005},
	pages = {45--68},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\J239K4MI\\books.html:text/html},
}

@article{cain_matthew_2011,
	title = {Matthew effects in young readers: {Reading} comprehension and reading experience aid vocabulary development},
	volume = {44},
	shorttitle = {Matthew effects in young readers},
	number = {5},
	journal = {Journal of learning disabilities},
	author = {Cain, Kate and Oakhill, Jane},
	year = {2011},
	note = {Publisher: Sage Publications Sage CA: Los Angeles, CA},
	pages = {431--443},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\5FBHB2GP\\0022219411410042.html:text/html},
}

@misc{noauthor_custom_nodate,
	title = {Custom instructions for {ChatGPT}},
	url = {https://openai.com/blog/custom-instructions-for-chatgpt},
	abstract = {We’re rolling out custom instructions to give you more control over how ChatGPT responds. Set your preferences, and ChatGPT will keep them in mind for all future conversations.},
	language = {en-US},
	urldate = {2023-09-06},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\ZLZNJLEA\\custom-instructions-for-chatgpt.html:text/html},
}

@book{schnell_understanding_2021,
	address = {London},
	title = {Understanding {Corpus} {Linguistics}},
	isbn = {978-0-429-26903-5},
	abstract = {This textbook introduces the fundamental concepts and methods of corpus linguistics for students approaching this topic for the first time, putting specific emphasis on the enormous linguistic diversity represented by approximately  7,000 human languages and broadening the scope of current concerns in general corpus linguistics.
Including a basic toolkit to help the reader investigate language in different usage contexts, this book:

Shows the relevance of corpora to a range of linguistic areas from phonology to sociolinguistics and discourse
Covers recent developments in the application of corpus linguistics to the study of understudied languages and linguistic typology
Features exercises, short problems, and questions
Includes examples from real studies in over 15 languages plus multilingual corpora
Providing the necessary corpus linguistics skills to critically evaluate and replicate studies, this book is essential reading for anyone studying corpus linguistics.},
	publisher = {Routledge},
	author = {Schnell, Stefan, Danielle Barth},
	month = nov,
	year = {2021},
	doi = {10.4324/9780429269035},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\T8AT6PKM\\Schnell - 2021 - Understanding Corpus Linguistics.pdf:application/pdf},
}

@inproceedings{liesenfeld_opening_2023,
	address = {New York, NY, USA},
	series = {{CUI} '23},
	title = {Opening up {ChatGPT}: {Tracking} openness, transparency, and accountability in instruction-tuned text generators},
	isbn = {9798400700149},
	shorttitle = {Opening up {ChatGPT}},
	url = {https://dl.acm.org/doi/10.1145/3571884.3604316},
	doi = {10.1145/3571884.3604316},
	abstract = {Large language models that exhibit instruction-following behaviour represent one of the biggest recent upheavals in conversational interfaces, a trend in large part fuelled by the release of OpenAI’s ChatGPT, a proprietary large language model for text generation fine-tuned through reinforcement learning from human feedback (LLM+RLHF). We review the risks of relying on proprietary software and survey the first crop of open-source projects of comparable architecture and functionality. The main contribution of this paper is to show that openness is differentiated, and to offer scientific documentation of degrees of openness in this fast-moving field. We evaluate projects in terms of openness of code, training data, model weights, RLHF data, licensing, scientific documentation, and access methods. We find that while there is a fast-growing list of projects billing themselves as ‘open source’, many inherit undocumented data of dubious legality, few share the all-important instruction-tuning (a key site where human annotation labour is involved), and careful scientific documentation is exceedingly rare. Degrees of openness are relevant to fairness and accountability at all points, from data collection and curation to model architecture, and from training and fine-tuning to release and deployment.},
	urldate = {2023-09-06},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Conversational} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Liesenfeld, Andreas and Lopez, Alianda and Dingemanse, Mark},
	month = jul,
	year = {2023},
	keywords = {chatGPT, large language models, open source, RLHF, survey},
	pages = {1--6},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\CCD5NFLC\\Liesenfeld et al. - 2023 - Opening up ChatGPT Tracking openness, transparenc.pdf:application/pdf},
}

@misc{noauthor_openai_nodate,
	title = {{OpenAI}},
	url = {https://openai.com/},
	abstract = {Creating safe AGI that benefits all of humanity},
	language = {en-US},
	urldate = {2023-09-06},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\9N8UXMEX\\openai.com.html:text/html},
}

@misc{heilbron_prediction_2022,
	title = {Prediction and preview strongly affect reading times but not skipping during natural reading},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.10.06.463362v2},
	doi = {10.1101/2021.10.06.463362},
	abstract = {In a typical text, readers look much longer at some words than at others and fixate some words multiple times, while skipping others altogether. Historically, researchers explained this variation via low-level visual or oculomotor factors, but today it is primarily explained in terms of cognitive factors, such as how well word identity can be predicted from context or discerned from parafoveal preview. While the existence of these effects has been well established in experiments, the relative importance of prediction, preview and low-level factors for eye movement variation in natural reading is unclear. Here, we address this question in three large datasets (n=104, 1.5 million words), using a deep neural network and Bayesian ideal observer to model linguistic prediction and parafoveal preview from moment to moment in natural reading. Strikingly, neither prediction nor preview was important for explaining word skipping – the vast majority of skipping was explained by a simple oculomotor model. For reading times, by contrast, we found strong but independent contributions of both prediction and preview, with effect sizes matching those from controlled experiments. Together, these results challenge dominant models of eye movements in reading by showing that linguistic prediction and parafoveal preview are not important determinants of word skipping.},
	language = {en},
	urldate = {2023-09-06},
	publisher = {bioRxiv},
	author = {Heilbron, Micha and Haren, Jorie van and Hagoort, Peter and Lange, Floris P. de},
	month = feb,
	year = {2022},
	note = {Pages: 2021.10.06.463362
Section: New Results},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\GHLPY7L4\\Heilbron et al. - 2022 - Prediction and preview strongly affect reading tim.pdf:application/pdf},
}

@article{heilbron_prediction_2021,
	title = {Prediction and preview strongly affect reading times but not skipping during natural reading},
	journal = {BioRxiv},
	author = {Heilbron, Micha and van Haren, Jorie and Hagoort, Peter and de Lange, Floris P.},
	year = {2021},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {2021--10},
}

@inproceedings{chandra_synthetic_2023,
	address = {New York, NY, USA},
	series = {{ETRA} '23},
	title = {Synthetic predictabilities from large language models explain reading eye movements},
	isbn = {9798400701504},
	url = {https://dl.acm.org/doi/10.1145/3588015.3588420},
	doi = {10.1145/3588015.3588420},
	abstract = {A long tradition in eye movement research has focused on three linguistic variables explaining fixation durations during sentence reading: word length, frequency, and predictability. Lengths and frequencies are easily obtainable but predictabilities are tedious to collect, requiring the incremental cloze procedure. Modern large language models are trained using the objective of predicting the next word given previous context, hence they readily provide predictability information. This capability has largely been overlooked in eye movement research. Here we investigate the suitability of a synthetic predictability measure, extracted from pretrained GPT-2 models, as a surrogate for cloze predictability. Using several published eye movement corpora, we find that synthetic and cloze predictabilities are highly correlated, and that their influence on eye movements is qualitatively similar. Similar patterns are obtained when including synthetic predictabilities in data sets lacking cloze predictabilities. In conclusion, synthetic predictabilities can serve as a substitute for empirical cloze predictabilities.},
	urldate = {2023-09-06},
	booktitle = {Proceedings of the 2023 {Symposium} on {Eye} {Tracking} {Research} and {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Chandra, Johan and Witzig, Nicholas and Laubrock, Jochen},
	month = may,
	year = {2023},
	pages = {1--7},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\JRHV4IN2\\Chandra et al. - 2023 - Synthetic predictabilities from large language mod.pdf:application/pdf},
}

@article{kasneci_chatgpt_2023,
	title = {{ChatGPT} for good? {On} opportunities and challenges of large language models for education},
	volume = {103},
	issn = {1041-6080},
	shorttitle = {{ChatGPT} for good?},
	url = {https://www.sciencedirect.com/science/article/pii/S1041608023000195},
	doi = {10.1016/j.lindif.2023.102274},
	abstract = {Large language models represent a significant advancement in the field of AI. The underlying technology is key to further innovations and, despite critical views and even bans within communities and regions, large language models are here to stay. This commentary presents the potential benefits and challenges of educational applications of large language models, from student and teacher perspectives. We briefly discuss the current state of large language models and their applications. We then highlight how these models can be used to create educational content, improve student engagement and interaction, and personalize learning experiences. With regard to challenges, we argue that large language models in education require teachers and learners to develop sets of competencies and literacies necessary to both understand the technology as well as their limitations and unexpected brittleness of such systems. In addition, a clear strategy within educational systems and a clear pedagogical approach with a strong focus on critical thinking and strategies for fact checking are required to integrate and take full advantage of large language models in learning settings and teaching curricula. Other challenges such as the potential bias in the output, the need for continuous human oversight, and the potential for misuse are not unique to the application of AI in education. But we believe that, if handled sensibly, these challenges can offer insights and opportunities in education scenarios to acquaint students early on with potential societal biases, criticalities, and risks of AI applications. We conclude with recommendations for how to address these challenges and ensure that such models are used in a responsible and ethical manner in education.},
	urldate = {2023-09-06},
	journal = {Learning and Individual Differences},
	author = {Kasneci, Enkelejda and Sessler, Kathrin and Küchemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and Günnemann, Stephan and Hüllermeier, Eyke and Krusche, Stephan and Kutyniok, Gitta and Michaeli, Tilman and Nerdel, Claudia and Pfeffer, Jürgen and Poquet, Oleksandra and Sailer, Michael and Schmidt, Albrecht and Seidel, Tina and Stadler, Matthias and Weller, Jochen and Kuhn, Jochen and Kasneci, Gjergji},
	month = apr,
	year = {2023},
	keywords = {Education, Artificial intelligence, Large language models, Educational technologies},
	pages = {102274},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\3YBHVQLN\\Kasneci et al. - 2023 - ChatGPT for good On opportunities and challenges .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\IFIAGXUD\\S1041608023000195.html:text/html;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\SNL3K4DE\\S1041608023000195.html:text/html},
}

@article{singhal_large_2023,
	title = {Large language models encode clinical knowledge},
	volume = {620},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06291-2},
	doi = {10.1038/s41586-023-06291-2},
	abstract = {Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA3, MedMCQA4, PubMedQA5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics6), including 67.6\% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17\%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.},
	language = {en},
	number = {7972},
	urldate = {2023-09-06},
	journal = {Nature},
	author = {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly, Chris and Babiker, Abubakr and Schärli, Nathanael and Chowdhery, Aakanksha and Mansfield, Philip and Demner-Fushman, Dina and Agüera y Arcas, Blaise and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Chou, Katherine and Gottweis, Juraj and Tomasev, Nenad and Liu, Yun and Rajkomar, Alvin and Barral, Joelle and Semturs, Christopher and Karthikesalingam, Alan and Natarajan, Vivek},
	month = aug,
	year = {2023},
	note = {Number: 7972
Publisher: Nature Publishing Group},
	keywords = {Medical research, Health care},
	pages = {172--180},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\KQWEMSAZ\\Singhal et al. - 2023 - Large language models encode clinical knowledge.pdf:application/pdf},
}

@article{min_recent_2021,
	title = {Recent advances in natural language processing via large pre-trained language models: {A} survey},
	shorttitle = {Recent advances in natural language processing via large pre-trained language models},
	journal = {ACM Computing Surveys},
	author = {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heintz, Ilana and Roth, Dan},
	year = {2021},
	note = {Publisher: ACM New York, NY},
}

@book{baayen_word_2001,
	title = {Word frequency distributions},
	volume = {18},
	publisher = {Springer Science \& Business Media},
	author = {Baayen, R. Harald},
	year = {2001},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\9TZIILMK\\books.html:text/html},
}

@article{gagl_lexical_2022,
	title = {The lexical categorization model: {A} computational model of left ventral occipito-temporal cortex activation in visual word recognition},
	volume = {18},
	shorttitle = {The lexical categorization model},
	number = {6},
	journal = {Plos Computational Biology},
	author = {Gagl, Benjamin and Richlan, Fabio and Ludersdorfer, Philipp and Sassenhagen, Jona and Eisenhauer, Susanne and Gregorova, Klara and Fiebach, Christian J.},
	year = {2022},
	note = {Publisher: Public Library of Science San Francisco, CA USA},
	pages = {e1009995},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\VVQMRECY\\article.html:text/html},
}

@article{dehaene_unique_2011,
	title = {The unique role of the visual word form area in reading},
	volume = {15},
	number = {6},
	journal = {Trends in cognitive sciences},
	author = {Dehaene, Stanislas and Cohen, Laurent},
	year = {2011},
	note = {Publisher: Elsevier},
	pages = {254--262},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\RPLKBWIJ\\S1364661311000738.html:text/html;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\I6TRKNH5\\S1364-6613(11)00073-8.html:text/html},
}

@misc{noauthor_gagl_nodate,
	title = {Gagl: {The} lexical categorization model: {A} computational... - {Google} {Scholar}},
	url = {https://scholar.google.de/scholar?cluster=12191111889664444113&hl=en&as_sdt=0,5},
	urldate = {2023-08-24},
	file = {Gagl\: The lexical categorization model\: A computational... - Google Scholar:C\:\\Users\\Job Schepens\\Zotero\\storage\\QUW68RUJ\\scholar.html:text/html},
}

@article{brysbaert_word_2011,
	title = {The {Word} {Frequency} {Effect}},
	volume = {58},
	url = {https://doi.org/10.1027\%2F1618-3169\%2Fa000123},
	doi = {10.1027/1618-3169/a000123},
	number = {5},
	journal = {Experimental Psychology},
	author = {Brysbaert, Marc and Buchmeier, Matthias and Conrad, Markus and Jacobs, Arthur M. and Bölte, Jens and Böhl, Andrea},
	month = jul,
	year = {2011},
	note = {Publisher: Hogrefe Publishing Group},
	keywords = {word recognition, lexical decision, megastudy, word frequency},
	pages = {412--424},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\RPDXHDDQ\\Brysbaert et al. - 2011 - The Word Frequency Effect.pdf:application/pdf},
}

@article{yarkoni_moving_2008,
	title = {Moving beyond {Coltheart}’s {N}: {A} new measure of orthographic similarity},
	volume = {15},
	number = {5},
	journal = {Psychonomic bulletin \& review},
	author = {Yarkoni, Tal and Balota, David and Yap, Melvin},
	year = {2008},
	note = {Publisher: Springer},
	pages = {971--979},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\7PAT84NK\\r855u1v1mr980v32.html:text/html},
}

@article{schroeder_childlex_2015,
	title = {{childLex}: {A} lexical database of {German} read by children},
	volume = {47},
	journal = {Behavior research methods},
	author = {Schroeder, Sascha and Würzner, Kay-Michael and Heister, Julian and Geyken, Alexander and Kliegl, Reinhold},
	year = {2015},
	note = {Publisher: Springer},
	keywords = {Child language, Lexical database, Reading development},
	pages = {1085--1094},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\Y4CP5SK6\\Schroeder et al. - 2015 - childLex a lexical database of German read by chi.pdf:application/pdf},
}

@article{bates_fitting_2015,
	title = {Fitting {Linear} {Mixed}-{Effects} {Models}, {Using} lme4},
	volume = {67},
	url = {https://doi.org/10.18637\%2Fjss.v067.i01},
	doi = {10.18637/jss.v067.i01},
	number = {1},
	journal = {Journal of Statistical Software},
	author = {Bates, Douglas and Mächler, Martin and Bolker, Ben and Walker, Steve},
	year = {2015},
	note = {Publisher: Foundation for Open Access Statistic},
	keywords = {Statistics - Computation},
	file = {arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\RRP9C9MR\\1406.html:text/html;Bates et al_2014_Fitting linear mixed-effects models using lme4.pdf:C\:\\Users\\Job Schepens\\Zotero\\storage\\34C89HP4\\Bates et al_2014_Fitting linear mixed-effects models using lme4.pdf:application/pdf},
}

@article{heister_dlexdb_2011,
	title = {{dlexDB} – eine lexikalische {Datenbank} für die psychologische und linguistische {Forschung}},
	volume = {62},
	url = {https://doi.org/10.1026/0033-3042/a000029},
	doi = {10.1026/0033-3042/a000029},
	abstract = {Zusammenfassung. Mit der lexikalischen Datenbank dlexDB stellen wir der psychologischen und linguistischen Forschung im World Wide Web online statistische Kennwerte für eine Vielzahl von verarbeitungsrelevanten Merkmalen von Wörtern zur Verfügung. Diese Kennwerte umfassen die durch CELEX (Baayen, Piepenbrock und Gulikers, 1995) bekannten Variablen der Häufigkeiten von Wortformen und Lemmata in Texten geschriebener Sprache. Darüber hinaus berechnen wir eine Reihe neuer Kennwerte wie die Häufigkeiten von Silben, Morphemen, Zeichenfolgen und Mehrwortverbindungen sowie Wortähnlichkeitsmaße. Die Datengrundlage bildet das Kernkorpus des Digitalen Wörterbuchs der deutschen Sprache (DWDS) mit über 100 Millionen laufenden Wörtern. Wir illustrieren die Validität dieser Kennwerte mit neuen Ergebnissen zu ihrem Einfluss auf Fixationsdauern beim Lesen von Sätzen.},
	number = {1},
	journal = {Psychologische Rundschau},
	author = {Heister, Julian and Würzner, Kay-Michael and Bubenzer, Johannes and Pohl, Edmund and Hanneforth, Thomas and Geyken, Alexander and Kliegl, Reinhold},
	year = {2011},
	note = {\_eprint: https://doi.org/10.1026/0033-3042/a000029},
	pages = {10--20},
}

@article{akaike_new_1974,
	title = {A new look at the statistical model identification},
	volume = {19},
	doi = {10.1109/TAC.1974.1100705},
	number = {6},
	journal = {IEEE Transactions on Automatic Control},
	author = {Akaike, H.},
	year = {1974},
	pages = {716--723},
}

@article{schroter_developmental_2017,
	title = {The {Developmental} {Lexicon} {Project}: {A} behavioral database to investigate visual word recognition across the lifespan},
	volume = {49},
	journal = {Behavior Research Methods},
	author = {Schröter, Pauline and Schroeder, Sascha},
	year = {2017},
	note = {Publisher: Springer},
	pages = {2183--2203},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\JPC9E5CE\\s13428-016-0851-9.html:text/html;Schröter and Schroeder - 2017 - The Developmental Lexicon Project A behavioral da.pdf:C\:\\Users\\Job Schepens\\Zotero\\storage\\KK5WVLVW\\Schröter and Schroeder - 2017 - The Developmental Lexicon Project A behavioral da.pdf:application/pdf},
}

@incollection{baayen_41_2009,
	title = {41. {Corpus} linguistics in morphology: {Morphological} productivity},
	isbn = {978-3-11-021388-1},
	shorttitle = {41. {Corpus} linguistics in morphology},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110213881.2.899/html?lang=en},
	abstract = {41. Corpus linguistics in morphology: Morphological
          productivity was published in Volume 2 on page 899.},
	language = {en},
	urldate = {2023-07-12},
	booktitle = {41. {Corpus} linguistics in morphology: {Morphological}},
	publisher = {De Gruyter Mouton},
	author = {Baayen, R. Harald},
	month = mar,
	year = {2009},
	doi = {10.1515/9783110213881.2.899},
	pages = {899--919},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\H6CEMVRU\\Baayen - 2009 - 41. Corpus linguistics in morphology Morphologica.pdf:application/pdf},
}

@inproceedings{pierrehumbert_hapax_2018,
	title = {On hapax legomena and morphological productivity},
	booktitle = {Proceedings of the fifteenth workshop on computational research in phonetics, phonology, and morphology},
	author = {Pierrehumbert, Janet and Granell, Ramon},
	year = {2018},
	pages = {125--130},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\VAKHHM2S\\Pierrehumbert and Granell - 2018 - On hapax legomena and morphological productivity.pdf:application/pdf;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\BS4I8VTS\\W18-5814.html:text/html},
}

@incollection{baayen_quantitative_1992,
	address = {Dordrecht},
	series = {Yearbook of {Morphology}},
	title = {Quantitative aspects of morphological productivity},
	isbn = {978-94-011-2516-1},
	url = {https://doi.org/10.1007/978-94-011-2516-1_8},
	abstract = {Research into the phenomenon of morphological productivity, “the possibility for language users to coin, unintentionally, a number of formations which are in principle uncountable” (Schultink 1961), has mainly focused on the qualitative factors which jointly determine the productivity of word formation rules. It is well known that word formation processes are subject to various syntagmatic conditions. Booij (1977) develops a typology of such conditioning factors, distinguishing between rule-specific and rule-independent restrictions on the one hand, and between restrictions pertaining to phonological, stratal and syntactic characteristics on the other.1 The rôle of pardigmatic factors is discussed in van Marie (1985). He points out that (roughly) synonymous affixes tend to select their base words from complementary domains. Hence they can be analyzed as mutually affecting their respective degrees of productivity.},
	language = {en},
	urldate = {2023-07-12},
	booktitle = {Yearbook of {Morphology} 1991},
	publisher = {Springer Netherlands},
	author = {Baayen, Harald},
	editor = {Booij, Geert and van Marle, Jaap},
	year = {1992},
	doi = {10.1007/978-94-011-2516-1_8},
	keywords = {Lexical Access, Lexical Representation, Token Frequency, Word Formation, Word Frequency},
	pages = {109--149},
}

@misc{vanmassenhove_machine_2021,
	title = {Machine {Translationese}: {Effects} of {Algorithmic} {Bias} on {Linguistic} {Complexity} in {Machine} {Translation}},
	shorttitle = {Machine {Translationese}},
	url = {http://arxiv.org/abs/2102.00287},
	doi = {10.48550/arXiv.2102.00287},
	abstract = {Recent studies in the field of Machine Translation (MT) and Natural Language Processing (NLP) have shown that existing models amplify biases observed in the training data. The amplification of biases in language technology has mainly been examined with respect to specific phenomena, such as gender bias. In this work, we go beyond the study of gender in MT and investigate how bias amplification might affect language in a broader sense. We hypothesize that the 'algorithmic bias', i.e. an exacerbation of frequently observed patterns in combination with a loss of less frequent ones, not only exacerbates societal biases present in current datasets but could also lead to an artificially impoverished language: 'machine translationese'. We assess the linguistic richness (on a lexical and morphological level) of translations created by different data-driven MT paradigms - phrase-based statistical (PB-SMT) and neural MT (NMT). Our experiments show that there is a loss of lexical and morphological richness in the translations produced by all investigated MT paradigms for two language pairs (EN{\textless}={\textgreater}FR and EN{\textless}={\textgreater}ES).},
	urldate = {2023-07-12},
	publisher = {arXiv},
	author = {Vanmassenhove, Eva and Shterionov, Dimitar and Gwilliam, Matthew},
	month = jan,
	year = {2021},
	note = {arXiv:2102.00287 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\ICYQ3YDX\\Vanmassenhove et al. - 2021 - Machine Translationese Effects of Algorithmic Bia.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\BJ5L53Z5\\2102.html:text/html},
}

@article{tweedie_how_1998,
	title = {How variable may a constant be? {Measures} of lexical richness in perspective},
	volume = {32},
	shorttitle = {How variable may a constant be?},
	journal = {Computers and the Humanities},
	author = {Tweedie, Fiona J. and Baayen, R. Harald},
	year = {1998},
	note = {Publisher: Springer},
	pages = {323--352},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\RWMJL7LB\\TWEEDIE and BAAYEN - 1998 - How Variable May a Constant be Measures of Lexica.pdf:application/pdf},
}

@article{kyle_measuring_2019,
	title = {Measuring lexical richness},
	journal = {The Routledge handbook of vocabulary studies},
	author = {Kyle, Kristopher},
	year = {2019},
	note = {Publisher: Routledge},
	pages = {454--475},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\7BLN5W59\\books.html:text/html},
}

@inproceedings{evert_simple_2004,
	title = {A simple {LNRE} model for random character sequences},
	volume = {2004},
	booktitle = {Proceedings of {JADT}},
	author = {Evert, Stefan},
	year = {2004},
	pages = {411--422},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\2DIWWT2Z\\Evert - 2004 - A simple LNRE model for random character sequences.pdf:application/pdf},
}

@misc{kumarage_neural_2023,
	title = {Neural {Authorship} {Attribution}: {Stylometric} {Analysis} on {Large} {Language} {Models}},
	shorttitle = {Neural {Authorship} {Attribution}},
	url = {http://arxiv.org/abs/2308.07305},
	doi = {10.48550/arXiv.2308.07305},
	abstract = {Large language models (LLMs) such as GPT-4, PaLM, and Llama have significantly propelled the generation of AI-crafted text. With rising concerns about their potential misuse, there is a pressing need for AI-generated-text forensics. Neural authorship attribution is a forensic effort, seeking to trace AI-generated text back to its originating LLM. The LLM landscape can be divided into two primary categories: proprietary and open-source. In this work, we delve into these emerging categories of LLMs, focusing on the nuances of neural authorship attribution. To enrich our understanding, we carry out an empirical analysis of LLM writing signatures, highlighting the contrasts between proprietary and open-source models, and scrutinizing variations within each group. By integrating stylometric features across lexical, syntactic, and structural aspects of language, we explore their potential to yield interpretable results and augment pre-trained language model-based classifiers utilized in neural authorship attribution. Our findings, based on a range of state-of-the-art LLMs, provide empirical insights into neural authorship attribution, paving the way for future investigations aimed at mitigating the threats posed by AI-generated misinformation.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Kumarage, Tharindu and Liu, Huan},
	month = aug,
	year = {2023},
	note = {arXiv:2308.07305 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\BQJFMFRI\\Kumarage and Liu - 2023 - Neural Authorship Attribution Stylometric Analysi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\SJD68H74\\2308.html:text/html},
}

@misc{sawicki_bits_2023,
	title = {Bits of {Grass}: {Does} {GPT} already know how to write like {Whitman}?},
	shorttitle = {Bits of {Grass}},
	url = {http://arxiv.org/abs/2305.11064},
	doi = {10.48550/arXiv.2305.11064},
	abstract = {This study examines the ability of GPT-3.5, GPT-3.5-turbo (ChatGPT) and GPT-4 models to generate poems in the style of specific authors using zero-shot and many-shot prompts (which use the maximum context length of 8192 tokens). We assess the performance of models that are not fine-tuned for generating poetry in the style of specific authors, via automated evaluation. Our findings indicate that without fine-tuning, even when provided with the maximum number of 17 poem examples (8192 tokens) in the prompt, these models do not generate poetry in the desired style.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Sawicki, Piotr and Grzes, Marek and Goes, Fabricio and Brown, Dan and Peeperkorn, Max and Khatun, Aisha},
	month = may,
	year = {2023},
	note = {arXiv:2305.11064 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\QFUT4YX2\\Sawicki et al. - 2023 - Bits of Grass Does GPT already know how to write .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\G87A2LSC\\2305.html:text/html},
}

@incollection{yli-jyra_tagh_2006,
	address = {Berlin, Heidelberg},
	title = {{TAGH}: {A} {Complete} {Morphology} for {German} {Based} on {Weighted} {Finite} {State} {Automata}},
	volume = {4002},
	isbn = {978-3-540-35467-3 978-3-540-35469-7},
	shorttitle = {{TAGH}},
	url = {http://link.springer.com/10.1007/11780885_7},
	urldate = {2023-10-16},
	booktitle = {Finite-{State} {Methods} and {Natural} {Language} {Processing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Geyken, Alexander and Hanneforth, Thomas},
	editor = {Yli-Jyrä, Anssi and Karttunen, Lauri and Karhumäki, Juhani},
	year = {2006},
	doi = {10.1007/11780885_7},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {55--66},
}

@article{lestrade_unzipping_2017,
	title = {Unzipping {Zipf}’s law},
	volume = {12},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0181987},
	doi = {10.1371/journal.pone.0181987},
	abstract = {In spite of decades of theorizing, the origins of Zipf’s law remain elusive. I propose that a Zipfian distribution straightforwardly follows from the interaction of syntax (word classes differing in class size) and semantics (words having to be sufficiently specific to be distinctive and sufficiently general to be reusable). These factors are independently motivated and well-established ingredients of a natural-language system. Using a computational model, it is shown that neither of these ingredients suffices to produce a Zipfian distribution on its own and that the results deviate from the Zipfian ideal only in the same way as natural language itself does.},
	language = {en},
	number = {8},
	urldate = {2023-10-13},
	journal = {PLOS ONE},
	author = {Lestrade, Sander},
	month = aug,
	year = {2017},
	note = {Publisher: Public Library of Science},
	keywords = {Language, Semantics, Syntax, Computational linguistics, Lexical semantics, Lexicons, Natural language, Taxonomy},
	pages = {e0181987},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\9M24TRLU\\Lestrade - 2017 - Unzipping Zipf’s law.pdf:application/pdf},
}

@article{thurner_introduction_2018,
	title = {Introduction to the {Theory} of {Complex} {Systems}},
	url = {https://academic.oup.com/book/25504?sid=oup:oxfordacademic&genre=book&aulast=Thurner&aufirst=Stefan&title=Introduction+to+the+Theory+of+Complex+Systems&date=2018-09-20},
	urldate = {2023-10-13},
	author = {Thurner, Stefan and Klimek, Peter and Hanel, Rudolf},
	year = {2018},
}

@article{corominas-murtra_universality_2010,
	title = {Universality of {Zipf}'s law},
	volume = {82},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.82.011102},
	doi = {10.1103/PhysRevE.82.011102},
	abstract = {Zipf’s law is the most common statistical distribution displaying scaling behavior. Cities, populations or firms are just examples of this seemingly universal law. Although many different models have been proposed, no general theoretical explanation has been shown to exist for its universality. Here, we show that Zipf’s law is, in fact, an inevitable outcome of a very general class of stochastic systems. Borrowing concepts from Algorithmic Information Theory, our derivation is based on the properties of the symbolic sequence obtained through successive observations over a system with an ubounded number of possible states. Specifically, we assume that the complexity of the description of the system provided by the sequence of observations is the one expected for a system evolving to a stable state between order and disorder. This result is obtained from a small set of mild, physically relevant assumptions. The general nature of our derivation and its model-free basis would explain the ubiquity of such a law in real systems.},
	number = {1},
	urldate = {2023-10-13},
	journal = {Physical Review E},
	author = {Corominas-Murtra, Bernat and Solé, Ricard V.},
	month = jul,
	year = {2010},
	note = {Publisher: American Physical Society},
	pages = {011102},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\W2WBMP8P\\Corominas-Murtra and Solé - 2010 - Universality of Zipf's law.pdf:application/pdf},
}

@article{corominas-murtra_understanding_2015,
	title = {Understanding scaling through history-dependent processes with collapsing sample space},
	volume = {112},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1420946112},
	doi = {10.1073/pnas.1420946112},
	abstract = {Significance
            Many complex systems reduce their flexibility over time in the sense that the number of options (possible states) diminishes over time. We show that rank distributions of the visits to these states that emerge from such processes are exact power laws with an exponent −1 (Zipf’s law). When noise is added to such processes, meaning that from time to time they can also increase the number of their options, the rank distribution remains a power law, with an exponent that is related to the noise level in a remarkably simple way. Sample-space-reducing processes provide a new route to understand the phenomenon of scaling and provide an alternative to the known mechanisms of self-organized criticality, multiplicative processes, or preferential attachment.
          , 
            
              History-dependent processes are ubiquitous in natural and social systems. Many such stochastic processes, especially those that are associated with complex systems, become more constrained as they unfold, meaning that their sample space, or their set of possible outcomes, reduces as they age. We demonstrate that these sample-space-reducing (SSR) processes necessarily lead to Zipf’s law in the rank distributions of their outcomes. We show that by adding noise to SSR processes the corresponding rank distributions remain exact power laws,
              
                
                  
                    p
                    
                      (
                      x
                      )
                    
                    ∼
                    
                      x
                      
                        −
                        λ
                      
                    
                  
                
              
              , where the exponent directly corresponds to the mixing ratio of the SSR process and noise. This allows us to give a precise meaning to the scaling exponent in terms of the degree to which a given process reduces its sample space as it unfolds. Noisy SSR processes further allow us to explain a wide range of scaling exponents in frequency distributions ranging from
              
                
                  
                    α
                    =
                    2
                  
                
              
              to
              
                
                  ∞
                
              
              . We discuss several applications showing how SSR processes can be used to understand Zipf’s law in word frequencies, and how they are related to diffusion processes in directed networks, or aging processes such as in fragmentation processes. SSR processes provide a new alternative to understand the origin of scaling in complex systems without the recourse to multiplicative, preferential, or self-organized critical processes.},
	language = {en},
	number = {17},
	urldate = {2023-10-13},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Corominas-Murtra, Bernat and Hanel, Rudolf and Thurner, Stefan},
	month = apr,
	year = {2015},
	pages = {5348--5353},
	file = {Available Version (via Google Scholar):C\:\\Users\\Job Schepens\\Zotero\\storage\\6QDMEA6U\\pnas.html:text/html;Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\NIENKYRL\\Corominas-Murtra et al. - 2015 - Understanding scaling through history-dependent pr.pdf:application/pdf},
}

@article{thurner_understanding_2015,
	title = {Understanding {Zipf}'s law of word frequencies through sample-space collapse in sentence formation},
	volume = {12},
	issn = {1742-5689, 1742-5662},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsif.2015.0330},
	doi = {10.1098/rsif.2015.0330},
	abstract = {The formation of sentences is a highly structured and history-dependent process. The probability of using a specific word in a sentence strongly depends on the ‘history’ of word usage earlier in that sentence. We study a simple history-dependent model of text generation assuming that the sample-space of word usage reduces along sentence formation, on average. We first show that the model explains the approximate Zipf law found in word frequencies as a direct consequence of sample-space reduction. We then empirically quantify the amount of sample-space reduction in the sentences of 10 famous English books, by analysis of corresponding word-transition tables that capture which words can follow any given word in a text. We find a highly nested structure in these transition tables and show that this ‘nestedness’ is tightly related to the power law exponents of the observed word frequency distributions. With the proposed model, it is possible to understand that the nestedness of a text can be the origin of the actual scaling exponent and that deviations from the exact Zipf law can be understood by variations of the degree of nestedness on a book-by-book basis. On a theoretical level, we are able to show that in the case of weak nesting, Zipf's law breaks down in a fast transition. Unlike previous attempts to understand Zipf's law in language the sample-space reducing model is not based on assumptions of multiplicative, preferential or self-organized critical mechanisms behind language formation, but simply uses the empirically quantifiable parameter ‘nestedness’ to understand the statistics of word frequencies.},
	language = {en},
	number = {108},
	urldate = {2023-10-13},
	journal = {Journal of The Royal Society Interface},
	author = {Thurner, Stefan and Hanel, Rudolf and Liu, Bo and Corominas-Murtra, Bernat},
	month = jul,
	year = {2015},
	pages = {20150330},
}

@article{mollica_humans_2019,
	title = {Humans store about 1.5 megabytes of information during language acquisition},
	volume = {6},
	issn = {2054-5703},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.181393},
	doi = {10.1098/rsos.181393},
	abstract = {We introduce theory-neutral estimates of the amount of information learners possess about how language works. We provide estimates at several levels of linguistic analysis: phonemes, wordforms, lexical semantics, word frequency and syntax. Our best guess is that the average English-speaking adult has learned 12.5 million bits of information, the majority of which is lexical semantics. Interestingly, very little of this information is syntactic, even in our upper bound analyses. Generally, our results suggest that learners possess remarkable inferential mechanisms capable of extracting, on average, nearly 2000 bits of information about how language works
              each day
              for 18 years.},
	language = {en},
	number = {3},
	urldate = {2023-10-12},
	journal = {Royal Society Open Science},
	author = {Mollica, Francis and Piantadosi, Steven T.},
	month = mar,
	year = {2019},
	pages = {181393},
	file = {Available Version (via Google Scholar):C\:\\Users\\Job Schepens\\Zotero\\storage\\SR9Y7NR2\\Mollica and Piantadosi - 2019 - Humans store about 1.5 megabytes of information du.pdf:application/pdf},
}

@article{schmidt_uncontrolled_2021,
	title = {Uncontrolled corpus composition drives an apparent surge in cognitive distortions},
	volume = {118},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.2115010118},
	doi = {10.1073/pnas.2115010118},
	number = {45},
	urldate = {2023-10-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Schmidt, Benjamin and Piantadosi, Steven T. and Mahowald, Kyle},
	month = nov,
	year = {2021},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2115010118},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\3LTTM9SS\\Schmidt et al. - 2021 - Uncontrolled corpus composition drives an apparent.pdf:application/pdf},
}

@article{yang_one_2022,
	title = {One model for the learning of language},
	volume = {119},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.2021865119},
	doi = {10.1073/pnas.2021865119},
	abstract = {Significance
            It has long been hypothesized that language acquisition may be impossible without innate knowledge of the structures that occur in natural language. Here, we show that a domain general learning setup, originally developed in cognitive psychology to model rule learning, is able to acquire key pieces of natural language from relatively few examples of sentences. This develops a new approach to formalizing linguistic learning and highlights some features of language and language acquisition that may arise from general cognitive processes.
          , 
            
              A major goal of linguistics and cognitive science is to understand what class of learning systems can acquire natural language. Until recently, the computational requirements of language have been used to argue that learning is impossible without a highly constrained hypothesis space. Here, we describe a learning system that is maximally unconstrained, operating over the space of all computations, and is able to acquire many of the key structures present in natural language from positive evidence alone. We demonstrate this by providing the same learning model with data from 74 distinct formal languages which have been argued to capture key features of language, have been studied in experimental work, or come from an interesting complexity class. The model is able to successfully induce the latent system generating the observed strings from small amounts of evidence in almost all cases, including for regular (e.g.,
              
                a
                n
              
              ,
              
                
                  
                    
                      
                        (
                        a
                        b
                        )
                      
                      n
                    
                  
                
              
              , and
              
                
                  
                    
                      
                        \{\vphantom{\}}
                        a
                        ,
                        b
                        \vphantom{\{}\}
                      
                      +
                    
                  
                
              
              ), context-free (e.g.,
              
                
                  
                    
                      a
                      n
                    
                    
                      b
                      n
                    
                    ,
                     
                    
                      a
                      n
                    
                    
                      b
                      
                        n
                        +
                        m
                      
                    
                  
                
              
              , and
              
                
                  
                    x
                    
                      x
                      R
                    
                  
                
              
              ), and context-sensitive (e.g.,
              
                
                  
                    
                      a
                      n
                    
                    
                      b
                      n
                    
                    
                      c
                      n
                    
                    ,
                     
                    
                      a
                      n
                    
                    
                      b
                      m
                    
                    
                      c
                      n
                    
                    
                      d
                      m
                    
                  
                
              
              , and
              xx
              ) languages, as well as for many languages studied in learning experiments. These results show that relatively small amounts of positive evidence can support learning of rich classes of generative computations over structures. The model provides an idealized learning setup upon which additional cognitive constraints and biases can be formalized.},
	language = {en},
	number = {5},
	urldate = {2023-10-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yang, Yuan and Piantadosi, Steven T.},
	month = feb,
	year = {2022},
	pages = {e2021865119},
	file = {Available Version (via Google Scholar):C\:\\Users\\Job Schepens\\Zotero\\storage\\BNTC6XTM\\pnas.html:text/html;Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\5CT8SHI2\\Yang and Piantadosi - 2022 - One model for the learning of language.pdf:application/pdf},
}

@article{holdaway_stochastic_2022,
	title = {Stochastic {Time}‐{Series} {Analyses} {Highlight} the {Day}‐{To}‐{Day} {Dynamics} of {Lexical} {Frequencies}},
	volume = {46},
	issn = {0364-0213, 1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cogs.13215},
	doi = {10.1111/cogs.13215},
	abstract = {Abstract
            Standard models in quantitative linguistics assume that word usage follows a fixed frequency distribution, often Zipf's law or a close relative. This view, however, does not capture the near daily variations in topics of conversation, nor the short‐term dynamics of language change. In order to understand the dynamics of human language use, we present a corpus of daily word frequency variation scraped from online news sources every 20 min for more than 2 years. We construct a simple time‐varying model with a latent state, which is observed via word frequency counts. We use Bayesian techniques to infer the parameters of this model for 20,000 words, allowing us to convert complex word‐frequency trajectories into low‐dimensional parameters in word usage. By analyzing the inferred parameters of this model, we quantify the relative mobility and drift of words on a day‐to‐day basis, while accounting for sampling error. We quantify this variation and show evidence against “rich‐get‐richer” models of word use, which have been previously hypothesized to explain statistical patterns in language.},
	language = {en},
	number = {12},
	urldate = {2023-10-12},
	journal = {Cognitive Science},
	author = {Holdaway, Cameron and Piantadosi, Steven T.},
	month = dec,
	year = {2022},
	pages = {e13215},
}

@misc{yiu_imitation_2023,
	title = {Imitation versus {Innovation}: {What} children can do that large language and language-and-vision models cannot (yet)?},
	shorttitle = {Imitation versus {Innovation}},
	url = {http://arxiv.org/abs/2305.07666},
	doi = {10.48550/arXiv.2305.07666},
	abstract = {Much discussion about large language models and language-and-vision models has focused on whether these models are intelligent agents. We present an alternative perspective. We argue that these artificial intelligence models are cultural technologies that enhance cultural transmission in the modern world, and are efficient imitation engines. We explore what AI models can tell us about imitation and innovation by evaluating their capacity to design new tools and discover novel causal structures, and contrast their responses with those of human children. Our work serves as a first step in determining which particular representations and competences, as well as which kinds of knowledge or skill, can be derived from particular learning techniques and data. Critically, our findings suggest that machines may need more than large scale language and images to achieve what a child can do.},
	urldate = {2023-10-12},
	publisher = {arXiv},
	author = {Yiu, Eunice and Kosoy, Eliza and Gopnik, Alison},
	month = may,
	year = {2023},
	note = {arXiv:2305.07666 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\Y97JSJDM\\Yiu et al. - 2023 - Imitation versus Innovation What children can do .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\UGLI3PCE\\2305.html:text/html},
}

@article{li_computational_nodate,
	title = {Computational {Modeling} of {Bilingual} {Language} {Learning}: {Current} {Models} and {Future} {Directions}},
	volume = {n/a},
	copyright = {© 2022 The Authors. Language Learning published by Wiley Periodicals LLC on behalf of Language Learning Research Club, University of Michigan.},
	issn = {1467-9922},
	shorttitle = {Computational {Modeling} of {Bilingual} {Language} {Learning}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/lang.12529},
	doi = {10.1111/lang.12529},
	abstract = {The last two decades have seen a significant amount of interest in bilingual language learning and processing. A number of computational models have also been developed to account for bilingualism, with varying degrees of success. In this article, we first briefly introduce the significance of computational approaches to bilingual language learning, along with a discussion of the major contributions of current models, their implications, and their limitations. We show that the current models have contributed to progress in understanding the bilingual mind, but significant gaps exist. We advocate a new research agenda integrating progress across different disciplines, such as computational neuroscience, natural language processing, and first language acquisition, to construct a pluralist computational account that combines high-level cognitive theories and neurobiological foundations for bilingual language learning. We outline the contributions and promises of this interdisciplinary approach in which we view bilingual language learning as a dynamic, interactive, and developmental process.},
	language = {en},
	number = {n/a},
	urldate = {2023-10-12},
	journal = {Language Learning},
	author = {Li, Ping and Xu, Qihui},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/lang.12529},
	keywords = {bilingualism, computational modeling, language learning},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\W3F4V7ZQ\\Li and Xu - Computational Modeling of Bilingual Language Learn.pdf:application/pdf;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\48IQ9P5W\\lang.html:text/html},
}

@article{dijkstra_inventing_nodate,
	title = {Inventing and {Reinventing} the {Cog}: {A} {Commentary} on “{Computational} {Modeling} of {Bilingual} {Language} {Learning}: {Current} {Models} and {Future} {Directions}”},
	volume = {n/a},
	copyright = {© 2022 The Authors. Language Learning published by Wiley Periodicals LLC on behalf of Language Learning Research Club, University of Michigan.},
	issn = {1467-9922},
	shorttitle = {Inventing and {Reinventing} the {Cog}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/lang.12532},
	doi = {10.1111/lang.12532},
	language = {en},
	number = {n/a},
	urldate = {2023-10-12},
	journal = {Language Learning},
	author = {Dijkstra, Ton and van Heuven, Walter J. B.},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/lang.12532},
	keywords = {multilingualism, modeling, word learning},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\EZURXFX7\\Dijkstra and van Heuven - Inventing and Reinventing the Cog A Commentary on.pdf:application/pdf;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\AHW25XDC\\lang.html:text/html},
}

@article{xu_computational_nodate,
	title = {Computational {Modeling} of {Language} {Learning} in the {Era} of {Generative} {Artificial} {Intelligence}: {A} {Response} to {Open} {Peer} {Commentaries}},
	volume = {n/a},
	copyright = {© 2023 Language Learning Research Club, University of Michigan.},
	issn = {1467-9922},
	shorttitle = {Computational {Modeling} of {Language} {Learning} in the {Era} of {Generative} {Artificial} {Intelligence}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/lang.12605},
	doi = {10.1111/lang.12605},
	language = {en},
	number = {n/a},
	urldate = {2023-10-12},
	journal = {Language Learning},
	author = {Xu, Qihui and Li, Ping},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/lang.12605},
	keywords = {bilingualism, computational modeling, large language models, cross-disciplinary integration, generative AI},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\ZSITS97T\\Xu and Li - Computational Modeling of Language Learning in the.pdf:application/pdf;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\RIW9NZXV\\lang.html:text/html},
}

@article{corral_distinct_2020,
	title = {Distinct flavors of {Zipf}'s law and its maximum likelihood fitting: {Rank}-size and size-distribution representations},
	volume = {102},
	issn = {2470-0045, 2470-0053},
	shorttitle = {Distinct flavors of {Zipf}'s law and its maximum likelihood fitting},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.102.052113},
	doi = {10.1103/PhysRevE.102.052113},
	language = {en},
	number = {5},
	urldate = {2023-10-12},
	journal = {Physical Review E},
	author = {Corral, Álvaro and Serra, Isabel and Ferrer-i-Cancho, Ramon},
	month = nov,
	year = {2020},
	pages = {052113},
	file = {Available Version (via Google Scholar):C\:\\Users\\Job Schepens\\Zotero\\storage\\KCQJDBC9\\Corral et al. - 2020 - Distinct flavors of Zipf's law and its maximum lik.pdf:application/pdf},
}

@article{ferrer-i-cancho_optimal_2022,
	title = {Optimal {Coding} and the {Origins} of {Zipfian} {Laws}},
	volume = {29},
	issn = {0929-6174, 1744-5035},
	url = {https://www.tandfonline.com/doi/full/10.1080/09296174.2020.1778387},
	doi = {10.1080/09296174.2020.1778387},
	language = {en},
	number = {2},
	urldate = {2023-10-12},
	journal = {Journal of Quantitative Linguistics},
	author = {Ferrer-i-Cancho, Ramon and Bentz, Christian and Seguin, Caio},
	month = apr,
	year = {2022},
	pages = {165--194},
	file = {Available Version (via Google Scholar):C\:\\Users\\Job Schepens\\Zotero\\storage\\F77GNNL6\\Ferrer-i-Cancho et al. - 2022 - Optimal Coding and the Origins of Zipfian Laws.pdf:application/pdf},
}

@article{piantadosi_zipfs_2014,
	title = {Zipf’s word frequency law in natural language: {A} critical review and future directions},
	volume = {21},
	issn = {1531-5320},
	shorttitle = {Zipf’s word frequency law in natural language},
	url = {https://doi.org/10.3758/s13423-014-0585-6},
	doi = {10.3758/s13423-014-0585-6},
	abstract = {The frequency distribution of words has been a key object of study in statistical linguistics for the past 70 years. This distribution approximately follows a simple mathematical form known as Zipf’s law. This article first shows that human language has a highly complex, reliable structure in the frequency distribution over and above this classic law, although prior data visualization methods have obscured this fact. A number of empirical phenomena related to word frequencies are then reviewed. These facts are chosen to be informative about the mechanisms giving rise to Zipf’s law and are then used to evaluate many of the theoretical explanations of Zipf’s law in language. No prior account straightforwardly explains all the basic facts or is supported with independent evaluation of its underlying assumptions. To make progress at understanding why language obeys Zipf’s law, studies must seek evidence beyond the law itself, testing assumptions and evaluating novel predictions with new, independent data.},
	language = {en},
	number = {5},
	urldate = {2023-10-12},
	journal = {Psychonomic Bulletin \& Review},
	author = {Piantadosi, Steven T.},
	month = oct,
	year = {2014},
	keywords = {Statistics, Language, Zipf’s law},
	pages = {1112--1130},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\FCFN7PJW\\Piantadosi - 2014 - Zipf’s word frequency law in natural language A c.pdf:application/pdf},
}

@misc{ziems_can_2023,
	title = {Can {Large} {Language} {Models} {Transform} {Computational} {Social} {Science}?},
	url = {http://arxiv.org/abs/2305.03514},
	doi = {10.48550/arXiv.2305.03514},
	abstract = {Large Language Models (LLMs) like ChatGPT are capable of successfully performing many language processing tasks zero-shot (without the need for training data). If this capacity also applies to the coding of social phenomena like persuasiveness and political ideology, then LLMs could effectively transform Computational Social Science (CSS). This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 24 representative CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that today's LLMs can radically augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the hidden meaning behind text). In summary, LLMs can significantly reduce costs and increase efficiency of social science analysis in partnership with humans.},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
	month = apr,
	year = {2023},
	note = {arXiv:2305.03514 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\LE392F2E\\Ziems et al. - 2023 - Can Large Language Models Transform Computational .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\D3WADM8E\\2305.html:text/html},
}

@article{gagl_eye_2022,
	title = {Eye movements during text reading align with the rate of speech production},
	volume = {6},
	number = {3},
	journal = {Nature human behaviour},
	author = {Gagl, Benjamin and Gregorova, Klara and Golch, Julius and Hawelka, Stefan and Sassenhagen, Jona and Tavano, Alessandro and Poeppel, David and Fiebach, Christian J},
	year = {2022},
	note = {Publisher: Nature Publishing Group UK London},
	pages = {429--442},
}

@article{blasi_over-reliance_2022,
	title = {Over-reliance on {English} hinders cognitive science},
	volume = {26},
	number = {12},
	journal = {Trends in cognitive sciences},
	author = {Blasi, Damián E and Henrich, Joseph and Adamou, Evangelia and Kemmerer, David and Majid, Asifa},
	year = {2022},
	note = {Publisher: Elsevier},
	pages = {1153--1170},
}

@article{gagl_investigating_2023,
  title={Investigating lexical categorization in reading based on joint diagnostic and training approaches for language learners},
  author={Gagl, Benjamin and Gregorov{\'a}, Klara},
  journal={npj Science of Learning},
  volume={9},
  number={1},
  pages={29},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{kliegl_length_2004,
	title = {Length, frequency, and predictability effects of words on eye movements in reading},
	volume = {16},
	number = {1-2},
	journal = {European journal of cognitive psychology},
	author = {Kliegl, Reinhold and Grabner, Ellen and Rolfs, Martin and Engbert, Ralf},
	year = {2004},
	note = {Publisher: Taylor \& Francis},
	pages = {262--284},
}

@article{hawelka_dual-route_2010,
	title = {A dual-route perspective on eye movements of dyslexic readers},
	volume = {115},
	number = {3},
	journal = {Cognition},
	author = {Hawelka, Stefan and Gagl, Benjamin and Wimmer, Heinz},
	year = {2010},
	note = {Publisher: Elsevier},
	pages = {367--379},
}

@article{hawelka_forward_2015,
	title = {On forward inferences of fast and slow readers. {An} eye movement study},
	volume = {5},
	number = {1},
	journal = {Scientific reports},
	author = {Hawelka, Stefan and Schuster, Sarah and Gagl, Benjamin and Hutzler, Florian},
	year = {2015},
	note = {Publisher: Nature Publishing Group UK London},
	pages = {8432},
}

@article{staub_effect_2015,
	title = {The effect of lexical predictability on eye movements in reading: {Critical} review and theoretical interpretation},
	volume = {9},
	number = {8},
	journal = {Language and Linguistics Compass},
	author = {Staub, Adrian},
	year = {2015},
	note = {Publisher: Wiley Online Library},
	pages = {311--327},
}

@article{weekes_effects_2006,
	title = {Effects of consistency and age of acquisition on reading and spelling among developing readers},
	volume = {19},
	journal = {Reading and Writing},
	author = {Weekes, Brendan S and Castles, Anne E and Davies, Robert A},
	year = {2006},
	note = {Publisher: Springer},
	pages = {133--169},
}

@article{gagl_sources_2015,
	title = {On sources of the word length effect in young readers},
	volume = {19},
	number = {4},
	journal = {Scientific Studies of Reading},
	author = {Gagl, Benjamin and Hawelka, Stefan and Wimmer, Heinz},
	year = {2015},
	note = {Publisher: Taylor \& Francis},
	pages = {289--306},
}

@article{marinus_variability_2010,
	title = {Variability in the word-reading performance of dyslexic readers: {Effects} of letter length, phoneme length and digraph presence},
	volume = {46},
	number = {10},
	journal = {Cortex},
	author = {Marinus, Eva and de Jong, Peter F},
	year = {2010},
	note = {Publisher: Elsevier},
	pages = {1259--1271},
}

@article{zoccolotti_word_2005,
	title = {Word length effect in early reading and in developmental dyslexia},
	volume = {93},
	number = {3},
	journal = {Brain and language},
	author = {Zoccolotti, Pierluigi and De Luca, Maria and Di Pace, Enrico and Gasperini, Filippo and Judica, Anna and Spinelli, Donatella},
	year = {2005},
	note = {Publisher: Elsevier},
	pages = {369--373},
}

@inproceedings{mcdonald_universal_2013,
	title = {Universal dependency annotation for multilingual parsing},
	booktitle = {Proceedings of the 51st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	author = {McDonald, Ryan and Nivre, Joakim and Quirmbach-Brundage, Yvonne and Goldberg, Yoav and Das, Dipanjan and Ganchev, Kuzman and Hall, Keith and Petrov, Slav and Zhang, Hao and Täckström, Oscar and {others}},
	year = {2013},
	pages = {92--97},
}

@article{hawelka_beyond_2013,
	title = {Beyond single syllables: the effect of first syllable frequency and orthographic similarity on eye movements during silent reading},
	volume = {28},
	number = {8},
	journal = {Language and Cognitive processes},
	author = {Hawelka, Stefan and Schuster, Sarah and Gagl, Benjamin and Hutzler, Florian},
	year = {2013},
	note = {Publisher: Taylor \& Francis},
	pages = {1134--1153},
}

@article{huestegge_oculomotor_2009,
	title = {Oculomotor and linguistic determinants of reading development: {A} longitudinal study},
	volume = {49},
	number = {24},
	journal = {Vision research},
	author = {Huestegge, Lynn and Radach, Ralph and Corbic, Daniel and Huestegge, Sujata M},
	year = {2009},
	note = {Publisher: Elsevier},
	pages = {2948--2959},
}

@misc{korochkina_children_2023,
	title = {The {Children} and {Young} {People}’s {Books} {Lexicon} ({CYP}-{LEX}): {A} large-scale lexical database of books read by children and young people in the {United} {Kingdom}},
	shorttitle = {The {Children} and {Young} {People}’s {Books} {Lexicon} ({CYP}-{LEX})},
	url = {https://osf.io/preprints/psyarxiv/nha8t/},
	doi = {10.31234/osf.io/nha8t},
	abstract = {This article introduces CYP-LEX, a large-scale lexical database derived from books popular among children and young people in the United Kingdom. CYP-LEX includes 1,200 books evenly distributed across three age bands (7–9, 10–12, 13+) and comprises over 70 million tokens and over 105,000 types. For each word in each age band, we provide its raw and Zipf-transformed frequencies, all parts-of-speech in which it occurs with raw frequency and lemma for each occurrence, and measures of count-based contextual diversity. Together and individually, the three CYP-LEX subcorpora contain substantially more words than any other publicly available database of books for primary and secondary school children. Most of these words are very low in frequency, and a substantial proportion of the words in each age band do not occur on British television. Although the three age bands share some very frequent words, they differ substantially regarding words that occur less frequently, and this pattern also holds at the level of individual books. Initial analyses of CYP-LEX illustrate why independent reading constitutes a challenge for children and young people, and they also underscore the importance of reading widely for the development of reading expertise. Overall, CYP-LEX provides unprecedented information into the nature of vocabulary in books that British children aged 7+ read, and is a highly valuable resource for those studying reading and language development.},
	language = {en-us},
	urldate = {2023-10-20},
	publisher = {PsyArXiv},
	author = {Korochkina, Maria and Marelli, Marco and Brysbaert, Marc and Rastle, Kathleen},
	month = oct,
	year = {2023},
	keywords = {Cognitive Psychology, Linguistics, Language, Social and Behavioral Sciences, word frequency, children's books, Developmental Psychology, Language Aquisition, lexical database, lexical statistics, Psycholinguistics and Neurolinguistics, reading, Text and Discourse},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\B9T69AQY\\Korochkina et al. - 2023 - The Children and Young People’s Books Lexicon (CYP.pdf:application/pdf},
}

@article{henrich_weirdest_2010,
	title = {The weirdest people in the world?},
	volume = {33},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/weirdest-people-inthe-world/BF84F7517D56AFF7B7EB58411A554C17},
	number = {2-3},
	urldate = {2023-10-20},
	journal = {Behavioral and brain sciences},
	author = {Henrich, Joseph and Heine, Steven J. and Norenzayan, Ara},
	year = {2010},
	note = {Publisher: Cambridge University Press},
	keywords = {Culture, evolutionary psychology, behavioral economics, cross-cultural research, cultural psychology, experiments, external validity, generalizability, human universals, population variability},
	pages = {61--83},
	file = {Cambridge Journals Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\SU3C53AU\\displayAbstract.html:text/html;The WEIRD Evolution of Human Psychology | The Primate Diaries, Scientific American Blog Network:C\:\\Users\\Job Schepens\\Zotero\\storage\\V3UES3IZ\\the-weird-evolution-of-human-psychology.html:text/html},
}

@misc{schepens_can_2023,
	title = {Can we utilize {Large} {Language} {Models} ({LLMs}) to generate useful linguistic corpora? {A} case study of the word frequency effect in young {German} readers},
	shorttitle = {Can we utilize {Large} {Language} {Models} ({LLMs}) to generate useful linguistic corpora?},
	url = {https://osf.io/preprints/psyarxiv/gm9b6/},
	doi = {10.31234/osf.io/gm9b6},
	abstract = {LLMs for generating linguistic corpora},
	language = {en-us},
	urldate = {2023-10-20},
	publisher = {PsyArXiv},
	author = {Schepens, Job and Marx, Nicole and Gagl, Benjamin},
	month = oct,
	year = {2023},
	keywords = {Cognitive Psychology, Linguistics, Applied Linguistics, Language, Social and Behavioral Sciences, LLM, word frequency, Developmental Psychology, Language Aquisition, lexical statistics, Psycholinguistics and Neurolinguistics, reading, Text and Discourse, children’s books, Computational Linguistics, First and Second Language Acquisition, linguistic corpora},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\N7AVCADC\\Schepens et al. - 2023 - Can we utilize Large Language Models (LLMs) to gen.pdf:application/pdf},
}

@inproceedings{de_varda_effects_2022,
	title = {The effects of surprisal across languages: {Results} from native and non-native reading},
	shorttitle = {The effects of surprisal across languages},
	url = {https://aclanthology.org/2022.findings-aacl.13/},
	urldate = {2023-10-26},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {AACL}-{IJCNLP} 2022},
	author = {De Varda, Andrea and Marelli, Marco},
	year = {2022},
	pages = {138--144},
	file = {Available Version (via Google Scholar):C\:\\Users\\Job Schepens\\Zotero\\storage\\USSNS8B9\\De Varda and Marelli - 2022 - The effects of surprisal across languages Results.pdf:application/pdf},
}

@incollection{schluter_comparing_2022,
	address = {Cambridge},
	title = {Comparing {Methods} for the {Evaluation} of {Cluster} {Structures} in {Multidimensional} {Analyses}: {Concessive} {Constructions} in {Varieties} of {English}},
	isbn = {978-1-108-49964-4},
	shorttitle = {Comparing {Methods} for the {Evaluation} of {Cluster} {Structures} in {Multidimensional} {Analyses}},
	url = {https://www.cambridge.org/core/books/data-and-methods-in-corpus-linguistics/comparing-methods-for-the-evaluation-of-cluster-structures-in-multidimensional-analyses/862D8C86BA26F0C2E4E1B7C3D2CC0A88},
	abstract = {This chapter sets out by discussing the way in which multidimensional techniques and visualizations have been used to analyse linguistic data. While, for instance, multidimensional scaling and unrooted phenograms (or NeighborNets) have primarily been designed for exploratory purposes, the author argues that they are in fact regularly used to put linguistic assumptions or hypotheses to the test. Cluster goodness (in terms of internal coherence and external distance from other clusters) in such approaches are typically evaluated based on a two-dimensional visualization. The author compares the affordances and limitations of visual inspection with a quantitative set of metrics that directly relates to visual displays but adds a degree of precision not attained by the human eye. The empirical part of the paper applies both approaches to a study of concessive constructions in six varieties of English, based on spoken and written material from the International Corpus of English. The author suggests that the new metrics can be usefully applied to a variety of multidimensional techniques to endow them with a measure of objectivity.},
	urldate = {2023-10-26},
	booktitle = {Data and {Methods} in {Corpus} {Linguistics}: {Comparative} {Approaches}},
	publisher = {Cambridge University Press},
	author = {Schützler, Ole},
	editor = {Schlüter, Julia and Schützler, Ole},
	year = {2022},
	doi = {10.1017/9781108589314.010},
	keywords = {cluster, dendrogram, distance, distance matrix, International Corpus of English, MDS, multidimensional data, NeighborNet},
	pages = {259--288},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\EGD2XUVH\\Schützler - 2022 - Comparing Methods for the Evaluation of Cluster St.pdf:application/pdf},
}

@incollection{schluter_comparing_2022-1,
	address = {Cambridge},
	title = {Comparing {Bayesian} and {Frequentist} {Models} of {Language} {Variation}: {The} {Case} of {Help} + (to-){Infinitive}},
	isbn = {978-1-108-49964-4},
	shorttitle = {Comparing {Bayesian} and {Frequentist} {Models} of {Language} {Variation}},
	url = {https://www.cambridge.org/core/books/data-and-methods-in-corpus-linguistics/comparing-bayesian-and-frequentist-models-of-language-variation/CE7171C38D5C9CD137430C41FF32E5CF},
	abstract = {This chapter compares standard frequentist and more recent Bayesian approaches to logistic regression analyses. Starting out from a multifactorial case study of the verb help complemented by either the bare infinitive or the to-infinitive, the key components and the main conceptual differences of frequentist and Bayesian inference are discussed. Conceptually, the Bayesian rationale of directly testing hypotheses on the effects of multiple factors on an outcome variable is argued to be preferable and more sensitive than the conventional approach of testing null hypotheses. On the practical side, Bayesian statistics enables the researcher to recycle and integrate the results of previous analyses based on different datasets as informative priors, which can help improve and stabilize statistical modelling. Recourse to prior research can thus produce synergies and reduce data preparation expense. In cases of data sparsity, it can by the same token enable researchers to analyse small samples. Bayesian methods are thus put forward as powerful tools for overcoming the limitations of isolated corpus studies and for promoting synergies between data collected by individual researchers.},
	urldate = {2023-10-26},
	booktitle = {Data and {Methods} in {Corpus} {Linguistics}: {Comparative} {Approaches}},
	publisher = {Cambridge University Press},
	author = {Levshina, Natalia},
	editor = {Schlüter, Julia and Schützler, Ole},
	year = {2022},
	doi = {10.1017/9781108589314.009},
	keywords = {Bayesian inference, confidence interval, credible interval, horror aequi, MCMC algorithm, posteriors, sample size},
	pages = {224--258},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\4RV7HV7Q\\Levshina - 2022 - Comparing Bayesian and Frequentist Models of Langu.pdf:application/pdf;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\QELNUNNH\\CE7171C38D5C9CD137430C41FF32E5CF.html:text/html},
}

@misc{tedeschi_whats_2023,
	title = {What's the {Meaning} of {Superhuman} {Performance} in {Today}'s {NLU}?},
	url = {http://arxiv.org/abs/2305.08414},
	doi = {10.48550/arXiv.2305.08414},
	abstract = {In the last five years, there has been a significant focus in Natural Language Processing (NLP) on developing larger Pretrained Language Models (PLMs) and introducing benchmarks such as SuperGLUE and SQuAD to measure their abilities in language understanding, reasoning, and reading comprehension. These PLMs have achieved impressive results on these benchmarks, even surpassing human performance in some cases. This has led to claims of superhuman capabilities and the provocative idea that certain tasks have been solved. In this position paper, we take a critical look at these claims and ask whether PLMs truly have superhuman abilities and what the current benchmarks are really evaluating. We show that these benchmarks have serious limitations affecting the comparison between humans and PLMs and provide recommendations for fairer and more transparent benchmarks.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Tedeschi, Simone and Bos, Johan and Declerck, Thierry and Hajic, Jan and Hershcovich, Daniel and Hovy, Eduard H. and Koller, Alexander and Krek, Simon and Schockaert, Steven and Sennrich, Rico and Shutova, Ekaterina and Navigli, Roberto},
	month = may,
	year = {2023},
	note = {arXiv:2305.08414 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\WLSVGFX5\\Tedeschi et al. - 2023 - What's the Meaning of Superhuman Performance in To.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\R2UH92U4\\2305.html:text/html},
}

@misc{choenni_data-efficient_2022,
	title = {Data-{Efficient} {Cross}-{Lingual} {Transfer} with {Language}-{Specific} {Subnetworks}},
	url = {http://arxiv.org/abs/2211.00106},
	doi = {10.48550/arXiv.2211.00106},
	abstract = {Large multilingual language models typically share their parameters across all languages, which enables cross-lingual task transfer, but learning can also be hindered when training updates from different languages are in conflict. In this paper, we propose novel methods for using language-specific subnetworks, which control cross-lingual parameter sharing, to reduce conflicts and increase positive transfer during fine-tuning. We introduce dynamic subnetworks, which are jointly updated with the model, and we combine our methods with meta-learning, an established, but complementary, technique for improving cross-lingual transfer. Finally, we provide extensive analyses of how each of our methods affects the models.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Choenni, Rochelle and Garrette, Dan and Shutova, Ekaterina},
	month = oct,
	year = {2022},
	note = {arXiv:2211.00106 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\3CKW9V7V\\Choenni et al. - 2022 - Data-Efficient Cross-Lingual Transfer with Languag.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\WJL8HVE4\\2211.html:text/html},
}

@article{choenni_cross-lingual_2023,
	title = {Cross-{Lingual} {Transfer} with {Language}-{Specific} {Subnetworks} for {Low}-{Resource} {Dependency} {Parsing}},
	issn = {0891-2017},
	url = {https://doi.org/10.1162/coli_a_00482},
	doi = {10.1162/coli_a_00482},
	abstract = {Large multilingual language models typically share their parameters across all languages, which enables cross-lingual task transfer, but learning can also be hindered when training updates from different languages are in conflict. In this article, we propose novel methods for using language-specific subnetworks, which control cross-lingual parameter sharing, to reduce conflicts and increase positive transfer during fine-tuning. We introduce dynamic subnetworks, which are jointly updated with the model, and we combine our methods with meta-learning, an established, but complementary, technique for improving cross-lingual transfer. Finally, we provide extensive analyses of how each of our methods affects the models.},
	urldate = {2023-11-09},
	journal = {Computational Linguistics},
	author = {Choenni, Rochelle and Garrette, Dan and Shutova, Ekaterina},
	month = aug,
	year = {2023},
	pages = {1--29},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\WQZ7R8DB\\Choenni et al. - 2023 - Cross-Lingual Transfer with Language-Specific Subn.pdf:application/pdf;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\EWSZXAKG\\116157.html:text/html},
}

@misc{choenni_how_2023,
	title = {How do languages influence each other? {Studying} cross-lingual data sharing during {LLM} fine-tuning},
	shorttitle = {How do languages influence each other?},
	url = {http://arxiv.org/abs/2305.13286},
	doi = {10.48550/arXiv.2305.13286},
	abstract = {Multilingual large language models (MLLMs) are jointly trained on data from many different languages such that representation of individual languages can benefit from other languages' data. Impressive performance on zero-shot cross-lingual transfer shows that these models are capable of exploiting data from other languages. Yet, it remains unclear to what extent, and under which conditions, languages rely on each other's data. In this study, we use TracIn (Pruthi et al., 2020), a training data attribution (TDA) method, to retrieve the most influential training samples seen during multilingual fine-tuning for a particular test language. This allows us to analyse cross-lingual sharing mechanisms of MLLMs from a new perspective. While previous work studied cross-lingual sharing at the level of model parameters, we present the first approach to study cross-lingual sharing at the data level. We find that MLLMs rely on data from multiple languages from the early stages of fine-tuning and that this reliance gradually increases as fine-tuning progresses. We further study how different fine-tuning languages influence model performance on a given test language and find that they can both reinforce and complement the knowledge acquired from data of the test language itself.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Choenni, Rochelle and Garrette, Dan and Shutova, Ekaterina},
	month = may,
	year = {2023},
	note = {arXiv:2305.13286 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\FQY32LE6\\Choenni et al. - 2023 - How do languages influence each other Studying cr.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\9UBYYC2H\\2305.html:text/html},
}

@misc{starace_probing_2023,
	title = {Probing {LLMs} for {Joint} {Encoding} of {Linguistic} {Categories}},
	url = {http://arxiv.org/abs/2310.18696},
	doi = {10.48550/arXiv.2310.18696},
	abstract = {Large Language Models (LLMs) exhibit impressive performance on a range of NLP tasks, due to the general-purpose linguistic knowledge acquired during pretraining. Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM layers, with lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing. Yet, little is known about how encodings of different linguistic phenomena interact within the models and to what extent processing of linguistically-related categories relies on the same, shared model representations. In this paper, we propose a framework for testing the joint encoding of linguistic categories in LLMs. Focusing on syntax, we find evidence of joint encoding both at the same (related part-of-speech (POS) classes) and different (POS classes and related syntactic dependency relations) levels of linguistic hierarchy. Our cross-lingual experiments show that the same patterns hold across languages in multilingual LLMs.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Starace, Giulio and Papakostas, Konstantinos and Choenni, Rochelle and Panagiotopoulos, Apostolos and Rosati, Matteo and Leidinger, Alina and Shutova, Ekaterina},
	month = oct,
	year = {2023},
	note = {arXiv:2310.18696 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\4NNCSC48\\Starace et al. - 2023 - Probing LLMs for Joint Encoding of Linguistic Cate.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\6BTQLUET\\2310.html:text/html},
}

@article{choenni_investigating_2022,
	title = {Investigating {Language} {Relationships} in {Multilingual} {Sentence} {Encoders} {Through} the {Lens} of {Linguistic} {Typology}},
	volume = {48},
	issn = {0891-2017},
	url = {https://doi.org/10.1162/coli_a_00444},
	doi = {10.1162/coli_a_00444},
	abstract = {Multilingual sentence encoders have seen much success in cross-lingual model transfer for downstream NLP tasks. The success of this transfer is, however, dependent on the model’s ability to encode the patterns of cross-lingual similarity and variation. Yet, we know relatively little about the properties of individual languages or the general patterns of linguistic variation that the models encode. In this article, we investigate these questions by leveraging knowledge from the field of linguistic typology, which studies and documents structural and semantic variation across languages. We propose methods for separating language-specific subspaces within state-of-the-art multilingual sentence encoders (LASER, M-BERT, XLM, and XLM-R) with respect to a range of typological properties pertaining to lexical, morphological, and syntactic structure. Moreover, we investigate how typological information about languages is distributed across all layers of the models. Our results show interesting differences in encoding linguistic variation associated with different pretraining strategies. In addition, we propose a simple method to study how shared typological properties of languages are encoded in two state-of-the-art multilingual models—M-BERT and XLM-R. The results provide insight into their information-sharing mechanisms and suggest that these linguistic properties are encoded jointly across typologically similar languages in these models.},
	number = {3},
	urldate = {2023-11-09},
	journal = {Computational Linguistics},
	author = {Choenni, Rochelle and Shutova, Ekaterina},
	month = sep,
	year = {2022},
	pages = {635--672},
	file = {Available Version (via Google Scholar):C\:\\Users\\Job Schepens\\Zotero\\storage\\5F2QZGFL\\110573.html:text/html;Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\6S5YI86Z\\Choenni and Shutova - 2022 - Investigating Language Relationships in Multilingu.pdf:application/pdf;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\K34D34KS\\Investigating-Language-Relationships-in.html:text/html},
}

@misc{jumelet_transparency_2023,
	title = {Transparency at the {Source}: {Evaluating} and {Interpreting} {Language} {Models} {With} {Access} to the {True} {Distribution}},
	shorttitle = {Transparency at the {Source}},
	url = {http://arxiv.org/abs/2310.14840},
	doi = {10.48550/arXiv.2310.14840},
	abstract = {We present a setup for training, evaluating and interpreting neural language models, that uses artificial, language-like data. The data is generated using a massive probabilistic grammar (based on state-split PCFGs), that is itself derived from a large natural language corpus, but also provides us complete control over the generative process. We describe and release both grammar and corpus, and test for the naturalness of our generated data. This approach allows us to define closed-form expressions to efficiently compute exact lower bounds on obtainable perplexity using both causal and masked language modelling. Our results show striking differences between neural language modelling architectures and training objectives in how closely they allow approximating the lower bound on perplexity. Our approach also allows us to directly compare learned representations to symbolic rules in the underlying source. We experiment with various techniques for interpreting model behaviour and learning dynamics. With access to the underlying true source, our results show striking differences and outcomes in learning dynamics between different classes of words.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Jumelet, Jaap and Zuidema, Willem},
	month = oct,
	year = {2023},
	note = {arXiv:2310.14840 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\UEEMZAER\\Jumelet and Zuidema - 2023 - Transparency at the Source Evaluating and Interpr.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\BFTMAXTI\\2310.html:text/html},
}

@misc{van_der_wal_undesirable_2023,
	title = {Undesirable biases in {NLP}: {Addressing} challenges of measurement},
	shorttitle = {Undesirable biases in {NLP}},
	url = {http://arxiv.org/abs/2211.13709},
	doi = {10.48550/arXiv.2211.13709},
	abstract = {As Large Language Models and Natural Language Processing (NLP) technology rapidly develop and spread into daily life, it becomes crucial to anticipate how their use could harm people. One problem that has received a lot of attention in recent years is that this technology has displayed harmful biases, from generating derogatory stereotypes to producing disparate outcomes for different social groups. Although a lot of effort has been invested in assessing and mitigating these biases, our methods of measuring the biases of NLP models have serious problems and it is often unclear what they actually measure. In this paper, we provide an interdisciplinary approach to discussing the issue of NLP model bias by adopting the lens of psychometrics -- a field specialized in the measurement of concepts like bias that are not directly observable. In particular, we will explore two central notions from psychometrics, the {\textbackslash}emph\{construct validity\} and the {\textbackslash}emph\{reliability\} of measurement tools, and discuss how they can be applied in the context of measuring model bias. Our goal is to provide NLP practitioners with methodological tools for designing better bias measures, and to inspire them more generally to explore tools from psychometrics when working on bias measurement tools.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {van der Wal, Oskar and Bachmann, Dominik and Leidinger, Alina and van Maanen, Leendert and Zuidema, Willem and Schulz, Katrin},
	month = nov,
	year = {2023},
	note = {arXiv:2211.13709 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\HQGU9WAH\\van der Wal et al. - 2023 - Undesirable biases in NLP Addressing challenges o.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\6SE3HIL5\\2211.html:text/html},
}

@article{patterson_multilingual_2023,
	title = {Multilingual semantic distance: {Automatic} verbal creativity assessment in many languages},
	volume = {17},
	issn = {1931-390X},
	shorttitle = {Multilingual semantic distance},
	doi = {10.1037/aca0000618},
	abstract = {Creativity research commonly involves recruiting human raters to judge the originality of responses to divergent thinking tasks, such as the alternate uses task (AUT). These manual scoring practices have benefited the field, but they also have limitations, including labor-intensiveness and subjectivity, which can adversely impact the reliability and validity of assessments. To address these challenges, researchers are increasingly employing automatic scoring approaches, such as distributional models of semantic distance. However, semantic distance has primarily been studied in English-speaking samples, with very little research in the many other languages of the world. In a multilab study (N = 6,522 participants), we aimed to validate semantic distance on the AUT in 12 languages: Arabic, Chinese, Dutch, English, Farsi, French, German, Hebrew, Italian, Polish, Russian, and Spanish. We gathered AUT responses and human creativity ratings (N = 107,672 responses), as well as criterion measures for validation (e.g., creative achievement). We compared two deep learning-based semantic models—multilingual bidirectional encoder representations from transformers and cross-lingual language model RoBERTa—to compute semantic distance and validate this automated metric with human ratings and criterion measures. We found that the top-performing model for each language correlated positively with human creativity ratings, with correlations ranging from medium to large across languages. Regarding criterion validity, semantic distance showed small-to-moderate effect sizes (comparable to human ratings) for openness, creative behavior/achievement, and creative self-concept. We provide open access to our multilingual dataset for future algorithmic development, along with Python code to compute semantic distance in 12 languages. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
	number = {4},
	journal = {Psychology of Aesthetics, Creativity, and the Arts},
	author = {Patterson, John D. and Merseal, Hannah M. and Johnson, Dan R. and Agnoli, Sergio and Baas, Matthijs and Baker, Brendan S. and Barbot, Baptiste and Benedek, Mathias and Borhani, Khatereh and Chen, Qunlin and Christensen, Julia F. and Corazza, Giovanni Emanuele and Forthmann, Boris and Karwowski, Maciej and Kazemian, Nastaran and Kreisberg-Nitzav, Ariel and Kenett, Yoed N. and Link, Allison and Lubart, Todd and Mercier, Maxence and Miroshnik, Kirill and Ovando-Tellez, Marcela and Primi, Ricardo and Puente-Díaz, Rogelio and Said-Metwaly, Sameh and Stevenson, Claire and Vartanian, Meghedi and Volle, Emannuelle and van Hell, Janet G. and Beaty, Roger E.},
	year = {2023},
	note = {Place: US
Publisher: Educational Publishing Foundation},
	keywords = {Linguistics, Natural Language Processing, Semantics, Creativity, Creativity Measurement, Multilingualism, Rating, Test Validity},
	pages = {495--507},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\Q9FJ9ZUZ\\2024-07870-002.html:text/html},
}

@misc{stevenson_putting_2022,
	title = {Putting {GPT}-3's {Creativity} to the ({Alternative} {Uses}) {Test}},
	url = {http://arxiv.org/abs/2206.08932},
	doi = {10.48550/arXiv.2206.08932},
	abstract = {AI large language models have (co-)produced amazing written works from newspaper articles to novels and poetry. These works meet the standards of the standard definition of creativity: being original and useful, and sometimes even the additional element of surprise. But can a large language model designed to predict the next text fragment provide creative, out-of-the-box, responses that still solve the problem at hand? We put Open AI's generative natural language model, GPT-3, to the test. Can it provide creative solutions to one of the most commonly used tests in creativity research? We assessed GPT-3's creativity on Guilford's Alternative Uses Test and compared its performance to previously collected human responses on expert ratings of originality, usefulness and surprise of responses, flexibility of each set of ideas as well as an automated method to measure creativity based on the semantic distance between a response and the AUT object in question. Our results show that -- on the whole -- humans currently outperform GPT-3 when it comes to creative output. But, we believe it is only a matter of time before GPT-3 catches up on this particular task. We discuss what this work reveals about human and AI creativity, creativity testing and our definition of creativity.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Stevenson, Claire and Smal, Iris and Baas, Matthijs and Grasman, Raoul and van der Maas, Han},
	month = jun,
	year = {2022},
	note = {arXiv:2206.08932 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\CF9VESL5\\Stevenson et al. - 2022 - Putting GPT-3's Creativity to the (Alternative Use.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\VHDUJI85\\2206.html:text/html},
}

@article{van_dis_chatgpt_2023,
	title = {{ChatGPT}: five priorities for research},
	volume = {614},
	copyright = {2023 Springer Nature Limited},
	shorttitle = {{ChatGPT}},
	url = {https://www.nature.com/articles/d41586-023-00288-7},
	doi = {10.1038/d41586-023-00288-7},
	abstract = {Conversational AI is a game-changer for science. Here’s how to respond.},
	language = {en},
	number = {7947},
	urldate = {2023-11-09},
	journal = {Nature},
	author = {van Dis, Eva A. M. and Bollen, Johan and Zuidema, Willem and van Rooij, Robert and Bockting, Claudi L.},
	month = feb,
	year = {2023},
	note = {Bandiera\_abtest: a
Cg\_type: Comment
Number: 7947
Publisher: Nature Publishing Group
Subject\_term: Computer science, Research management, Publishing, Machine learning},
	keywords = {Computer science, Publishing, Research management, Machine learning},
	pages = {224--226},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\7ITG6M5G\\van Dis et al. - 2023 - ChatGPT five priorities for research.pdf:application/pdf;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\NJFUCNXK\\d41586-023-00288-7.html:text/html},
}

@article{rosenbusch_how_2023,
	title = {How {Accurate} are {GPT}-3’s {Hypotheses} {About} {Social} {Science} {Phenomena}?},
	volume = {2},
	issn = {2731-4669},
	url = {https://doi.org/10.1007/s44206-023-00054-2},
	doi = {10.1007/s44206-023-00054-2},
	abstract = {We test whether GPT-3 can accurately predict simple study outcomes in the social sciences. Ground truth outcomes were obtained by surveying 600 adult US citizens about their political attitudes. GPT-3 was prompted to predict the direction of the empirical inter-attitude correlations. Machine-generated hypotheses were accurate in 78\% (zero-shot), 94\% (five-shot and chained prompting), and 97\% (extensive finetuning) of cases. Positive and negative correlations were balanced in the ground truth data. These results encourage the development of hypothesis engines for more challenging contexts. Moreover, they highlight the importance of addressing the numerous ethical and philosophical challenges that arise with hypothesis automation. While future hypothesis engines could potentially compete with human researchers in terms of empirical accuracy, they have inherent drawbacks that preclude full automations for the foreseeable future.},
	language = {en},
	number = {2},
	urldate = {2023-11-09},
	journal = {Digital Society},
	author = {Rosenbusch, Hannes and Stevenson, Claire E. and van der Maas, Han L. J.},
	month = jul,
	year = {2023},
	keywords = {Attitudes, GPT-3, Hypotheses, Meta science, Research automation, Social science},
	pages = {26},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\WCVLZX2R\\Rosenbusch et al. - 2023 - How Accurate are GPT-3’s Hypotheses About Social S.pdf:application/pdf},
}

@misc{stevenson_large_2023,
	title = {Do large language models solve verbal analogies like children do?},
	url = {http://arxiv.org/abs/2310.20384},
	doi = {10.48550/arXiv.2310.20384},
	abstract = {Analogy-making lies at the heart of human cognition. Adults solve analogies such as {\textbackslash}textit\{Horse belongs to stable like chicken belongs to ...?\} by mapping relations ({\textbackslash}textit\{kept in\}) and answering {\textbackslash}textit\{chicken coop\}. In contrast, children often use association, e.g., answering {\textbackslash}textit\{egg\}. This paper investigates whether large language models (LLMs) solve verbal analogies in A:B::C:? form using associations, similar to what children do. We use verbal analogies extracted from an online adaptive learning environment, where 14,002 7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six tested Dutch monolingual and multilingual LLMs performed around the same level as children, with MGPT performing worst, around the 7-year-old level, and XLM-V and GPT-3 the best, slightly above the 11-year-old level. However, when we control for associative processes this picture changes and each model's performance level drops 1-2 years. Further experiments demonstrate that associative processes often underlie correctly solved analogies. We conclude that the LLMs we tested indeed tend to solve verbal analogies by association with C like children do.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Stevenson, Claire E. and ter Veen, Mathilde and Choenni, Rochelle and van der Maas, Han L. J. and Shutova, Ekaterina},
	month = oct,
	year = {2023},
	note = {arXiv:2310.20384 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\NL9623DR\\Stevenson et al. - 2023 - Do large language models solve verbal analogies li.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\9CVLLM8C\\2310.html:text/html},
}

@misc{adams_sparse_2023,
	title = {From {Sparse} to {Dense}: {GPT}-4 {Summarization} with {Chain} of {Density} {Prompting}},
	shorttitle = {From {Sparse} to {Dense}},
	url = {http://arxiv.org/abs/2309.04269},
	doi = {10.48550/arXiv.2309.04269},
	abstract = {Selecting the ``right'' amount of information to include in a summary is a difficult task. A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a ``Chain of Density'' (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries generated by CoD are more abstractive, exhibit more fusion, and have less of a lead bias than GPT-4 summaries generated by a vanilla prompt. We conduct a human preference study on 100 CNN DailyMail articles and find that that humans prefer GPT-4 summaries that are more dense than those generated by a vanilla prompt and almost as dense as human written summaries. Qualitative analysis supports the notion that there exists a tradeoff between informativeness and readability. 500 annotated CoD summaries, as well as an extra 5,000 unannotated summaries, are freely available on HuggingFace (https://huggingface.co/datasets/griffin/chain\_of\_density).},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Adams, Griffin and Fabbri, Alexander and Ladhak, Faisal and Lehman, Eric and Elhadad, Noémie},
	month = sep,
	year = {2023},
	note = {arXiv:2309.04269 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\DCW2H9LD\\Adams et al. - 2023 - From Sparse to Dense GPT-4 Summarization with Cha.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\MU66FSD5\\2309.html:text/html},
}

@misc{pu_chatgpt_2023,
	title = {{ChatGPT} vs {Human}-authored {Text}: {Insights} into {Controllable} {Text} {Summarization} and {Sentence} {Style} {Transfer}},
	shorttitle = {{ChatGPT} vs {Human}-authored {Text}},
	url = {http://arxiv.org/abs/2306.07799},
	doi = {10.48550/arXiv.2306.07799},
	abstract = {Large-scale language models, like ChatGPT, have garnered significant media attention and stunned the public with their remarkable capacity for generating coherent text from short natural language prompts. In this paper, we aim to conduct a systematic inspection of ChatGPT's performance in two controllable generation tasks, with respect to ChatGPT's ability to adapt its output to different target audiences (expert vs. layman) and writing styles (formal vs. informal). Additionally, we evaluate the faithfulness of the generated text, and compare the model's performance with human-authored texts. Our findings indicate that the stylistic variations produced by humans are considerably larger than those demonstrated by ChatGPT, and the generated texts diverge from human samples in several characteristics, such as the distribution of word types. Moreover, we observe that ChatGPT sometimes incorporates factual errors or hallucinations when adapting the text to suit a specific style.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Pu, Dongqi and Demberg, Vera},
	month = jun,
	year = {2023},
	note = {arXiv:2306.07799 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\P6YGQC7B\\Pu and Demberg - 2023 - ChatGPT vs Human-authored Text Insights into Cont.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\BWH5NJKK\\2306.html:text/html},
}

@misc{zhang_can_2023,
	title = {Can {Language} {Models} {Be} {Tricked} by {Language} {Illusions}? {Easier} with {Syntax}, {Harder} with {Semantics}},
	shorttitle = {Can {Language} {Models} {Be} {Tricked} by {Language} {Illusions}?},
	url = {http://arxiv.org/abs/2311.01386},
	doi = {10.48550/arXiv.2311.01386},
	abstract = {Language models (LMs) have been argued to overlap substantially with human beings in grammaticality judgment tasks. But when humans systematically make errors in language processing, should we expect LMs to behave like cognitive models of language and mimic human behavior? We answer this question by investigating LMs' more subtle judgments associated with "language illusions" -- sentences that are vague in meaning, implausible, or ungrammatical but receive unexpectedly high acceptability judgments by humans. We looked at three illusions: the comparative illusion (e.g. "More people have been to Russia than I have"), the depth-charge illusion (e.g. "No head injury is too trivial to be ignored"), and the negative polarity item (NPI) illusion (e.g. "The hunter who no villager believed to be trustworthy will ever shoot a bear"). We found that probabilities represented by LMs were more likely to align with human judgments of being "tricked" by the NPI illusion which examines a structural dependency, compared to the comparative and the depth-charge illusions which require sophisticated semantic understanding. No single LM or metric yielded results that are entirely consistent with human behavior. Ultimately, we show that LMs are limited both in their construal as cognitive models of human language processing and in their capacity to recognize nuanced but critical information in complicated language materials.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Zhang, Yuhan and Gibson, Edward and Davis, Forrest},
	month = nov,
	year = {2023},
	note = {arXiv:2311.01386 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\BH4H6PM4\\Zhang et al. - 2023 - Can Language Models Be Tricked by Language Illusio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\WHTSSHZM\\2311.html:text/html},
}

@article{chomsky_opinion_2023,
	chapter = {Opinion},
	title = {Opinion {\textbar} {Noam} {Chomsky}: {The} {False} {Promise} of {ChatGPT}},
	issn = {0362-4331},
	shorttitle = {Opinion {\textbar} {Noam} {Chomsky}},
	url = {https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html},
	abstract = {The most prominent strain of A.I. encodes a flawed conception of language and knowledge.},
	language = {en-US},
	urldate = {2023-11-09},
	journal = {The New York Times},
	author = {Chomsky, Noam and Roberts, Ian and Watumull, Jeffrey},
	month = mar,
	year = {2023},
	keywords = {Philosophy, Computers and the Internet, Artificial Intelligence, ChatGPT, Ethics (Personal), internal-sub-only, Language and Languages},
}

@article{noauthor_constraint_2023,
	title = {Constraint satisfaction in large language models},
	url = {https://osf.io/https://osf.io/kjd63},
	abstract = {Presented by OSF},
	language = {en-us},
	urldate = {2023-11-09},
	month = oct,
	year = {2023},
	note = {Publisher: Open Science Framework},
	file = {manuscript.docx:C\:\\Users\\Job Schepens\\Zotero\\storage\\IBSSB46A\\manuscript.docx:application/vnd.openxmlformats-officedocument.wordprocessingml.document;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\QNRWMJJ7\\kjd63.html:text/html},
}

@inproceedings{hollenstein_cmcl_2022,
	address = {Dublin, Ireland},
	title = {{CMCL} 2022 {Shared} {Task} on {Multilingual} and {Crosslingual} {Prediction} of {Human} {Reading} {Behavior}},
	url = {https://aclanthology.org/2022.cmcl-1.14},
	doi = {10.18653/v1/2022.cmcl-1.14},
	abstract = {We present the second shared task on eye-tracking data prediction of the Cognitive Modeling and Computational Linguistics Workshop (CMCL). Differently from the previous edition, participating teams are asked to predict eye-tracking features from multiple languages, including a surprise language for which there were no available training data. Moreover, the task also included the prediction of standard deviations of feature values in order to account for individual differences between readers.A total of six teams registered to the task. For the first subtask on multilingual prediction, the winning team proposed a regression model based on lexical features, while for the second subtask on cross-lingual prediction, the winning team used a hybrid model based on a multilingual transformer embeddings as well as statistical features.},
	urldate = {2023-11-09},
	booktitle = {Proceedings of the {Workshop} on {Cognitive} {Modeling} and {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Hollenstein, Nora and Chersoni, Emmanuele and Jacobs, Cassandra and Oseki, Yohei and Prévot, Laurent and Santus, Enrico},
	editor = {Chersoni, Emmanuele and Hollenstein, Nora and Jacobs, Cassandra and Oseki, Yohei and Prévot, Laurent and Santus, Enrico},
	month = may,
	year = {2022},
	pages = {121--129},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\TZ7VVMLT\\Hollenstein et al. - 2022 - CMCL 2022 Shared Task on Multilingual and Crosslin.pdf:application/pdf},
}

@article{jacobs_masked_2022,
	title = {Masked language models directly encode linguistic uncertainty},
	volume = {5},
	url = {https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1237&context=scil},
	number = {1},
	urldate = {2023-11-09},
	journal = {Proceedings of the Society for Computation in Linguistics},
	author = {Jacobs, Cassandra and Hubbard, Ryan J. and Federmeier, Kara D.},
	year = {2022},
	pages = {225--228},
	file = {Available Version (via Google Scholar):C\:\\Users\\Job Schepens\\Zotero\\storage\\S46URVYX\\Jacobs et al. - 2022 - Masked language models directly encode linguistic .pdf:application/pdf},
}

@inproceedings{jacobs_human_2020,
	title = {The human unlikeness of neural language models in next-word prediction},
	url = {https://aclanthology.org/2020.winlp-1.29},
	doi = {10.18653/v1/2020.winlp-1.29},
	abstract = {Cassandra L. Jacobs, Arya D. McCarthy. Proceedings of the The Fourth Widening Natural Language Processing Workshop. 2020.},
	language = {en-us},
	urldate = {2023-11-09},
	author = {Jacobs, Cassandra L. and McCarthy, Arya D.},
	month = jul,
	year = {2020},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\Q5HDCDAM\\2020.winlp-1.29.html:text/html},
}

@misc{imperial_automatic_2023,
	title = {Automatic {Readability} {Assessment} for {Closely} {Related} {Languages}},
	url = {http://arxiv.org/abs/2305.13478},
	abstract = {In recent years, the main focus of research on automatic readability assessment (ARA) has shifted towards using expensive deep learning-based methods with the primary goal of increasing models' accuracy. This, however, is rarely applicable for low-resource languages where traditional handcrafted features are still widely used due to the lack of existing NLP tools to extract deeper linguistic representations. In this work, we take a step back from the technical component and focus on how linguistic aspects such as mutual intelligibility or degree of language relatedness can improve ARA in a low-resource setting. We collect short stories written in three languages in the Philippines-Tagalog, Bikol, and Cebuano-to train readability assessment models and explore the interaction of data and features in various cross-lingual setups. Our results show that the inclusion of CrossNGO, a novel specialized feature exploiting n-gram overlap applied to languages with high mutual intelligibility, significantly improves the performance of ARA models compared to the use of off-the-shelf large multilingual language models alone. Consequently, when both linguistic representations are combined, we achieve state-of-the-art results for Tagalog and Cebuano, and baseline scores for ARA in Bikol.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Imperial, Joseph Marvin and Kochmar, Ekaterina},
	month = may,
	year = {2023},
	note = {arXiv:2305.13478 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\QHKPYZC8\\Imperial and Kochmar - 2023 - Automatic Readability Assessment for Closely Relat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\8WKWC9WT\\2305.html:text/html},
}

@misc{bafna_simple_2023,
	title = {A {Simple} {Method} for {Unsupervised} {Bilingual} {Lexicon} {Induction} for {Data}-{Imbalanced}, {Closely} {Related} {Language} {Pairs}},
	url = {http://arxiv.org/abs/2305.14012},
	doi = {10.48550/arXiv.2305.14012},
	abstract = {Existing approaches for unsupervised bilingual lexicon induction (BLI) often depend on good quality static or contextual embeddings trained on large monolingual corpora for both languages. In reality, however, unsupervised BLI is most likely to be useful for dialects and languages that do not have abundant amounts of monolingual data. We introduce a simple and fast method for unsupervised BLI for low-resource languages with a related mid-to-high resource language, only requiring inference on the higher-resource language monolingual BERT. We work with two low-resource languages (\${\textless}5M\$ monolingual tokens), Bhojpuri and Magahi, of the severely under-researched Indic dialect continuum, showing that state-of-the-art methods in the literature show near-zero performance in these settings, and that our simpler method gives much better results. We repeat our experiments on Marathi and Nepali, two higher-resource Indic languages, to compare approach performances by resource range. We release automatically created bilingual lexicons for the first time for five languages of the Indic dialect continuum.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Bafna, Niyati and España-Bonet, Cristina and van Genabith, Josef and Sagot, Benoît and Bawden, Rachel},
	month = may,
	year = {2023},
	note = {arXiv:2305.14012 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\G376G2PV\\Bafna et al. - 2023 - A Simple Method for Unsupervised Bilingual Lexicon.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\4SJ6IRZ8\\2305.html:text/html},
}

@misc{caines_application_2023,
	title = {On the application of {Large} {Language} {Models} for language teaching and assessment technology},
	url = {http://arxiv.org/abs/2307.08393},
	doi = {10.48550/arXiv.2307.08393},
	abstract = {The recent release of very large language models such as PaLM and GPT-4 has made an unprecedented impact in the popular media and public consciousness, giving rise to a mixture of excitement and fear as to their capabilities and potential uses, and shining a light on natural language processing research which had not previously received so much attention. The developments offer great promise for education technology, and in this paper we look specifically at the potential for incorporating large language models in AI-driven language teaching and assessment systems. We consider several research areas and also discuss the risks and ethical considerations surrounding generative AI in education technology for language learners. Overall we find that larger language models offer improvements over previous models in text generation, opening up routes toward content generation which had not previously been plausible. For text generation they must be prompted carefully and their outputs may need to be reshaped before they are ready for use. For automated grading and grammatical error correction, tasks whose progress is checked on well-known benchmarks, early investigations indicate that large language models on their own do not improve on state-of-the-art results according to standard evaluation metrics. For grading it appears that linguistic features established in the literature should still be used for best performance, and for error correction it may be that the models can offer alternative feedback styles which are not measured sensitively with existing methods. In all cases, there is work to be done to experiment with the inclusion of large language models in education technology for language learners, in order to properly understand and report on their capacities and limitations, and to ensure that foreseeable risks such as misinformation and harmful bias are mitigated.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Caines, Andrew and Benedetto, Luca and Taslimipoor, Shiva and Davis, Christopher and Gao, Yuan and Andersen, Oeistein and Yuan, Zheng and Elliott, Mark and Moore, Russell and Bryant, Christopher and Rei, Marek and Yannakoudakis, Helen and Mullooly, Andrew and Nicholls, Diane and Buttery, Paula},
	month = jul,
	year = {2023},
	note = {arXiv:2307.08393 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\B8L5YXJF\\Caines et al. - 2023 - On the application of Large Language Models for la.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\CDM35DZS\\2307.html:text/html},
}

@misc{ivdntorg_basilex-corpus_nodate,
	title = {{BasiLex}-corpus},
	url = {https://taalmaterialen.ivdnt.org/download/tstc-basilex-corpus/},
	abstract = {Het BasiLex-corpus is een geannoteerde verzameling van teksten geschreven voor kinderen in de basisschoolleeftijd.  The Basilex corpus is an annotated collection of texts written for children in the age from four to twelve years.},
	language = {nl-NL},
	urldate = {2023-11-03},
	journal = {INT Taalmaterialen},
	author = {ivdnt.org},
}

@misc{noauthor_basilex_nodate,
	title = {Basilex {Corpus} {\textbar} {CLARIN} {VLO}},
	url = {https://vlo.clarin.eu/record/http_58__47__47_hdl.handle.net_47_10032_47_2bb9796b33a5ba6b172d52969e5c5043;jsessionid=C104EB768AA65AF252AFE1F3A1E5C5D3?2&count=19&fq=resourceClass:text&fq=modality:written&fq=languageCode:code:eng&fqType=resourceClass:or&fqType=modality:or&fqType=languageCode:or&index=3&tab=cmdi},
	urldate = {2023-11-03},
}

@article{monster_assessing_2021,
	title = {Assessing children’s incremental word knowledge in the upper primary grades},
	volume = {38},
	issn = {0265-5322},
	url = {https://doi.org/10.1177/0265532220961541},
	doi = {10.1177/0265532220961541},
	abstract = {Word knowledge acquisition is an incremental process that relies on exposure. As a result, word knowledge can broadly range from recognizing the word’s lexical status, to knowing its meaning in context, and to knowing its meaning independent of context. The present study aimed to model incremental word knowledge in 1454 upper primary school children from grades 3 to 5 by investigating their abilities on three word knowledge tasks originating from the same set of 300 words: lexical decision, context decision, and definitional decision. A mixed-effects model showed significant differences in performance between tasks and between grades, and a significant interaction indicating that task differences were different for children in grade 5 compared to children in grades 3 and 4. In order to examine further the different task relation patterns at the word level, a cluster analysis was performed using the observed item means, which were corrected for the guessing chance. The analysis showed that for most words, recognition of its lexical status was easier than knowing its meaning in context, which in turn was easier than knowing its meaning independent of context. It is concluded that task relation patterns differ based on mean log frequency as a proxy of word exposure.},
	language = {en},
	number = {4},
	urldate = {2023-11-03},
	journal = {Language Testing},
	author = {Monster, Iris and Tellings, Agnes and Burk, William J. and Keuning, Jos and Segers, Eliane and Verhoeven, Ludo},
	month = oct,
	year = {2021},
	note = {Publisher: SAGE Publications Ltd},
	pages = {536--557},
	file = {SAGE PDF Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\2QJDX6TV\\Monster et al. - 2021 - Assessing children’s incremental word knowledge in.pdf:application/pdf},
}

@misc{rooij_reclaiming_2023,
	title = {Reclaiming {AI} as a theoretical tool for cognitive science},
	url = {https://osf.io/preprints/psyarxiv/4cbuv/},
	doi = {10.31234/osf.io/4cbuv},
	abstract = {The idea that human cognition is, or can be understood as, a form of computation is a useful conceptual tool for cognitive science. It was a foundational assumption during the birth of cognitive science as a multidisciplinary field, with Artificial Intelligence (AI) as one of its contributing fields. One conception of AI in this context is as a provider of computational tools (frameworks, concepts, formalisms, models, proofs, simulations, etc.) that support theory building in cognitive science. The contemporary field of AI, however, has taken the theoretical possibility of explaining human cognition as a form of computation to imply the practical feasibility of realising human(-like or -level) cognition in factual computational systems; and, the field frames this realisation as a short-term inevitability. Yet, as we formally prove herein, creating systems with human(-like or -level) cognition is intrinsically computationally intractable. This means that any factual AI systems created in the short-run are at best decoys. When we think these systems capture something deep about ourselves and our thinking, we induce distorted and impoverished images of ourselves and our cognition. In other words, AI in current practice is deteriorating our theoretical understanding of cognition rather than advancing and enhancing it. The situation could be remediated by releasing the grip of the currently dominant view on AI and by returning to the idea of AI as a theoretical tool for cognitive science. In reclaiming this older idea of AI, however, it is important not to repeat conceptual mistakes of the past (and present) that brought us to where we are today.},
	language = {en-us},
	urldate = {2023-11-03},
	publisher = {PsyArXiv},
	author = {Rooij, Iris van and Guest, Olivia and Adolfi, Federico G. and Haan, Ronald de and Kolokolova, Antonina and Rich, Patricia},
	month = aug,
	year = {2023},
	keywords = {Cognitive Psychology, theory, cognitive science, Social and Behavioral Sciences, Meta-science, artificial intelligence (AI), computational complexity, engineering, explanation, Theory and Philosophy of Science},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\36BGZJIQ\\Rooij et al. - 2023 - Reclaiming AI as a theoretical tool for cognitive .pdf:application/pdf},
}

@article{enfield_scale_2023,
	title = {Scale in {Language}},
	volume = {47},
	copyright = {© 2023 The Authors. Cognitive Science published by Wiley Periodicals LLC on behalf of Cognitive Science Society (CSS).},
	issn = {1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13341},
	doi = {10.1111/cogs.13341},
	abstract = {A central concern of the cognitive science of language since its origins has been the concept of the linguistic system. Recent approaches to the system concept in language point to the exceedingly complex relations that hold between many kinds of interdependent systems, but it can be difficult to know how to proceed when “everything is connected.” This paper offers a framework for tackling that challenge by identifying *scale* as a conceptual mooring for the interdisciplinary study of language systems. The paper begins by defining the scale concept—simply, the possibility for a measure to be larger or smaller in different instances of a system, such as a phonemic inventory, a word's frequency value in a corpus, or a speaker population. We review sites of scale difference in and across linguistic subsystems, drawing on findings from linguistic typology, grammatical description, morphosyntactic theory, psycholinguistics, computational corpus work, and social network demography. We consider possible explanations for scaling differences and constraints in language. We then turn to the question of *dependencies between* sites of scale difference in language, reviewing four sample domains of scale dependency: in phonological systems, across levels of grammatical structure (Menzerath's Law), in corpora (Zipf's Law and related issues), and in speaker population size. Finally, we consider the implications of the review, including the utility of a scale framework for generating new questions and inspiring methodological innovations and interdisciplinary collaborations in cognitive-scientific research on language.},
	language = {en},
	number = {10},
	urldate = {2023-10-30},
	journal = {Cognitive Science},
	author = {Enfield, N. J.},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.13341},
	keywords = {Vocabulary, Complex adaptive systems, Dependencies, Language systems, Linguistic typology, Populations, Scale},
	pages = {e13341},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\7QIMG8P4\\Enfield - 2023 - Scale in Language.pdf:application/pdf},
}

@article{koplenig_languages_2023,
	title = {Languages with more speakers tend to be harder to (machine-)learn},
	volume = {13},
	copyright = {2023 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-45373-z},
	doi = {10.1038/s41598-023-45373-z},
	abstract = {Computational language models (LMs), most notably exemplified by the widespread success of OpenAI's ChatGPT chatbot, show impressive performance on a wide range of linguistic tasks, thus providing cognitive science and linguistics with a computational working model to empirically study different aspects of human language. Here, we use LMs to test the hypothesis that languages with more speakers tend to be easier to learn. In two experiments, we train several LMs—ranging from very simple n-gram models to state-of-the-art deep neural networks—on written cross-linguistic corpus data covering 1293 different languages and statistically estimate learning difficulty. Using a variety of quantitative methods and machine learning techniques to account for phylogenetic relatedness and geographical proximity of languages, we show that there is robust evidence for a relationship between learning difficulty and speaker population size. However, contrary to expectations derived from previous research, our results suggest that languages with more speakers tend to be harder to learn.},
	language = {en},
	number = {1},
	urldate = {2023-10-30},
	journal = {Scientific Reports},
	author = {Koplenig, Alexander and Wolfer, Sascha},
	month = oct,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Psychology, Human behaviour},
	pages = {18521},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\K8BFUHGY\\Koplenig and Wolfer - 2023 - Languages with more speakers tend to be harder to .pdf:application/pdf},
}

@article{meylan_how_2023,
	title = {How adults understand what young children say},
	copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-023-01698-3},
	doi = {10.1038/s41562-023-01698-3},
	abstract = {Children’s early speech often bears little resemblance to that of adults, and yet parents and other caregivers are able to interpret that speech and react accordingly. Here we investigate how adult listeners’ inferences reflect sophisticated beliefs about what children are trying to communicate, as well as how children are likely to pronounce words. Using a Bayesian framework for modelling spoken word recognition, we find that computational models can replicate adult interpretations of children’s speech only when they include strong, context-specific prior expectations about the messages that children will want to communicate. This points to a critical role of adult cognitive processes in supporting early communication and reveals how children can actively prompt adults to take actions on their behalf even when they have only a nascent understanding of the adult language. We discuss the wide-ranging implications of the powerful listening capabilities of adults for theories of first language acquisition.},
	language = {en},
	urldate = {2023-10-30},
	journal = {Nature Human Behaviour},
	author = {Meylan, Stephan C. and Foushee, Ruthe and Wong, Nicole H. and Bergelson, Elika and Levy, Roger P.},
	month = oct,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Language and linguistics, Development studies},
	pages = {1--15},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\8AWH5Z52\\Meylan et al. - 2023 - How adults understand what young children say.pdf:application/pdf},
}

@misc{ekgren_gpt-sw3_2023,
	title = {{GPT}-{SW3}: {An} {Autoregressive} {Language} {Model} for the {Nordic} {Languages}},
	shorttitle = {{GPT}-{SW3}},
	url = {http://arxiv.org/abs/2305.12987},
	doi = {10.48550/arXiv.2305.12987},
	abstract = {This paper details the process of developing the first native large generative language model for the Nordic languages, GPT-SW3. We cover all parts of the development process, from data collection and processing, training configuration and instruction finetuning, to evaluation and considerations for release strategies. We hope that this paper can serve as a guide and reference for other researchers that undertake the development of large generative models for smaller languages.},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Ekgren, Ariel and Gyllensten, Amaru Cuba and Stollenwerk, Felix and Öhman, Joey and Isbister, Tim and Gogoulou, Evangelia and Carlsson, Fredrik and Heiman, Alice and Casademont, Judit and Sahlgren, Magnus},
	month = may,
	year = {2023},
	note = {arXiv:2305.12987 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\4WCZCMA7\\Ekgren et al. - 2023 - GPT-SW3 An Autoregressive Language Model for the .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\2MI9TWSF\\2305.html:text/html},
}

@misc{buschoff_have_2023,
	title = {Have we built machines that think like people?},
	url = {http://arxiv.org/abs/2311.16093},
	doi = {10.48550/arXiv.2311.16093},
	abstract = {A chief goal of artificial intelligence is to build machines that think like people. Yet it has been argued that deep neural network architectures fail to accomplish this. Researchers have asserted these models' limitations in the domains of causal reasoning, intuitive physics, and intuitive psychology. Yet recent advancements, namely the rise of large language models, particularly those designed for visual processing, have rekindled interest in the potential to emulate human-like cognitive abilities. This paper evaluates the current state of vision-based large language models in the domains of intuitive physics, causal reasoning, and intuitive psychology. Through a series of controlled experiments, we investigate the extent to which these modern models grasp complex physical interactions, causal relationships, and intuitive understanding of others' preferences. Our findings reveal that, while these models demonstrate a notable proficiency in processing and interpreting visual data, they still fall short of human capabilities in these areas. The models exhibit a rudimentary understanding of physical laws and causal relationships, but their performance is hindered by a lack of deeper insights-a key aspect of human cognition. Furthermore, in tasks requiring an intuitive theory of mind, the models fail altogether. Our results emphasize the need for integrating more robust mechanisms for understanding causality, physical dynamics, and social cognition into modern-day, vision-based language models, and point out the importance of cognitively-inspired benchmarks.},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Buschoff, Luca M. Schulze and Akata, Elif and Bethge, Matthias and Schulz, Eric},
	month = nov,
	year = {2023},
	note = {arXiv:2311.16093 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\RJAMV6BF\\Buschoff et al. - 2023 - Have we built machines that think like people.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\WZEM4HXF\\2311.html:text/html},
}

@misc{nasr_scalable_2023,
	title = {Scalable {Extraction} of {Training} {Data} from ({Production}) {Language} {Models}},
	url = {http://arxiv.org/abs/2311.17035},
	doi = {10.48550/arXiv.2311.17035},
	abstract = {This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.},
	urldate = {2023-11-30},
	publisher = {arXiv},
	author = {Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A. Feder and Ippolito, Daphne and Choquette-Choo, Christopher A. and Wallace, Eric and Tramèr, Florian and Lee, Katherine},
	month = nov,
	year = {2023},
	note = {arXiv:2311.17035 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\3VI3WVCW\\Nasr et al. - 2023 - Scalable Extraction of Training Data from (Product.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\M65YZ2YL\\2311.html:text/html},
}

@misc{hu_language_2024,
	title = {Language models align with human judgments on key grammatical constructions},
	url = {http://arxiv.org/abs/2402.01676},
	abstract = {Do Large Language Models (LLMs) make human-like linguistic generalizations? Dentella et al. (2023; "DGL") prompt several LLMs ("Is the following sentence grammatically correct in English?") to elicit grammaticality judgments of 80 English sentences, concluding that LLMs demonstrate a "yes-response bias" and a "failure to distinguish grammatical from ungrammatical sentences". We re-evaluate LLM performance using well-established practices and find that DGL's data in fact provide evidence for just how well LLMs capture human behaviors. Models not only achieve high accuracy overall, but also capture fine-grained variation in human linguistic judgments.},
	urldate = {2024-03-05},
	publisher = {arXiv},
	author = {Hu, Jennifer and Mahowald, Kyle and Lupyan, Gary and Ivanova, Anna and Levy, Roger},
	month = jan,
	year = {2024},
	note = {arXiv:2402.01676 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\VM6LKLNF\\Hu et al. - 2024 - Language models align with human judgments on key .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\IGRUPHJA\\2402.html:text/html},
}

@article{dentella_systematic_2023,
	title = {Systematic testing of three {Language} {Models} reveals low language accuracy, absence of response stability, and a yes-response bias},
	volume = {120},
	url = {https://www.pnas.org/doi/10.1073/pnas.2309583120},
	doi = {10.1073/pnas.2309583120},
	abstract = {Humans are universally good in providing stable and accurate judgments about what forms part of their language and what not. Large Language Models (LMs) are claimed to possess human-like language abilities; hence, they are expected to emulate this behavior by providing both stable and accurate answers, when asked whether a string of words complies with or deviates from their next-word predictions. This work tests whether stability and accuracy are showcased by GPT-3/text-davinci-002, GPT-3/text-davinci-003, and ChatGPT, using a series of judgment tasks that tap on 8 linguistic phenomena: plural attraction, anaphora, center embedding, comparatives, intrusive resumption, negative polarity items, order of adjectives, and order of adverbs. For every phenomenon, 10 sentences (5 grammatical and 5 ungrammatical) are tested, each randomly repeated 10 times, totaling 800 elicited judgments per LM (total n = 2,400). Our results reveal variable above-chance accuracy in the grammatical condition, below-chance accuracy in the ungrammatical condition, a significant instability of answers across phenomena, and a yes-response bias for all the tested LMs. Furthermore, we found no evidence that repetition aids the Models to converge on a processing strategy that culminates in stable answers, either accurate or inaccurate. We demonstrate that the LMs’ performance in identifying (un)grammatical word patterns is in stark contrast to what is observed in humans (n = 80, tested on the same tasks) and argue that adopting LMs as theories of human language is not motivated at their current stage of development.},
	number = {51},
	urldate = {2024-03-05},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Dentella, Vittoria and Günther, Fritz and Leivada, Evelina},
	month = dec,
	year = {2023},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2309583120},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\EC99RLXQ\\Dentella et al. - 2023 - Systematic testing of three Language Models reveal.pdf:application/pdf},
}

@phdthesis{hu_neural_2023,
	type = {Thesis},
	title = {Neural language models and human linguistic knowledge},
	copyright = {Attribution 4.0 International (CC BY 4.0)},
	url = {https://dspace.mit.edu/handle/1721.1/152578},
	abstract = {Language is one of the hallmarks of intelligence, demanding explanation in a theory of human cognition. However, language presents unique practical challenges for quantitative empirical research, making many linguistic theories difficult to test at naturalistic scales. Artificial neural network language models (LMs) provide a new tool for studying language with mathematical precision and control, as they exhibit remarkably sophisticated linguistic behaviors while being fully intervenable. While LMs differ from humans in many ways, the learning outcomes of these models can reveal the behaviors that may emerge through expressive statistical learning algorithms applied to linguistic input.  
 
In this thesis, I demonstrate this approach through three case studies using LMs to investigate open questions in language acquisition and comprehension. First, I use LMs to perform controlled manipulations of language learning, and find that syntactic generalizations depend more on a learner's inductive bias than on training data size. Second, I use LMs to explain systematic variation in scalar inferences by approximating human listeners' expectations over unspoken alternative sentences (e.g., "The bill was supported overwhelmingly" implies that the bill was not supported unanimously). Finally, I show that LMs and humans exhibit similar behaviors on a set of non-literal comprehension tasks which are hypothesized to require social reasoning (e.g., inferring a speaker's intended meaning from ironic statements). These findings suggest that certain aspects of linguistic knowledge could emerge through domain-general prediction mechanisms, while other aspects may require specific inductive biases and conceptual structures.},
	language = {en},
	urldate = {2024-03-05},
	school = {Massachusetts Institute of Technology},
	author = {Hu, Jennifer},
	month = jun,
	year = {2023},
	note = {Accepted: 2023-10-30T20:04:19Z},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\PRNV9TMN\\Hu - 2023 - Neural language models and human linguistic knowle.pdf:application/pdf},
}

@article{trott_can_2024,
	title = {Can large language models help augment {English} psycholinguistic datasets?},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-024-02337-z},
	doi = {10.3758/s13428-024-02337-z},
	abstract = {Research on language and cognition relies extensively on psycholinguistic datasets or “norms”. These datasets contain judgments of lexical properties like concreteness and age of acquisition, and can be used to norm experimental stimuli, discover empirical relationships in the lexicon, and stress-test computational models. However, collecting human judgments at scale is both time-consuming and expensive. This issue of scale is compounded for multi-dimensional norms and those incorporating context. The current work asks whether large language models (LLMs) can be leveraged to augment the creation of large, psycholinguistic datasets in English. I use GPT-4 to collect multiple kinds of semantic judgments (e.g., word similarity, contextualized sensorimotor associations, iconicity) for English words and compare these judgments against the human “gold standard”. For each dataset, I find that GPT-4’s judgments are positively correlated with human judgments, in some cases rivaling or even exceeding the average inter-annotator agreement displayed by humans. I then identify several ways in which LLM-generated norms differ from human-generated norms systematically. I also perform several “substitution analyses”, which demonstrate that replacing human-generated norms with LLM-generated norms in a statistical model does not change the sign of parameter estimates (though in select cases, there are significant changes to their magnitude). I conclude by discussing the considerations and limitations associated with LLM-generated norms in general, including concerns of data contamination, the choice of LLM, external validity, construct validity, and data quality. Additionally, all of GPT-4’s judgments (over 30,000 in total) are made available online for further analysis.},
	language = {en},
	urldate = {2024-03-18},
	journal = {Behavior Research Methods},
	author = {Trott, Sean},
	month = jan,
	year = {2024},
	keywords = {Psycholinguistic resource, Large language models, ChatGPT, Dataset},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\HVCEQNZI\\Trott - 2024 - Can large language models help augment English psy.pdf:application/pdf},
}

@misc{griffiths_bayes_2023,
	title = {Bayes in the age of intelligent machines},
	url = {http://arxiv.org/abs/2311.10206},
	abstract = {The success of methods based on artificial neural networks in creating intelligent machines seems like it might pose a challenge to explanations of human cognition in terms of Bayesian inference. We argue that this is not the case, and that in fact these systems offer new opportunities for Bayesian modeling. Specifically, we argue that Bayesian models of cognition and artificial neural networks lie at different levels of analysis and are complementary modeling approaches, together offering a way to understand human cognition that spans these levels. We also argue that the same perspective can be applied to intelligent machines, where a Bayesian approach may be uniquely valuable in understanding the behavior of large, opaque artificial neural networks that are trained on proprietary data.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Griffiths, Thomas L. and Zhu, Jian-Qiao and Grant, Erin and McCoy, R. Thomas},
	month = nov,
	year = {2023},
	note = {arXiv:2311.10206 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\XQW2CY3J\\Griffiths et al. - 2023 - Bayes in the age of intelligent machines.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\L7I7MX6U\\2311.html:text/html},
}

@misc{mccoy_right_2019,
	title = {Right for the {Wrong} {Reasons}: {Diagnosing} {Syntactic} {Heuristics} in {Natural} {Language} {Inference}},
	shorttitle = {Right for the {Wrong} {Reasons}},
	url = {http://arxiv.org/abs/1902.01007},
	abstract = {A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {McCoy, R. Thomas and Pavlick, Ellie and Linzen, Tal},
	month = jun,
	year = {2019},
	note = {arXiv:1902.01007 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\HHIFHUSW\\McCoy et al. - 2019 - Right for the Wrong Reasons Diagnosing Syntactic .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\GSR8CHTL\\1902.html:text/html},
}

@article{mccoy_how_2023,
	title = {How much do language models copy from their training data? evaluating linguistic novelty in text generation using raven},
	volume = {11},
	shorttitle = {How much do language models copy from their training data?},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00567/116616},
	urldate = {2024-05-15},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {McCoy, R. Thomas and Smolensky, Paul and Linzen, Tal and Gao, Jianfeng and Celikyilmaz, Asli},
	year = {2023},
	note = {Publisher: MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA …},
	pages = {652--670},
	file = {Available Version (via Google Scholar):C\:\\Users\\Job Schepens\\Zotero\\storage\\8RM79MFR\\116616.html:text/html},
}

@misc{mccoy_embers_2023,
	title = {Embers of {Autoregression}: {Understanding} {Large} {Language} {Models} {Through} the {Problem} {They} are {Trained} to {Solve}},
	shorttitle = {Embers of {Autoregression}},
	url = {http://arxiv.org/abs/2309.13638},
	abstract = {The widespread adoption of large language models (LLMs) makes it important to recognize their strengths and limitations. We argue that in order to develop a holistic understanding of these systems we need to consider the problem that they were trained to solve: next-word prediction over Internet text. By recognizing the pressures that this task exerts we can make predictions about the strategies that LLMs will adopt, allowing us to reason about when they will succeed or fail. This approach - which we call the teleological approach - leads us to identify three factors that we hypothesize will influence LLM accuracy: the probability of the task to be performed, the probability of the target output, and the probability of the provided input. We predict that LLMs will achieve higher accuracy when these probabilities are high than when they are low - even in deterministic settings where probability should not matter. To test our predictions, we evaluate two LLMs (GPT-3.5 and GPT-4) on eleven tasks, and we find robust evidence that LLMs are influenced by probability in the ways that we have hypothesized. In many cases, the experiments reveal surprising failure modes. For instance, GPT-4's accuracy at decoding a simple cipher is 51\% when the output is a high-probability word sequence but only 13\% when it is low-probability. These results show that AI practitioners should be careful about using LLMs in low-probability situations. More broadly, we conclude that we should not evaluate LLMs as if they are humans but should instead treat them as a distinct type of system - one that has been shaped by its own particular set of pressures.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {McCoy, R. Thomas and Yao, Shunyu and Friedman, Dan and Hardy, Matthew and Griffiths, Thomas L.},
	month = sep,
	year = {2023},
	note = {arXiv:2309.13638 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\3TLG78SH\\McCoy et al. - 2023 - Embers of Autoregression Understanding Large Lang.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\E3PR78ZI\\2309.html:text/html},
}

@article{goldstein_shared_2022,
	title = {Shared computational principles for language processing in humans and deep language models},
	volume = {25},
	url = {https://www.nature.com/articles/s41593-022-01026-4},
	number = {3},
	urldate = {2024-05-15},
	journal = {Nature neuroscience},
	author = {Goldstein, Ariel and Zada, Zaid and Buchnik, Eliav and Schain, Mariano and Price, Amy and Aubrey, Bobbi and Nastase, Samuel A. and Feder, Amir and Emanuel, Dotan and Cohen, Alon},
	year = {2022},
	note = {Publisher: Nature Publishing Group US New York},
	pages = {369--380},
	file = {Available Version (via Google Scholar):C\:\\Users\\Job Schepens\\Zotero\\storage\\IMZVXJYR\\s41593-022-01026-4.html:text/html},
}

@misc{taubenfeld_systematic_2024,
	title = {Systematic {Biases} in {LLM} {Simulations} of {Debates}},
	url = {http://arxiv.org/abs/2402.04049},
	abstract = {Recent advancements in natural language processing, especially the emergence of Large Language Models (LLMs), have opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Taubenfeld, Amir and Dover, Yaniv and Reichart, Roi and Goldstein, Ariel},
	month = feb,
	year = {2024},
	note = {arXiv:2402.04049 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\A6Y8VF22\\Taubenfeld et al. - 2024 - Systematic Biases in LLM Simulations of Debates.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\VCJRLTEN\\2402.html:text/html},
}

@misc{goldstein_zombies_2024,
	title = {Do {Zombies} {Understand}? {A} {Choose}-{Your}-{Own}-{Adventure} {Exploration} of {Machine} {Cognition}},
	shorttitle = {Do {Zombies} {Understand}?},
	url = {http://arxiv.org/abs/2403.00499},
	abstract = {Recent advances in LLMs have sparked a debate on whether they understand text. In this position paper, we argue that opponents in this debate hold different definitions for understanding, and particularly differ in their view on the role of consciousness. To substantiate this claim, we propose a thought experiment involving an open-source chatbot \$Z\$ which excels on every possible benchmark, seemingly without subjective experience. We ask whether \$Z\$ is capable of understanding, and show that different schools of thought within seminal AI research seem to answer this question differently, uncovering their terminological disagreement. Moving forward, we propose two distinct working definitions for understanding which explicitly acknowledge the question of consciousness, and draw connections with a rich literature in philosophy, psychology and neuroscience.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Goldstein, Ariel and Stanovsky, Gabriel},
	month = mar,
	year = {2024},
	note = {arXiv:2403.00499 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\VQ2WET6Q\\Goldstein and Stanovsky - 2024 - Do Zombies Understand A Choose-Your-Own-Adventure.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\92UISC2Y\\2403.html:text/html},
}

@article{goldstein_brain_2022,
	title = {Brain embeddings with shared geometry to artificial contextual embeddings, as a code for representing language in the human brain},
	url = {https://www.nature.com/articles/s41467-024-46631-y},
	urldate = {2024-05-15},
	journal = {BioRxiv},
	author = {Goldstein, Ariel and Dabush, Avigail and Aubrey, Bobbi and Schain, Mariano and Nastase, Samuel A. and Zada, Zaid and Ham, Eric and Hong, Zhuoqiao and Feder, Amir and Gazula, Harshvardhan},
	year = {2022},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {2022--03},
	file = {Available Version (via Google Scholar):C\:\\Users\\Job Schepens\\Zotero\\storage\\HKJ4XAPS\\s41467-024-46631-y.html:text/html},
}

@misc{chang_when_2023,
	title = {When {Is} {Multilinguality} a {Curse}? {Language} {Modeling} for 250 {High}- and {Low}-{Resource} {Languages}},
	shorttitle = {When {Is} {Multilinguality} a {Curse}?},
	url = {http://arxiv.org/abs/2311.09205},
	abstract = {Multilingual language models are widely used to extend NLP systems to low-resource languages. However, concrete evidence for the effects of multilinguality on language modeling performance in individual languages remains scarce. Here, we pre-train over 10,000 monolingual and multilingual language models for over 250 languages, including multiple language families that are under-studied in NLP. We assess how language modeling performance in each language varies as a function of (1) monolingual dataset size, (2) added multilingual dataset size, (3) linguistic similarity of the added languages, and (4) model size (up to 45M parameters). We find that in moderation, adding multilingual data improves low-resource language modeling performance, similar to increasing low-resource dataset sizes by up to 33\%. Improvements depend on the syntactic similarity of the added multilingual data, with marginal additional effects of vocabulary overlap. However, high-resource languages consistently perform worse in multilingual pre-training scenarios. As dataset sizes increase, adding multilingual data begins to hurt performance for both low-resource and high-resource languages, likely due to limited model capacity (the "curse of multilinguality"). These results suggest that massively multilingual pre-training may not be optimal for any languages involved, but that more targeted models can significantly improve performance.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Chang, Tyler A. and Arnett, Catherine and Tu, Zhuowen and Bergen, Benjamin K.},
	month = nov,
	year = {2023},
	note = {arXiv:2311.09205 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\753YA6LK\\Chang et al. - 2023 - When Is Multilinguality a Curse Language Modeling.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\JH5M5MVC\\2311.html:text/html},
}

@misc{arnett_bit_2024,
	title = {A {Bit} of a {Problem}: {Measurement} {Disparities} in {Dataset} {Sizes} {Across} {Languages}},
	shorttitle = {A {Bit} of a {Problem}},
	url = {http://arxiv.org/abs/2403.00686},
	abstract = {How should text dataset sizes be compared across languages? Even for content-matched (parallel) corpora, UTF-8 encoded text can require a dramatically different number of bytes for different languages. In our work, we define the byte premium between two languages as the ratio of bytes used to encode content-matched text in those languages. We compute byte premiums for 1155 languages, and we use linear regressions to estimate byte premiums for other languages. We release a tool to obtain byte premiums for any two languages, enabling comparisons of dataset sizes across languages for more equitable multilingual model development and data practices.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Arnett, Catherine and Chang, Tyler A. and Bergen, Benjamin K.},
	month = mar,
	year = {2024},
	note = {arXiv:2403.00686 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\YDMAZDSI\\Arnett et al. - 2024 - A Bit of a Problem Measurement Disparities in Dat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\R5M6GYTM\\2403.html:text/html},
}

@misc{michaelov_revenge_2024,
	title = {Revenge of the {Fallen}? {Recurrent} {Models} {Match} {Transformers} at {Predicting} {Human} {Language} {Comprehension} {Metrics}},
	shorttitle = {Revenge of the {Fallen}?},
	url = {http://arxiv.org/abs/2404.19178},
	abstract = {Transformers have supplanted Recurrent Neural Networks as the dominant architecture for both natural language processing tasks and, despite criticisms of cognitive implausibility, for modelling the effect of predictability on online human language comprehension. However, two recently developed recurrent neural network architectures, RWKV and Mamba, appear to perform natural language tasks comparably to or better than transformers of equivalent scale. In this paper, we show that contemporary recurrent models are now also able to match - and in some cases, exceed - performance of comparably sized transformers at modeling online human language comprehension. This suggests that transformer language models are not uniquely suited to this task, and opens up new directions for debates about the extent to which architectural features of language models make them better or worse models of human language comprehension.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Michaelov, James A. and Arnett, Catherine and Bergen, Benjamin K.},
	month = apr,
	year = {2024},
	note = {arXiv:2404.19178 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\GWTZ3KYA\\Michaelov et al. - 2024 - Revenge of the Fallen Recurrent Models Match Tran.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\N69IRENH\\2404.html:text/html},
}

@article{yao_tree_2024,
	title = {Tree of thoughts: {Deliberate} problem solving with large language models},
	volume = {36},
	shorttitle = {Tree of thoughts},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html},
	urldate = {2024-05-06},
	journal = {Advances in Neural Information Processing Systems},
	author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
	year = {2024},
	file = {Available Version (via Google Scholar):C\:\\Users\\Job Schepens\\Zotero\\storage\\FP6YWS7A\\Yao et al. - 2024 - Tree of thoughts Deliberate problem solving with .pdf:application/pdf},
}

@article{linzen_what_2019,
	title = {What can linguistics and deep learning contribute to each other? {Response} to {Pater}},
	volume = {95},
	issn = {1535-0665},
	shorttitle = {What can linguistics and deep learning contribute to each other?},
	url = {https://muse.jhu.edu/pub/24/article/719237},
	abstract = {Joe Pater’s (2019) target article calls for greater interaction between neural network research and linguistics. I expand on this call and show how such interaction can benefit both fields. Linguists can contribute to research on neural networks for language technologies by clearly delineating the linguistic capabilities that can be expected of such systems, and by constructing controlled experimental paradigms that can determine whether those desiderata have been met. In the other direction, neural networks can benefit the scientific study of language by providing infrastructure for modeling human sentence processing and for evaluating the necessity of particular innate constraints on language acquisition.*},
	number = {1},
	urldate = {2024-04-25},
	journal = {Language},
	author = {Linzen, Tal},
	year = {2019},
	note = {Publisher: Linguistic Society of America},
	keywords = {neural networks, syntax, deep learning, poverty of the stimulus, psycholinguistics, sentence processing},
	pages = {e99--e108},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\IFXFTMGP\\Linzen - 2019 - What can linguistics and deep learning contribute .pdf:application/pdf},
}

@article{frank_openly_2023,
	title = {Openly accessible {LLMs} can help us to understand human cognition},
	volume = {7},
	copyright = {2023 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-023-01732-4},
	doi = {10.1038/s41562-023-01732-4},
	abstract = {Large language models can be construed as ‘cognitive models’, scientific artefacts that help us to understand the human mind. If made openly accessible, they may provide a valuable model system for studying the emergence of language, reasoning and other uniquely human behaviours.},
	language = {en},
	number = {11},
	urldate = {2024-04-25},
	journal = {Nature Human Behaviour},
	author = {Frank, Michael C.},
	month = nov,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Language and linguistics},
	pages = {1825--1827},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\XQUP473L\\Frank - 2023 - Openly accessible LLMs can help us to understand h.pdf:application/pdf},
}

@misc{cao_explanatory_2021,
	title = {Explanatory models in neuroscience: {Part} 1 -- taking mechanistic abstraction seriously},
	shorttitle = {Explanatory models in neuroscience},
	url = {http://arxiv.org/abs/2104.01490},
	doi = {10.48550/arXiv.2104.01490},
	abstract = {Despite the recent success of neural network models in mimicking animal performance on visual perceptual tasks, critics worry that these models fail to illuminate brain function. We take it that a central approach to explanation in systems neuroscience is that of mechanistic modeling, where understanding the system is taken to require fleshing out the parts, organization, and activities of a system, and how those give rise to behaviors of interest. However, it remains somewhat controversial what it means for a model to describe a mechanism, and whether neural network models qualify as explanatory. We argue that certain kinds of neural network models are actually good examples of mechanistic models, when the right notion of mechanistic mapping is deployed. Building on existing work on model-to-mechanism mapping (3M), we describe criteria delineating such a notion, which we call 3M++. These criteria require us, first, to identify a level of description that is both abstract but detailed enough to be "runnable", and then, to construct model-to-brain mappings using the same principles as those employed for brain-to-brain mapping across individuals. Perhaps surprisingly, the abstractions required are those already in use in experimental neuroscience, and are of the kind deployed in the construction of more familiar computational models, just as the principles of inter-brain mappings are very much in the spirit of those already employed in the collection and analysis of data across animals. In a companion paper, we address the relationship between optimization and intelligibility, in the context of functional evolutionary explanations. Taken together, mechanistic interpretations of computational models and the dependencies between form and function illuminated by optimization processes can help us to understand why brain systems are built they way they are.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Cao, Rosa and Yamins, Daniel},
	month = apr,
	year = {2021},
	note = {arXiv:2104.01490 [cs, q-bio]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\92QYM4PS\\Cao and Yamins - 2021 - Explanatory models in neuroscience Part 1 -- taki.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\PWSYXBSY\\2104.html:text/html},
}

@inproceedings{xu_linearity_2023,
	address = {Singapore},
	title = {The {Linearity} of the {Effect} of {Surprisal} on {Reading} {Times} across {Languages}},
	url = {https://aclanthology.org/2023.findings-emnlp.1052},
	doi = {10.18653/v1/2023.findings-emnlp.1052},
	abstract = {In psycholinguistics, surprisal theory posits that the amount of online processing effort expended by a human comprehender per word positively correlates with the surprisal of that word given its preceding context. In addition to this overall correlation, more importantly, the specific quantitative form taken by the processing effort as a function of surprisal offers insights into the underlying cognitive mechanisms of language processing. Focusing on English, previous studies have looked into the linearity of surprisal on reading times. Here, we extend the investigation by examining eyetracking corpora of seven languages: Danish, Dutch, English, German, Japanese, Mandarin, and Russian. We find evidence for superlinearity in some languages, but the results are highly sensitive to which language model is used to estimate surprisal.},
	urldate = {2024-04-25},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Weijie and Chon, Jason and Liu, Tianran and Futrell, Richard},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {15711--15721},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\W2332PCZ\\Xu et al. - 2023 - The Linearity of the Effect of Surprisal on Readin.pdf:application/pdf},
}

@misc{kallini_mission_2024,
	title = {Mission: {Impossible} {Language} {Models}},
	shorttitle = {Mission},
	url = {http://arxiv.org/abs/2401.06416},
	doi = {10.48550/arXiv.2401.06416},
	abstract = {Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout training to compare the learning process for each language. Our core finding is that GPT-2 struggles to learn impossible languages when compared to English as a control, challenging the core claim. More importantly, we hope our approach opens up a productive line of inquiry in which different LLM architectures are tested on a variety of impossible languages in an effort to learn more about how LLMs can be used as tools for these cognitive and typological investigations.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Kallini, Julie and Papadimitriou, Isabel and Futrell, Richard and Mahowald, Kyle and Potts, Christopher},
	month = jan,
	year = {2024},
	note = {arXiv:2401.06416 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\758DJ4SG\\Kallini et al. - 2024 - Mission Impossible Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\PIVCBKIP\\2401.html:text/html},
}

@incollection{baroni_proper_2022,
	title = {On the {Proper} {Role} of {Linguistically} {Oriented} {Deep} {Net} {Analysis} in {Linguistic} {Theorising}},
	isbn = {978-1-00-320538-8},
	abstract = {A lively research field has recently emerged that uses experimental methods to probe the linguistic behaviour of modern deep networks. While work in this tradition often reports intriguing results about the grammatical skills of deep nets, it is not clear what their implications for linguistic theorising should be. As a consequence, linguistically oriented deep net analysis has had very little impact on linguistics at large. In this chapter, I suggest that deep networks should be treated as theories making explicit predictions about the acceptability of linguistic utterances. I argue that if we overcome some obstacles standing in the way of seriously pursuing this idea, we will gain a powerful new theoretical tool, complementary to mainstream algebraic approaches.},
	booktitle = {Algebraic {Structures} in {Natural} {Language}},
	publisher = {CRC Press},
	author = {Baroni, Marco},
	year = {2022},
	note = {Num Pages: 16},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\4DX5BCBW\\Baroni - 2022 - On the Proper Role of Linguistically Oriented Deep.pdf:application/pdf},
}

@misc{bommasani_opportunities_2022,
	title = {On the {Opportunities} and {Risks} of {Foundation} {Models}},
	url = {http://arxiv.org/abs/2108.07258},
	doi = {10.48550/arXiv.2108.07258},
	abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	month = jul,
	year = {2022},
	note = {arXiv:2108.07258 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\627D8UP8\\Bommasani et al. - 2022 - On the Opportunities and Risks of Foundation Model.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\47EMFFL8\\2108.html:text/html},
}

@article{portelance_predicting_2023,
	title = {Predicting {Age} of {Acquisition} for {Children}'s {Early} {Vocabulary} in {Five} {Languages} {Using} {Language} {Model} {Surprisal}},
	volume = {47},
	issn = {0364-0213, 1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cogs.13334},
	doi = {10.1111/cogs.13334},
	abstract = {Abstract
            What makes a word easy to learn? Early‐learned words are frequent and tend to name concrete referents. But words typically do not occur in isolation. Some words are predictable from their contexts; others are less so. Here, we investigate whether predictability relates to when children start producing different words (age of acquisition; AoA). We operationalized predictability in terms of a word's surprisal in child‐directed speech, computed using n‐gram and long‐short‐term‐memory (LSTM) language models. Predictability derived from LSTMs was generally a better predictor than predictability derived from n‐gram models. Across five languages, average surprisal was positively correlated with the AoA of predicates and function words but not nouns. Controlling for concreteness and word frequency, more predictable predicates and function words were learned earlier. Differences in predictability between languages were associated with cross‐linguistic differences in AoA: the same word (when it was a predicate) was produced earlier in languages where the word was more predictable.},
	language = {en},
	number = {9},
	urldate = {2024-04-24},
	journal = {Cognitive Science},
	author = {Portelance, Eva and Duan, Yuguang and Frank, Michael C. and Lupyan, Gary},
	month = sep,
	year = {2023},
	pages = {e13334},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\3KLZIBRL\\Portelance et al. - 2023 - Predicting Age of Acquisition for Children's Early.pdf:application/pdf},
}

@inproceedings{warstadt_findings_2023,
	title = {Findings of the {BabyLM} {Challenge}: {Sample}-efficient pretraining on developmentally plausible corpora},
	shorttitle = {Findings of the {BabyLM} {Challenge}},
	url = {https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/650680/2/2023.conll-babylm.1.pdf},
	urldate = {2024-04-24},
	booktitle = {Proceedings of the {BabyLM} {Challenge} at the 27th {Conference} on {Computational} {Natural} {Language} {Learning}},
	author = {Warstadt, Alex and Mueller, Aaron and Choshen, Leshem and Wilcox, Ethan and Zhuang, Chengxu and Ciro, Juan and Mosquera, Rafael and Paranjabe, Bhargavi and Williams, Adina and Linzen, Tal},
	year = {2023},
	file = {Available Version (via Google Scholar):C\:\\Users\\Job Schepens\\Zotero\\storage\\KRN9U8WT\\Warstadt et al. - 2023 - Findings of the BabyLM Challenge Sample-efficient.pdf:application/pdf},
}

@article{vong_grounded_2024,
	title = {Grounded language acquisition through the eyes and ears of a single child},
	volume = {383},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.adi1374},
	doi = {10.1126/science.adi1374},
	abstract = {Starting around 6 to 9 months of age, children begin acquiring their first words, linking spoken words to their visual counterparts. How much of this knowledge is learnable from sensory input with relatively generic learning mechanisms, and how much requires stronger inductive biases? Using longitudinal head-mounted camera recordings from one child aged 6 to 25 months, we trained a relatively generic neural network on 61 hours of correlated visual-linguistic data streams, learning feature-based representations and cross-modal associations. Our model acquires many word-referent mappings present in the child’s everyday experience, enables zero-shot generalization to new visual referents, and aligns its visual and linguistic conceptual systems. These results show how critical aspects of grounded word meaning are learnable through joint representation and associative learning from one child’s input.
          , 
            Editor’s summary
            
              How do young children learn to associate new words with specific objects or visually represented concepts? This hotly debated question in early language acquisition has been traditionally examined in laboratories, limiting generalizability to real-world settings. Vong
              et al
              . investigated the question in an unprecedented, longitudinal manner using head-mounted video recordings from a single child’s first-person experiences in naturalistic settings. By applying machine learning, they introduced the Child’s View for Contrastive Learning (CVCL) model, pairing video frames that co-occurred with uttered words, and embedded the images and words in shared representational spaces. CVCL represents sets of visually similar things from one concept (e.g., puzzles) through distinct subclusters (animal versus alphabet puzzles). It combines associative and representation learning that fills gaps in language acquisition research and theories. —Ekeoma Uzogara
            
          , 
            Machine learning advances research into early language acquisition in children.},
	language = {en},
	number = {6682},
	urldate = {2024-04-24},
	journal = {Science},
	author = {Vong, Wai Keen and Wang, Wentao and Orhan, A. Emin and Lake, Brenden M.},
	month = feb,
	year = {2024},
	pages = {504--511},
}

@incollection{warstadt_what_2022,
	title = {What artificial neural networks can tell us about human language acquisition},
	url = {https://www.taylorfrancis.com/chapters/edit/10.1201/9781003205388-2/artificial-neural-networks-tell-us-human-language-acquisition-alex-warstadt-samuel-bowman},
	urldate = {2024-04-24},
	booktitle = {Algebraic structures in natural language},
	publisher = {CRC Press},
	author = {Warstadt, Alex and Bowman, Samuel R.},
	year = {2022},
	pages = {17--60},
}

@article{frank_bridging_2023,
	title = {Bridging the data gap between children and large language models},
	volume = {27},
	issn = {1364-6613, 1879-307X},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(23)00203-6},
	doi = {10.1016/j.tics.2023.08.007},
	language = {English},
	number = {11},
	urldate = {2024-04-24},
	journal = {Trends in Cognitive Sciences},
	author = {Frank, Michael C.},
	month = nov,
	year = {2023},
	pmid = {37659919},
	note = {Publisher: Elsevier},
	keywords = {language learning, artificial intelligence, large language models, human learning},
	pages = {990--992},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\TYYJ7TT2\\Frank - 2023 - Bridging the data gap between children and large l.pdf:application/pdf},
}

@article{blank_what_2023,
	title = {What are large language models supposed to model?},
	volume = {27},
	issn = {1364-6613, 1879-307X},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(23)00202-4},
	doi = {10.1016/j.tics.2023.08.006},
	language = {English},
	number = {11},
	urldate = {2024-04-24},
	journal = {Trends in Cognitive Sciences},
	author = {Blank, Idan A.},
	month = nov,
	year = {2023},
	pmid = {37659920},
	note = {Publisher: Elsevier},
	keywords = {language, deep neural network, distributed representation, symbolic representation},
	pages = {987--989},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\MWUB44PH\\Blank - 2023 - What are large language models supposed to model.pdf:application/pdf},
}

@article{momennejad_evaluating_2023,
	title = {Evaluating {Cognitive} {Maps} and {Planning} in {Large} {Language} {Models} with {CogEval}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/dc9d5dcf3e86b83e137bad367227c8ca-Abstract-Conference.html},
	language = {en},
	urldate = {2024-04-24},
	journal = {Advances in Neural Information Processing Systems},
	author = {Momennejad, Ida and Hasanbeig, Hosein and Vieira Frujeri, Felipe and Sharma, Hiteshi and Jojic, Nebojsa and Palangi, Hamid and Ness, Robert and Larson, Jonathan},
	month = dec,
	year = {2023},
	pages = {69736--69751},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\A6KBM8A4\\Momennejad et al. - 2023 - Evaluating Cognitive Maps and Planning in Large La.pdf:application/pdf},
}

@misc{lai_chatgpt_2023,
	title = {{ChatGPT} {Beyond} {English}: {Towards} a {Comprehensive} {Evaluation} of {Large} {Language} {Models} in {Multilingual} {Learning}},
	shorttitle = {{ChatGPT} {Beyond} {English}},
	url = {http://arxiv.org/abs/2304.05613},
	abstract = {Over the last few years, large language models (LLMs) have emerged as the most important breakthroughs in natural language processing (NLP) that fundamentally transform research and developments in the field. ChatGPT represents one of the most exciting LLM systems developed recently to showcase impressive skills for language generation and highly attract public attention. Among various exciting applications discovered for ChatGPT in English, the model can process and generate texts for multiple languages due to its multilingual training data. Given the broad adoption of ChatGPT for English in different problems and areas, a natural question is whether ChatGPT can also be applied effectively for other languages or it is necessary to develop more language-specific technologies. The answer to this question requires a thorough evaluation of ChatGPT over multiple tasks with diverse languages and large datasets (i.e., beyond reported anecdotes), which is still missing or limited in current research. Our work aims to fill this gap for the evaluation of ChatGPT and similar LLMs to provide more comprehensive information for multilingual NLP applications. While this work will be an ongoing effort to include additional experiments in the future, our current paper evaluates ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium, low, and extremely low resources. We also focus on the zero-shot learning setting for ChatGPT to improve reproducibility and better simulate the interactions of general users. Compared to the performance of previous models, our extensive experimental results demonstrate a worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Lai, Viet Dac and Ngo, Nghia Trung and Veyseh, Amir Pouran Ben and Man, Hieu and Dernoncourt, Franck and Bui, Trung and Nguyen, Thien Huu},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05613 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\LKG2GWDW\\Lai et al. - 2023 - ChatGPT Beyond English Towards a Comprehensive Ev.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\8MT4RCQZ\\2304.html:text/html},
}

@article{katz_gpt-4_2024,
	title = {{GPT}-4 passes the bar exam},
	volume = {382},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2023.0254},
	doi = {10.1098/rsta.2023.0254},
	abstract = {In this paper, we experimentally evaluate the zero-shot performance of GPT-4 against prior generations of GPT on the entire uniform bar examination (UBE), including not only the multiple-choice multistate bar examination (MBE), but also the open-ended multistate essay exam (MEE) and multistate performance test (MPT) components. On the MBE, GPT-4 significantly outperforms both human test-takers and prior models, demonstrating a 26\% increase over ChatGPT and beating humans in five of seven subject areas. On the MEE and MPT, which have not previously been evaluated by scholars, GPT-4 scores an average of 4.2/6.0 when compared with much lower scores for ChatGPT. Graded across the UBE components, in the manner in which a human test-taker would be, GPT-4 scores approximately 297 points, significantly in excess of the passing threshold for all UBE jurisdictions. These findings document not just the rapid and remarkable advance of large language model performance generally, but also the potential for such models to support the delivery of legal services in society.
            This article is part of the theme issue ‘A complexity science approach to law and governance’.},
	language = {en},
	number = {2270},
	urldate = {2024-04-24},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Katz, Daniel Martin and Bommarito, Michael James and Gao, Shang and Arredondo, Pablo},
	month = apr,
	year = {2024},
	pages = {20230254},
}

@article{mitchell_debate_2023,
	title = {The debate over understanding in {AI}’s large language models},
	volume = {120},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/10.1073/pnas.2215907120},
	doi = {10.1073/pnas.2215907120},
	abstract = {We survey a current, heated debate in the artificial intelligence (AI) research community on whether large pretrained language models can be said to understand language—and the physical and social situations language encodes—in any humanlike sense. We describe arguments that have been made for and against such understanding and key questions for the broader sciences of intelligence that have arisen in light of these arguments. We contend that an extended science of intelligence can be developed that will provide insight into distinct modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition.},
	language = {en},
	number = {13},
	urldate = {2024-04-24},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mitchell, Melanie and Krakauer, David C.},
	month = mar,
	year = {2023},
	pages = {e2215907120},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\8XJINEH8\\Mitchell and Krakauer - 2023 - The debate over understanding in AI’s large langua.pdf:application/pdf},
}

@article{floridi_ai_2023,
	title = {{AI} as {Agency} {Without} {Intelligence}: on {ChatGPT}, {Large} {Language} {Models}, and {Other} {Generative} {Models}},
	volume = {36},
	issn = {2210-5433, 2210-5441},
	shorttitle = {{AI} as {Agency} {Without} {Intelligence}},
	url = {https://link.springer.com/10.1007/s13347-023-00621-y},
	doi = {10.1007/s13347-023-00621-y},
	language = {en},
	number = {1},
	urldate = {2024-04-24},
	journal = {Philosophy \& Technology},
	author = {Floridi, Luciano},
	month = mar,
	year = {2023},
	pages = {15, s13347--023--00621--y},
	file = {Available Version (via Google Scholar):C\:\\Users\\Job Schepens\\Zotero\\storage\\XQ88UNXF\\s13347-023-00621-y.html:text/html;Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\NXAVYQRT\\Floridi - 2023 - AI as Agency Without Intelligence on ChatGPT, Lar.pdf:application/pdf},
}

@article{pavlick_symbols_2023,
	title = {Symbols and grounding in large language models},
	volume = {381},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2022.0041},
	doi = {10.1098/rsta.2022.0041},
	abstract = {Large language models (LLMs) are one of the most impressive achievements of artificial intelligence in recent years. However, their relevance to the study of language more broadly remains unclear. This article considers the potential of LLMs to serve as models of language understanding in humans. While debate on this question typically centres around models’ performance on challenging language understanding tasks, this article argues that the answer depends on models’ underlying competence, and thus that the focus of the debate should be on empirical work which seeks to characterize the representations and processing algorithms that underlie model behaviour. From this perspective, the article offers counterarguments to two commonly cited reasons why LLMs cannot serve as plausible models of language in humans: their lack of symbolic structure and their lack of grounding. For each, a case is made that recent empirical trends undermine the common assumptions about LLMs, and thus that it is premature to draw conclusions about LLMs’ ability (or lack thereof) to offer insights on human language representation and understanding.
            This article is part of a discussion meeting issue ‘Cognitive artificial intelligence’.},
	language = {en},
	number = {2251},
	urldate = {2024-04-24},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Pavlick, Ellie},
	month = jul,
	year = {2023},
	pages = {20220041},
}

@article{mahowald_dissociating_2024,
	title = {Dissociating language and thought in large language models},
	volume = {0},
	issn = {1364-6613, 1879-307X},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(24)00027-5},
	doi = {10.1016/j.tics.2024.01.011},
	language = {English},
	number = {0},
	urldate = {2024-04-24},
	journal = {Trends in Cognitive Sciences},
	author = {Mahowald, Kyle and Ivanova, Anna A. and Blank, Idan A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
	month = mar,
	year = {2024},
	pmid = {38508911},
	note = {Publisher: Elsevier},
	keywords = {computational modeling, cognitive neuroscience, language and thought, large language models, linguistic competence},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\GN8X7AT4\\Mahowald et al. - 2024 - Dissociating language and thought in large languag.pdf:application/pdf},
}

@article{chang_word_2022,
	title = {Word {Acquisition} in {Neural} {Language} {Models}},
	volume = {10},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00444},
	doi = {10.1162/tacl_a_00444},
	abstract = {We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007). Drawing on studies of word acquisition in children, we evaluate multiple predictors for words’ ages of acquisition in LSTMs, BERT, and GPT-2. We find that the effects of concreteness, word length, and lexical class are pointedly different in children and language models, reinforcing the importance of interaction and sensorimotor experience in child language acquisition. Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances. Interestingly, models follow consistent patterns during training for both unidirectional and bidirectional models, and for both LSTM and Transformer architectures. Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models.},
	urldate = {2024-04-24},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Chang, Tyler A. and Bergen, Benjamin K.},
	month = jan,
	year = {2022},
	pages = {1--16},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\YBJI58E3\\Chang and Bergen - 2022 - Word Acquisition in Neural Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\KHLAYICC\\Word-Acquisition-in-Neural-Language-Models.html:text/html},
}

@inproceedings{razeghi_impact_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Impact of {Pretraining} {Term} {Frequencies} on {Few}-{Shot} {Numerical} {Reasoning}},
	url = {https://aclanthology.org/2022.findings-emnlp.59},
	doi = {10.18653/v1/2022.findings-emnlp.59},
	abstract = {Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above 70\% (absolute) more accurate on the top 10\% frequent terms in comparison to the bottom 10\%. Overall, although LMs appear successful at few-shot numerical reasoning, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.},
	urldate = {2024-04-24},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Razeghi, Yasaman and Logan IV, Robert L and Gardner, Matt and Singh, Sameer},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {840--854},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\GASNJK3E\\Razeghi et al. - 2022 - Impact of Pretraining Term Frequencies on Few-Shot.pdf:application/pdf},
}

@inproceedings{wei_frequency_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Frequency {Effects} on {Syntactic} {Rule} {Learning} in {Transformers}},
	url = {https://aclanthology.org/2021.emnlp-main.72},
	doi = {10.18653/v1/2021.emnlp-main.72},
	abstract = {Pre-trained language models perform well on a variety of linguistic tasks that require symbolic reasoning, raising the question of whether such models implicitly represent abstract symbols and rules. We investigate this question using the case study of BERT's performance on English subject–verb agreement. Unlike prior work, we train multiple instances of BERT from scratch, allowing us to perform a series of controlled interventions at pre-training time. We show that BERT often generalizes well to subject–verb pairs that never occurred in training, suggesting a degree of rule-governed behavior. We also find, however, that performance is heavily influenced by word frequency, with experiments showing that both the absolute frequency of a verb form, as well as the frequency relative to the alternate inflection, are causally implicated in the predictions BERT makes at inference time. Closer analysis of these frequency effects reveals that BERT's behavior is consistent with a system that correctly applies the SVA rule in general but struggles to overcome strong training priors and to estimate agreement features (singular vs. plural) on infrequent lexical items.},
	urldate = {2024-04-24},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wei, Jason and Garrette, Dan and Linzen, Tal and Pavlick, Ellie},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {932--948},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\Z3LMJD35\\Wei et al. - 2021 - Frequency Effects on Syntactic Rule Learning in Tr.pdf:application/pdf},
}

@misc{chang_characterizing_2023,
	title = {Characterizing {Learning} {Curves} {During} {Language} {Model} {Pre}-{Training}: {Learning}, {Forgetting}, and {Stability}},
	shorttitle = {Characterizing {Learning} {Curves} {During} {Language} {Model} {Pre}-{Training}},
	url = {http://arxiv.org/abs/2308.15419},
	doi = {10.48550/arXiv.2308.15419},
	abstract = {How do language models learn to make predictions during pre-training? To study this question, we extract learning curves from five autoregressive English language model pre-training runs, for 1M tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be "forgotten" during pre-training. Higher n-gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Effects of part-of-speech are also small, although nouns tend to be acquired later and less stably than verbs, adverbs, and adjectives. Our work contributes to a better understanding of language model pre-training dynamics and informs the deployment of stable language models in practice.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Chang, Tyler A. and Tu, Zhuowen and Bergen, Benjamin K.},
	month = aug,
	year = {2023},
	note = {arXiv:2308.15419 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\EJU5QJ4A\\Chang et al. - 2023 - Characterizing Learning Curves During Language Mod.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\7VIDN2KX\\2308.html:text/html},
}

@article{michaelov_strong_2024,
	title = {Strong {Prediction}: {Language} {Model} {Surprisal} {Explains} {Multiple} {N400} {Effects}},
	volume = {5},
	issn = {2641-4368},
	shorttitle = {Strong {Prediction}},
	url = {https://doi.org/10.1162/nol_a_00105},
	doi = {10.1162/nol_a_00105},
	abstract = {Theoretical accounts of the N400 are divided as to whether the amplitude of the N400 response to a stimulus reflects the extent to which the stimulus was predicted, the extent to which the stimulus is semantically similar to its preceding context, or both. We use state-of-the-art machine learning tools to investigate which of these three accounts is best supported by the evidence. GPT-3, a neural language model trained to compute the conditional probability of any word based on the words that precede it, was used to operationalize contextual predictability. In particular, we used an information-theoretic construct known as surprisal (the negative logarithm of the conditional probability). Contextual semantic similarity was operationalized by using two high-quality co-occurrence-derived vector-based meaning representations for words: GloVe and fastText. The cosine between the vector representation of the sentence frame and final word was used to derive contextual cosine similarity estimates. A series of regression models were constructed, where these variables, along with cloze probability and plausibility ratings, were used to predict single trial N400 amplitudes recorded from healthy adults as they read sentences whose final word varied in its predictability, plausibility, and semantic relationship to the likeliest sentence completion. Statistical model comparison indicated GPT-3 surprisal provided the best account of N400 amplitude and suggested that apparently disparate N400 effects of expectancy, plausibility, and contextual semantic similarity can be reduced to variation in the predictability of words. The results are argued to support predictive coding in the human language network.},
	number = {1},
	urldate = {2024-04-24},
	journal = {Neurobiology of Language},
	author = {Michaelov, James A. and Bardolph, Megan D. and Van Petten, Cyma K. and Bergen, Benjamin K. and Coulson, Seana},
	month = apr,
	year = {2024},
	pages = {107--135},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\K2Y5L6FS\\Michaelov et al. - 2024 - Strong Prediction Language Model Surprisal Explai.pdf:application/pdf;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\J6GB2CWC\\Strong-Prediction-Language-Model-Surprisal.html:text/html},
}

@article{trott_large_2023,
	title = {Do {Large} {Language} {Models} {Know} {What} {Humans} {Know}?},
	volume = {47},
	issn = {0364-0213, 1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cogs.13309},
	doi = {10.1111/cogs.13309},
	abstract = {Abstract
            Humans can attribute beliefs to others. However, it is unknown to what extent this ability results from an innate biological endowment or from experience accrued through child development, particularly exposure to language describing others' mental states. We test the viability of the language exposure hypothesis by assessing whether models exposed to large quantities of human language display sensitivity to the implied knowledge states of characters in written passages. In pre‐registered analyses, we present a linguistic version of the False Belief Task to both human participants and a large language model, GPT‐3. Both are sensitive to others' beliefs, but while the language model significantly exceeds chance behavior, it does not perform as well as the humans nor does it explain the full extent of their behavior—despite being exposed to more language than a human would in a lifetime. This suggests that while statistical learning from language exposure may in part explain how humans develop the ability to reason about the mental states of others, other mechanisms are also responsible.},
	language = {en},
	number = {7},
	urldate = {2024-04-24},
	journal = {Cognitive Science},
	author = {Trott, Sean and Jones, Cameron and Chang, Tyler and Michaelov, James and Bergen, Benjamin},
	month = jul,
	year = {2023},
	pages = {e13309},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\YY8NLP3F\\Trott et al. - 2023 - Do Large Language Models Know What Humans Know.pdf:application/pdf},
}

@misc{mohammadi_creativity_2024,
	title = {Creativity {Has} {Left} the {Chat}: {The} {Price} of {Debiasing} {Language} {Models}},
	shorttitle = {Creativity {Has} {Left} the {Chat}},
	url = {http://arxiv.org/abs/2406.05587},
	doi = {10.48550/arXiv.2406.05587},
	abstract = {Large Language Models (LLMs) have revolutionized natural language processing but can exhibit biases and may generate toxic content. While alignment techniques like Reinforcement Learning from Human Feedback (RLHF) reduce these issues, their impact on creativity, defined as syntactic and semantic diversity, remains unexplored. We investigate the unintended consequences of RLHF on the creativity of LLMs through three experiments focusing on the Llama-2 series. Our findings reveal that aligned models exhibit lower entropy in token predictions, form distinct clusters in the embedding space, and gravitate towards "attractor states", indicating limited output diversity. Our findings have significant implications for marketers who rely on LLMs for creative tasks such as copywriting, ad creation, and customer persona generation. The trade-off between consistency and creativity in aligned models should be carefully considered when selecting the appropriate model for a given application. We also discuss the importance of prompt engineering in harnessing the creative potential of base models.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Mohammadi, Behnam},
	month = jun,
	year = {2024},
	note = {arXiv:2406.05587 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\G4EHCB3M\\Mohammadi - 2024 - Creativity Has Left the Chat The Price of Debiasi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\RNWH28WA\\2406.html:text/html},
}

@misc{kobak_delving_2024,
	title = {Delving into {ChatGPT} usage in academic writing through excess vocabulary},
	url = {http://arxiv.org/abs/2406.07016},
	doi = {10.48550/arXiv.2406.07016},
	abstract = {Recent large language models (LLMs) can generate and revise text with human-level performance, and have been widely commercialized in systems like ChatGPT. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists have been using them to assist their scholarly writing. How wide-spread is LLM usage in the academic literature currently? To answer this question, we use an unbiased, large-scale approach, free from any assumptions on academic LLM usage. We study vocabulary changes in 14 million PubMed abstracts from 2010-2024, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. Our analysis based on excess words usage suggests that at least 10\% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, and was as high as 30\% for some PubMed sub-corpora. We show that the appearance of LLM-based writing assistants has had an unprecedented impact in the scientific literature, surpassing the effect of major world events such as the Covid pandemic.},
	urldate = {2024-06-25},
	publisher = {arXiv},
	author = {Kobak, Dmitry and Márquez, Rita González and Horvát, Emőke-Ágnes and Lause, Jan},
	month = jun,
	year = {2024},
	note = {arXiv:2406.07016 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Digital Libraries, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\FBSY6FSK\\Kobak et al. - 2024 - Delving into ChatGPT usage in academic writing thr.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\DK2FPS65\\2406.html:text/html},
}

@inproceedings{shah_correlations_2024,
	address = {Torino, Italia},
	title = {Correlations between {Multilingual} {Language} {Model} {Geometry} and {Crosslingual} {Transfer} {Performance}},
	url = {https://aclanthology.org/2024.lrec-main.361},
	abstract = {A common approach to interpreting multilingual language models is to evaluate their internal representations. For example, studies have found that languages occupy distinct subspaces in the models' representation spaces, and geometric distances between languages often reflect linguistic properties such as language families and typological features. In our work, we investigate whether geometric distances between language representations correlate with zero-shot crosslingual transfer performance for POS-tagging and NER in three multilingual language models. We consider four distance metrics, including new metrics that identify a basis for a multilingual representation space that sorts axes based on their language-separability. We find that each distance metric either only moderately correlates or does not correlate with crosslingual transfer performance, and metrics do not generalize well across models, layers, and tasks. Although pairwise language separability is a reasonable predictor of crosslingual transfer, representational geometry overall is an inconsistent predictor for the crosslingual performance of multilingual language models.},
	urldate = {2024-06-25},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Shah, Cheril and Chandak, Yashashree and Mane, Atharv Mahesh and Bergen, Benjamin and Chang, Tyler A.},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {4059--4066},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\TZVVWZR2\\Shah et al. - 2024 - Correlations between Multilingual Language Model G.pdf:application/pdf},
}

@inproceedings{conneau_emerging_2020,
	address = {Online},
	title = {Emerging {Cross}-lingual {Structure} in {Pretrained} {Language} {Models}},
	url = {https://aclanthology.org/2020.acl-main.536},
	doi = {10.18653/v1/2020.acl-main.536},
	abstract = {We study the problem of multilingual masked language modeling, i.e. the training of a single model on concatenated text from multiple languages, and present a detailed study of several factors that influence why these models are so effective for cross-lingual transfer. We show, contrary to what was previously hypothesized, that transfer is possible even when there is no shared vocabulary across the monolingual corpora and also when the text comes from very different domains. The only requirement is that there are some shared parameters in the top layers of the multi-lingual encoder. To better understand this result, we also show that representations from monolingual BERT models in different languages can be aligned post-hoc quite effectively, strongly suggesting that, much like for non-contextual word embeddings, there are universal latent symmetries in the learned embedding spaces. For multilingual masked language modeling, these symmetries are automatically discovered and aligned during the joint training process.},
	urldate = {2024-06-25},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Conneau, Alexis and Wu, Shijie and Li, Haoran and Zettlemoyer, Luke and Stoyanov, Veselin},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {6022--6034},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\KAKXYSQQ\\Conneau et al. - 2020 - Emerging Cross-lingual Structure in Pretrained Lan.pdf:application/pdf},
}

@inproceedings{philippy_identifying_2023,
	title = {Identifying the {Correlation} {Between} {Language} {Distance} and {Cross}-{Lingual} {Transfer} in a {Multilingual} {Representation} {Space}},
	url = {http://arxiv.org/abs/2305.02151},
	doi = {10.18653/v1/2023.sigtyp-1.3},
	abstract = {Prior research has investigated the impact of various linguistic features on cross-lingual transfer performance. In this study, we investigate the manner in which this effect can be mapped onto the representation space. While past studies have focused on the impact on cross-lingual alignment in multilingual language models during fine-tuning, this study examines the absolute evolution of the respective language representation spaces produced by MLLMs. We place a specific emphasis on the role of linguistic characteristics and investigate their inter-correlation with the impact on representation spaces and cross-lingual transfer performance. Additionally, this paper provides preliminary evidence of how these findings can be leveraged to enhance transfer to linguistically distant languages.},
	urldate = {2024-06-25},
	booktitle = {Proceedings of the 5th {Workshop} on {Research} in {Computational} {Linguistic} {Typology} and {Multilingual} {NLP}},
	author = {Philippy, Fred and Guo, Siwen and Haddadan, Shohreh},
	year = {2023},
	note = {arXiv:2305.02151 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	pages = {22--29},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\M9NM5X5P\\Philippy et al. - 2023 - Identifying the Correlation Between Language Dista.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\2K3AJEZQ\\2305.html:text/html},
}

@misc{philippy_towards_2023,
	title = {Towards a {Common} {Understanding} of {Contributing} {Factors} for {Cross}-{Lingual} {Transfer} in {Multilingual} {Language} {Models}: {A} {Review}},
	shorttitle = {Towards a {Common} {Understanding} of {Contributing} {Factors} for {Cross}-{Lingual} {Transfer} in {Multilingual} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.16768},
	doi = {10.48550/arXiv.2305.16768},
	abstract = {In recent years, pre-trained Multilingual Language Models (MLLMs) have shown a strong ability to transfer knowledge across different languages. However, given that the aspiration for such an ability has not been explicitly incorporated in the design of the majority of MLLMs, it is challenging to obtain a unique and straightforward explanation for its emergence. In this review paper, we survey literature that investigates different factors contributing to the capacity of MLLMs to perform zero-shot cross-lingual transfer and subsequently outline and discuss these factors in detail. To enhance the structure of this review and to facilitate consolidation with future studies, we identify five categories of such factors. In addition to providing a summary of empirical evidence from past studies, we identify consensuses among studies with consistent findings and resolve conflicts among contradictory ones. Our work contextualizes and unifies existing research streams which aim at explaining the cross-lingual potential of MLLMs. This review provides, first, an aligned reference point for future research and, second, guidance for a better-informed and more efficient way of leveraging the cross-lingual capacity of MLLMs.},
	urldate = {2024-06-25},
	publisher = {arXiv},
	author = {Philippy, Fred and Guo, Siwen and Haddadan, Shohreh},
	month = may,
	year = {2023},
	note = {arXiv:2305.16768 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\MXGJ33MB\\Philippy et al. - 2023 - Towards a Common Understanding of Contributing Fac.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\95DUK5SB\\2305.html:text/html},
}

@inproceedings{littell_uriel_2017,
	address = {Valencia, Spain},
	title = {{URIEL} and lang2vec: {Representing} languages as typological, geographical, and phylogenetic vectors},
	shorttitle = {{URIEL} and lang2vec},
	url = {https://aclanthology.org/E17-2002},
	abstract = {We introduce the URIEL knowledge base for massively multilingual NLP and the lang2vec utility, which provides information-rich vector identifications of languages drawn from typological, geographical, and phylogenetic databases and normalized to have straightforward and consistent formats, naming, and semantics. The goal of URIEL and lang2vec is to enable multilingual NLP, especially on less-resourced languages and make possible types of experiments (especially but not exclusively related to NLP tasks) that are otherwise difficult or impossible due to the sparsity and incommensurability of the data sources. lang2vec vectors have been shown to reduce perplexity in multilingual language modeling, when compared to one-hot language identification vectors.},
	urldate = {2024-06-25},
	booktitle = {Proceedings of the 15th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Volume} 2, {Short} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Littell, Patrick and Mortensen, David R. and Lin, Ke and Kairis, Katherine and Turner, Carlisle and Levin, Lori},
	editor = {Lapata, Mirella and Blunsom, Phil and Koller, Alexander},
	month = apr,
	year = {2017},
	pages = {8--14},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\GJ5T9KT6\\Littell et al. - 2017 - URIEL and lang2vec Representing languages as typo.pdf:application/pdf},
}

@misc{lin_choosing_2019,
	title = {Choosing {Transfer} {Languages} for {Cross}-{Lingual} {Learning}},
	url = {http://arxiv.org/abs/1905.12688},
	abstract = {Cross-lingual transfer, where a high-resource transfer language is used to improve the accuracy of a low-resource task language, is now an invaluable tool for improving performance of natural language processing (NLP) on low-resource languages. However, given a particular task language, it is not clear which language to transfer from, and the standard strategy is to select languages based on ad hoc criteria, usually the intuition of the experimenter. Since a large number of features contribute to the success of cross-lingual transfer (including phylogenetic similarity, typological properties, lexical overlap, or size of available data), even the most enlightened experimenter rarely considers all these factors for the particular task at hand. In this paper, we consider this task of automatically selecting optimal transfer languages as a ranking problem, and build models that consider the aforementioned features to perform this prediction. In experiments on representative NLP tasks, we demonstrate that our model predicts good transfer languages much better than ad hoc baselines considering single features in isolation, and glean insights on what features are most informative for each different NLP tasks, which may inform future ad hoc selection even without use of our method. Code, data, and pre-trained models are available at https://github.com/neulab/langrank},
	urldate = {2024-06-25},
	publisher = {arXiv},
	author = {Lin, Yu-Hsiang and Chen, Chian-Yu and Lee, Jean and Li, Zirui and Zhang, Yuyan and Xia, Mengzhou and Rijhwani, Shruti and He, Junxian and Zhang, Zhisong and Ma, Xuezhe and Anastasopoulos, Antonios and Littell, Patrick and Neubig, Graham},
	month = jun,
	year = {2019},
	note = {arXiv:1905.12688 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\4PCGNVRI\\Lin et al. - 2019 - Choosing Transfer Languages for Cross-Lingual Lear.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\36YKYAEI\\1905.html:text/html},
}

@misc{perez_red_2022,
	title = {Red {Teaming} {Language} {Models} with {Language} {Models}},
	url = {http://arxiv.org/abs/2202.03286},
	abstract = {Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases ("red teaming") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.},
	urldate = {2024-07-11},
	publisher = {arXiv},
	author = {Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
	month = feb,
	year = {2022},
	note = {arXiv:2202.03286 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\HWU64H2J\\Perez et al. - 2022 - Red Teaming Language Models with Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\P7YQ2VL9\\2202.html:text/html},
}

@misc{ziegler_fine-tuning_2020,
	title = {Fine-{Tuning} {Language} {Models} from {Human} {Preferences}},
	url = {http://arxiv.org/abs/1909.08593},
	abstract = {Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.},
	urldate = {2024-07-11},
	publisher = {arXiv},
	author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
	month = jan,
	year = {2020},
	note = {arXiv:1909.08593 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\SAXQ3FYW\\Ziegler et al. - 2020 - Fine-Tuning Language Models from Human Preferences.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\85JKYFIL\\1909.html:text/html},
}

@misc{bai_training_2022,
	title = {Training a {Helpful} and {Harmless} {Assistant} with {Reinforcement} {Learning} from {Human} {Feedback}},
	url = {http://arxiv.org/abs/2204.05862},
	abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
	urldate = {2024-07-11},
	publisher = {arXiv},
	author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
	month = apr,
	year = {2022},
	note = {arXiv:2204.05862 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\99R8IMIR\\Bai et al. - 2022 - Training a Helpful and Harmless Assistant with Rei.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\AXIILAEW\\2204.html:text/html},
}

@article{gernsbacher_resolving_1984,
	title = {Resolving 20 years of inconsistent interactions between lexical familiarity and orthography, concreteness, and polysemy},
	volume = {113},
	issn = {1939-2222},
	doi = {10.1037/0096-3445.113.2.256},
	abstract = {Word recognition studies conducted over the past 2 decades manipulated lexical familiarity by presenting words of high vs low printed frequency, and most reported an interaction between printed frequency and one of several second variables, namely, orthographic regularity, semantic concreteness, or polysemy. However, the direction of these interactions was inconsistent from study to study. Six new experiments clarify these discordant results. Exps I and II, conducted with 89 college students, demonstrate that words of the same low printed frequency are not always equally familiar to Ss. Instead, Ss' ratings of experiential familiarity suggest that many of the low-printed-frequency words used in prior studies varied along this dimension. Four lexical decision experiments, conducted with 78 undergraduates, reexamined the prior findings by orthogonally manipulating lexical familiarity, as assessed by experiential familiarity ratings, with bigram frequency, semantic concreteness, and number of meanings. Results suggest that of these variables, only experiential familiarity reliably affects word recognition latencies. This in turn suggests that previous inconsistent findings were due to confounding experiential familiarity with a second variable. (68 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {2},
	journal = {Journal of Experimental Psychology: General},
	author = {Gernsbacher, Morton A.},
	year = {1984},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Familiarity, Recognition (Learning), Semantics, Word Frequency, Orthography, Response Latency, Word Meaning},
	pages = {256--281},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\7MHWRWZW\\1985-08562-001.html:text/html},
}

@misc{atari_which_2023,
  title    = {Which humans?},
  author   = {Atari, Mohammad and Xue, Mona J. and Park, Peter S. and Blasi, D. and Henrich, Joseph},
  year     = {2023},
  howpublished = {\url{https://osf.io/preprints/psyarxiv/5b26t/}},
  note     = {Preprint. Publisher: PsyArXiv},
  urldate  = {2024-07-03}
}

@misc{van_dijk_chiscor_2023,
	title = {{ChiSCor}: {A} {Corpus} of {Freely} {Told} {Fantasy} {Stories} by {Dutch} {Children} for {Computational} {Linguistics} and {Cognitive} {Science}},
	shorttitle = {{ChiSCor}},
	url = {http://arxiv.org/abs/2310.20328},
	doi = {10.48550/arXiv.2310.20328},
	abstract = {In this resource paper we release ChiSCor, a new corpus containing 619 fantasy stories, told freely by 442 Dutch children aged 4-12. ChiSCor was compiled for studying how children render character perspectives, and unravelling language and cognition in development, with computational tools. Unlike existing resources, ChiSCor's stories were produced in natural contexts, in line with recent calls for more ecologically valid datasets. ChiSCor hosts text, audio, and annotations for character complexity and linguistic complexity. Additional metadata (e.g. education of caregivers) is available for one third of the Dutch children. ChiSCor also includes a small set of 62 English stories. This paper details how ChiSCor was compiled and shows its potential for future work with three brief case studies: i) we show that the syntactic complexity of stories is strikingly stable across children's ages; ii) we extend work on Zipfian distributions in free speech and show that ChiSCor obeys Zipf's law closely, reflecting its social context; iii) we show that even though ChiSCor is relatively small, the corpus is rich enough to train informative lemma vectors that allow us to analyse children's language use. We end with a reflection on the value of narrative datasets in computational linguistics.},
	urldate = {2024-07-03},
	publisher = {arXiv},
	author = {van Dijk, Bram M. A. and van Duijn, Max J. and Verberne, Suzan and Spruit, Marco R.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.20328 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\YAPRHRNX\\van Dijk et al. - 2023 - ChiSCor A Corpus of Freely Told Fantasy Stories b.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\XYMHX8IV\\2310.html:text/html},
}

@article{kauf_event_2023,
	title = {Event {Knowledge} in {Large} {Language} {Models}: {The} {Gap} {Between} the {Impossible} and the {Unlikely}},
	volume = {47},
	issn = {0364-0213, 1551-6709},
	shorttitle = {Event {Knowledge} in {Large} {Language} {Models}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cogs.13386},
	doi = {10.1111/cogs.13386},
	abstract = {Abstract
            
              Word co‐occurrence patterns in language corpora contain a surprising amount of conceptual knowledge. Large language models (LLMs), trained to predict words in context, leverage these patterns to achieve impressive performance on diverse semantic tasks requiring world knowledge. An important but understudied question about LLMs’ semantic abilities is whether they acquire generalized knowledge of common events. Here, we test whether five pretrained LLMs (from 2018's BERT to 2023's MPT) assign a higher likelihood to plausible descriptions of agent−patient interactions than to minimally different implausible versions of the same event. Using three curated sets of minimal sentence pairs (total
              n
              = 1215), we found that pretrained LLMs possess substantial event knowledge, outperforming other distributional language models. In particular, they almost always assign a higher likelihood to possible versus impossible events (
              The teacher bought the laptop
              vs.
              The laptop bought the teacher
              ). However, LLMs show less consistent preferences for likely versus unlikely events (
              The nanny tutored the boy
              vs.
              The boy tutored the nanny
              ). In follow‐up analyses, we show that (i) LLM scores are driven by both plausibility and surface‐level sentence features, (ii) LLM scores generalize well across syntactic variants (active vs. passive constructions) but less well across semantic variants (synonymous sentences), (iii) some LLM errors mirror human judgment ambiguity, and (iv) sentence plausibility serves as an organizing dimension in internal LLM representations. Overall, our results show that important aspects of event knowledge naturally emerge from distributional linguistic patterns, but also highlight a gap between representations of possible/impossible and likely/unlikely events.},
	language = {en},
	number = {11},
	urldate = {2024-07-03},
	journal = {Cognitive Science},
	author = {Kauf, Carina and Ivanova, Anna A. and Rambelli, Giulia and Chersoni, Emmanuele and She, Jingyuan Selena and Chowdhury, Zawad and Fedorenko, Evelina and Lenci, Alessandro},
	month = nov,
	year = {2023},
	pages = {e13386},
}

@article{contreras_kallens_large_2023,
	title = {Large {Language} {Models} {Demonstrate} the {Potential} of {Statistical} {Learning} in {Language}},
	volume = {47},
	issn = {0364-0213, 1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cogs.13256},
	doi = {10.1111/cogs.13256},
	abstract = {Abstract
            To what degree can language be acquired from linguistic input alone? This question has vexed scholars for millennia and is still a major focus of debate in the cognitive science of language. The complexity of human language has hampered progress because studies of language–especially those involving computational modeling–have only been able to deal with small fragments of our linguistic skills. We suggest that the most recent generation of Large Language Models (LLMs) might finally provide the computational tools to determine empirically how much of the human language ability can be acquired from linguistic experience. LLMs are sophisticated deep learning architectures trained on vast amounts of natural language data, enabling them to perform an impressive range of linguistic tasks. We argue that, despite their clear semantic and pragmatic limitations, LLMs have already demonstrated that human‐like grammatical language can be acquired without the need for a built‐in grammar. Thus, while there is still much to learn about how humans acquire and use language, LLMs provide full‐fledged computational models for cognitive scientists to empirically evaluate just how far statistical learning might take us in explaining the full complexity of human language.},
	language = {en},
	number = {3},
	urldate = {2024-07-03},
	journal = {Cognitive Science},
	author = {Contreras Kallens, Pablo and Kristensen‐McLachlan, Ross Deans and Christiansen, Morten H.},
	month = mar,
	year = {2023},
	pages = {e13256},
}

@article{tuckute_driving_2024,
	title = {Driving and suppressing the human language network using large language models},
	volume = {8},
	copyright = {2024 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-023-01783-7},
	doi = {10.1038/s41562-023-01783-7},
	abstract = {Transformer models such as GPT generate human-like language and are predictive of human brain responses to language. Here, using functional-MRI-measured brain responses to 1,000 diverse sentences, we first show that a GPT-based encoding model can predict the magnitude of the brain response associated with each sentence. We then use the model to identify new sentences that are predicted to drive or suppress responses in the human language network. We show that these model-selected novel sentences indeed strongly drive and suppress the activity of human language areas in new individuals. A systematic analysis of the model-selected sentences reveals that surprisal and well-formedness of linguistic input are key determinants of response strength in the language network. These results establish the ability of neural network models to not only mimic human language but also non-invasively control neural activity in higher-level cortical areas, such as the language network.},
	language = {en},
	number = {3},
	urldate = {2024-07-03},
	journal = {Nature Human Behaviour},
	author = {Tuckute, Greta and Sathe, Aalok and Srikant, Shashank and Taliaferro, Maya and Wang, Mingye and Schrimpf, Martin and Kay, Kendrick and Fedorenko, Evelina},
	month = mar,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Language and linguistics, Neuroscience},
	pages = {544--561},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\W3VPUM74\\Tuckute et al. - 2024 - Driving and suppressing the human language network.pdf:application/pdf},
}

@article{hosseini_artificial_2022,
	title = {Artificial neural network language models align neurally and behaviorally with humans even after a developmentally realistic amount of training},
	url = {https://www.biorxiv.org/content/10.1101/2022.10.04.510681.abstract},
	urldate = {2024-07-03},
	journal = {BioRxiv},
	author = {Hosseini, Eghbal A. and Schrimpf, Martin and Zhang, Yian and Bowman, Samuel and Zaslavsky, Noga and Fedorenko, Evelina},
	year = {2022},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {2022--10},
}

@book{vasishth_chapter_nodate,
	title = {Chapter 20 {A} simple accumulator model to account for choice response time {\textbar} {An} {Introduction} to {Bayesian} {Data} {Analysis} for {Cognitive} {Science}},
	url = {https://vasishth.github.io/bayescogsci/book/ch-lognormalrace.html#ch-lognormalrace},
	abstract = {An introduction to Bayesian data analysis for Cognitive Science.},
	urldate = {2024-06-27},
	author = {Vasishth, Daniel Schad, {and} Shravan, Bruno Nicenboim},
	file = {Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\2JMWJFUX\\ch-lognormalrace.html:text/html},
}

@article{schaeffer_are_2023,
	title = {Are {Emergent} {Abilities} of {Large} {Language} {Models} a {Mirage}?},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/adc98a266f45005c403b8311ca7e8bd7-Abstract-Conference.html},
	language = {en},
	urldate = {2024-06-27},
	journal = {Advances in Neural Information Processing Systems},
	author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
	month = dec,
	year = {2023},
	pages = {55565--55581},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\WPA5C9YL\\Schaeffer et al. - 2023 - Are Emergent Abilities of Large Language Models a .pdf:application/pdf},
}

@article{chung_scaling_2024,
	title = {Scaling {Instruction}-{Finetuned} {Language} {Models}},
	volume = {25},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v25/23-0870.html},
	abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation, RealToxicityPrompts). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PaLM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks (at time of release), such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
	number = {70},
	urldate = {2024-06-27},
	journal = {Journal of Machine Learning Research},
	author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
	year = {2024},
	pages = {1--53},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\X4FWUMEH\\Chung et al. - 2024 - Scaling Instruction-Finetuned Language Models.pdf:application/pdf},
}

@inproceedings{kandpal_large_2023,
	title = {Large {Language} {Models} {Struggle} to {Learn} {Long}-{Tail} {Knowledge}},
	url = {https://proceedings.mlr.press/v202/kandpal23a.html},
	abstract = {The Internet contains a wealth of knowledge—from the birthdays of historical figures to tutorials on how to code—all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model’s ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models are better at learning long-tail knowledge, we estimate that today’s models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail.},
	language = {en},
	urldate = {2024-06-27},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {15696--15707},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\LUIKAG6I\\Kandpal et al. - 2023 - Large Language Models Struggle to Learn Long-Tail .pdf:application/pdf},
}

@misc{eldan_tinystories_2023,
	title = {{TinyStories}: {How} {Small} {Can} {Language} {Models} {Be} and {Still} {Speak} {Coherent} {English}?},
	shorttitle = {{TinyStories}},
	url = {http://arxiv.org/abs/2305.07759},
	doi = {10.48550/arXiv.2305.07759},
	abstract = {Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Eldan, Ronen and Li, Yuanzhi},
	month = may,
	year = {2023},
	note = {arXiv:2305.07759 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\9GNQMYUY\\Eldan and Li - 2023 - TinyStories How Small Can Language Models Be and .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\NE7JDZEF\\2305.html:text/html},
}

@article{speer_luminosoinsightwordfreq_2018,
	title = {Luminosoinsight/wordfreq: v2. 2},
	shorttitle = {Luminosoinsight/wordfreq},
	url = {https://scholar.google.com/scholar?cluster=15587437373231344023&hl=en&oi=scholarr},
	urldate = {2024-06-26},
	journal = {Zenodo [Computer Software]},
	author = {Speer, Robyn and Chin, Joshua and Lin, Andrew and Jewett, Sara and Nathan, Lance},
	year = {2018},
}

@misc{shah_geometry_2023,
	title = {The {Geometry} of {Multilingual} {Language} {Models}: {An} {Equality} {Lens}},
	shorttitle = {The {Geometry} of {Multilingual} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.07839},
	doi = {10.48550/arXiv.2305.07839},
	abstract = {Understanding the representations of different languages in multilingual language models is essential for comprehending their cross-lingual properties, predicting their performance on downstream tasks, and identifying any biases across languages. In our study, we analyze the geometry of three multilingual language models in Euclidean space and find that all languages are represented by unique geometries. Using a geometric separability index we find that although languages tend to be closer according to their linguistic family, they are almost separable with languages from other families. We also introduce a Cross-Lingual Similarity Index to measure the distance of languages with each other in the semantic space. Our findings indicate that the low-resource languages are not represented as good as high resource languages in any of the models},
	urldate = {2024-06-26},
	publisher = {arXiv},
	author = {Shah, Cheril and Chandak, Yashashree and Suri, Manan},
	month = may,
	year = {2023},
	note = {arXiv:2305.07839 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\NG38R4LK\\Shah et al. - 2023 - The Geometry of Multilingual Language Models An E.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\KGU6N7FH\\2305.html:text/html},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-08-09},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\7J4YYFPY\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\WBVAHVIF\\1706.html:text/html},
}

@inproceedings{bender_dangers_2021,
	address = {Virtual Event Canada},
	title = {On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models} {Be} {Too} {Big}?},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://dl.acm.org/doi/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	language = {en},
	urldate = {2024-08-09},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	month = mar,
	year = {2021},
	pages = {610--623},
	file = {Full Text:C\:\\Users\\Job Schepens\\Zotero\\storage\\L3HEEAKU\\Bender et al. - 2021 - On the Dangers of Stochastic Parrots Can Language.pdf:application/pdf},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2024-08-09},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\TK3GQSXX\\Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\JKFIC38Y\\2203.html:text/html},
}

@article{fox_generalized_1992,
	title = {Generalized {Collinearity} {Diagnostics}},
	volume = {87},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10475190},
	doi = {10.1080/01621459.1992.10475190},
	language = {en},
	number = {417},
	urldate = {2024-08-20},
	journal = {Journal of the American Statistical Association},
	author = {Fox, John and Monette, Georges},
	month = mar,
	year = {1992},
	pages = {178--183},
	file = {Available Version (via Google Scholar):C\:\\Users\\Job Schepens\\Zotero\\storage\\J2VD9LV3\\Fox and Monette - 1992 - Generalized Collinearity Diagnostics.pdf:application/pdf},
}

@article{hussain_tutorial_2024,
	title = {A tutorial on open-source large language models for behavioral science},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-024-02455-8},
	doi = {10.3758/s13428-024-02455-8},
	abstract = {Large language models (LLMs) have the potential to revolutionize behavioral science by accelerating and improving the research cycle, from conceptualization to data analysis. Unlike closed-source solutions, open-source frameworks for LLMs can enable transparency, reproducibility, and adherence to data protection standards, which gives them a crucial advantage for use in behavioral science. To help researchers harness the promise of LLMs, this tutorial offers a primer on the open-source Hugging Face ecosystem and demonstrates several applications that advance conceptual and empirical work in behavioral science, including feature extraction, fine-tuning of models for prediction, and generation of behavioral responses. Executable code is made available at github.com/Zak-Hussain/LLM4BeSci.git. Finally, the tutorial discusses challenges faced by research with (open-source) LLMs related to interpretability and safety and offers a perspective on future research at the intersection of language modeling and behavioral science.},
	language = {en},
	urldate = {2024-08-20},
	journal = {Behavior Research Methods},
	author = {Hussain, Zak and Binz, Marcel and Mata, Rui and Wulff, Dirk U.},
	month = aug,
	year = {2024},
	keywords = {Large language models, Artificial Intelligence, Behavioral science, Hugging face},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\JE2U4CGX\\Hussain et al. - 2024 - A tutorial on open-source large language models fo.pdf:application/pdf},
}

@misc{chilson_films_2024,
	title = {{FILMS}: {A} {Multilingual} {Word} {Frequency} {Corpus} based on {Film} {Subtitles} with {IPA} {Transcriptions}},
	shorttitle = {{FILMS}},
	url = {https://osf.io/zy5qf},
	doi = {10.31219/osf.io/zy5qf},
	abstract = {Word frequency plays an important role in diverse areas of psychology research, such as reading, memory, and word processing, with researchers traditionally relying on existing word-frequency corpora for their investigations. However, not all corpora are created equal, and factors such as data and language domain can significantly impact research outcomes. This is particularly problematic for cross-language research, where linguistic material needs to be comparable across languages. Databases derived from written texts have limitations in reflecting everyday language use, since people speak differently than they write. Recent research highlights the effectiveness of movie subtitles-based word frequency data, particularly for corpora exceeding 30 million words. Responding to these findings, this paper introduces the multilingual word-frequency corpus FILMS (Word Frequency IPA MultiLingual Subtitles Corpus), comprised of 52 languages from open-source subtitles. This paper provides a comprehensive account of the motivation for creating this corpus, the creation of the corpus, sources of data, a statistical overview, and future prospects. The goal of this project is to offer a corpus tailored to research in the field of psychology.},
	language = {en-us},
	urldate = {2024-08-13},
	publisher = {OSF},
	author = {Chilson, Sara and Schmalz, Xenia and Sineva, Elizaveta},
	month = jul,
	year = {2024},
	keywords = {multilingual, lexical decision, word frequency, corpus, subtitles, word frequency corpus, Zipf scale},
}

@misc{hu_auxiliary_2024,
	title = {Auxiliary task demands mask the capabilities of smaller language models},
	url = {http://arxiv.org/abs/2404.02418},
	abstract = {Developmental psychologists have argued about when cognitive capacities such as language understanding or theory of mind emerge. These debates often hinge on the concept of "task demands" -- the auxiliary challenges associated with performing a particular evaluation -- that may mask the child's underlying ability. The same issues arise when measuring the capacities of language models (LMs): performance on a task is a function of the model's underlying knowledge, combined with the model's ability to interpret and perform the task given its available resources. Here, we show that for analogical reasoning, reflective reasoning, word prediction, and grammaticality judgments, evaluation methods with greater task demands yield lower performance than evaluations with reduced demands. This "demand gap" is most pronounced for models with fewer parameters and less training data. Our results illustrate that LM performance should not be interpreted as a direct indication of intelligence (or lack thereof), but as a reflection of capacities seen through the lens of researchers' design choices.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Hu, Jennifer and Frank, Michael C.},
	month = jul,
	year = {2024},
	note = {arXiv:2404.02418 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\W76MB5RL\\Hu and Frank - 2024 - Auxiliary task demands mask the capabilities of sm.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\IA2Q5RY2\\2404.html:text/html},
}

@misc{tan_devbench_2024,
	title = {{DevBench}: {A} multimodal developmental benchmark for language learning},
	shorttitle = {{DevBench}},
	url = {http://arxiv.org/abs/2406.10215},
	abstract = {How (dis)similar are the learning trajectories of vision-language models and children? Recent modeling work has attempted to understand the gap between models' and humans' data efficiency by constructing models trained on less data, especially multimodal naturalistic data. However, such models are often evaluated on adult-level benchmarks, with limited breadth in language abilities tested, and without direct comparison to behavioral data. We introduce DevBench, a multimodal benchmark comprising seven language evaluation tasks spanning the domains of lexical, syntactic, and semantic ability, with behavioral data from both children and adults. We evaluate a set of vision-language models on these tasks, comparing models and humans not only on accuracy but on their response patterns. Across tasks, models exhibit variation in their closeness to human response patterns, and models that perform better on a task also more closely resemble human behavioral responses. We also examine the developmental trajectory of OpenCLIP over training, finding that greater training results in closer approximations to adult response patterns. DevBench thus provides a benchmark for comparing models to human language development. These comparisons highlight ways in which model and human language learning processes diverge, providing insight into entry points for improving language models.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Tan, Alvin Wei Ming and Yu, Sunny and Long, Bria and Ma, Wanjing Anya and Murray, Tonya and Silverman, Rebecca D. and Yeatman, Jason D. and Frank, Michael C.},
	month = jun,
	year = {2024},
	note = {arXiv:2406.10215 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\88WE99QB\\Tan et al. - 2024 - DevBench A multimodal developmental benchmark for.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\WXQFZMTL\\2406.html:text/html},
}

@misc{feng_is_2024,
	title = {Is {Child}-{Directed} {Speech} {Effective} {Training} {Data} for {Language} {Models}?},
	url = {http://arxiv.org/abs/2408.03617},
	abstract = {While high-performing language models are typically trained on hundreds of billions of words, human children become fluent language users with a much smaller amount of data. What are the features of the data they receive, and how do these features support language modeling objectives? To investigate this question, we train GPT-2 models on 29M words of English-language child-directed speech and a new matched, synthetic dataset (TinyDialogues), comparing to a heterogeneous blend of datasets from the BabyLM challenge. We evaluate both the syntactic and semantic knowledge of these models using developmentally-inspired evaluations. Through pretraining experiments, we test whether the global developmental ordering or the local discourse ordering of children's training data support high performance relative to other datasets. The local properties of the data affect model results, but somewhat surprisingly, global properties do not. Further, child language input is not uniquely valuable for training language models. These findings support the hypothesis that, rather than proceeding from better data, children's learning is instead substantially more efficient than current language modeling techniques.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Feng, Steven Y. and Goodman, Noah D. and Frank, Michael C.},
	month = aug,
	year = {2024},
	note = {arXiv:2408.03617 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\LV8NYE95\\Feng et al. - 2024 - Is Child-Directed Speech Effective Training Data f.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\XELTUPF9\\2408.html:text/html},
}

@misc{liu_datasets_2024,
	title = {Datasets for {Large} {Language} {Models}: {A} {Comprehensive} {Survey}},
	shorttitle = {Datasets for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2402.18041},
	abstract = {This paper embarks on an exploration into the Large Language Model (LLM) datasets, which play a crucial role in the remarkable advancements of LLMs. The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of LLMs. Consequently, examination of these datasets emerges as a critical topic in research. In order to address the current lack of a comprehensive overview and thorough analysis of LLM datasets, and to gain insights into their current status and future trends, this survey consolidates and categorizes the fundamental aspects of LLM datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Language Processing (NLP) Datasets. The survey sheds light on the prevailing challenges and points out potential avenues for future investigation. Additionally, a comprehensive review of the existing available dataset resources is also provided, including statistics from 444 datasets, covering 8 language categories and spanning 32 domains. Information from 20 dimensions is incorporated into the dataset statistics. The total data size surveyed surpasses 774.5 TB for pre-training corpora and 700M instances for other datasets. We aim to present the entire landscape of LLM text datasets, serving as a comprehensive reference for researchers in this field and contributing to future studies. Related resources are available at: https://github.com/lmmlzn/Awesome-LLMs-Datasets.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Liu, Yang and Cao, Jiahuan and Liu, Chongyu and Ding, Kai and Jin, Lianwen},
	month = feb,
	year = {2024},
	note = {arXiv:2402.18041 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\7AVJNFRC\\Liu et al. - 2024 - Datasets for Large Language Models A Comprehensiv.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\972HHHQL\\2402.html:text/html},
}

@article{hussain_tutorial_2024-1,
	title = {A tutorial on open-source large language models for behavioral science},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-024-02455-8},
	doi = {10.3758/s13428-024-02455-8},
	abstract = {Large language models (LLMs) have the potential to revolutionize behavioral science by accelerating and improving the research cycle, from conceptualization to data analysis. Unlike closed-source solutions, open-source frameworks for LLMs can enable transparency, reproducibility, and adherence to data protection standards, which gives them a crucial advantage for use in behavioral science. To help researchers harness the promise of LLMs, this tutorial offers a primer on the open-source Hugging Face ecosystem and demonstrates several applications that advance conceptual and empirical work in behavioral science, including feature extraction, fine-tuning of models for prediction, and generation of behavioral responses. Executable code is made available at github.com/Zak-Hussain/LLM4BeSci.git. Finally, the tutorial discusses challenges faced by research with (open-source) LLMs related to interpretability and safety and offers a perspective on future research at the intersection of language modeling and behavioral science.},
	language = {en},
	urldate = {2024-08-28},
	journal = {Behavior Research Methods},
	author = {Hussain, Zak and Binz, Marcel and Mata, Rui and Wulff, Dirk U.},
	month = aug,
	year = {2024},
	keywords = {Large language models, Artificial Intelligence, Behavioral science, Hugging face},
	file = {Full Text PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\65JNJ58A\\Hussain et al. - 2024 - A tutorial on open-source large language models fo.pdf:application/pdf},
}

@article{munoz-ortiz_contrasting_2024,
	title = {Contrasting {Linguistic} {Patterns} in {Human} and {LLM}-{Generated} {Text}},
	volume = {57},
	issn = {1573-7462},
	url = {http://arxiv.org/abs/2308.09067},
	doi = {10.1007/s10462-024-10903-2},
	abstract = {We conduct a quantitative analysis contrasting human-written English news text with comparable large language model (LLM) output from six different LLMs that cover three different families and four sizes in total. Our analysis spans several measurable linguistic dimensions, including morphological, syntactic, psychometric, and sociolinguistic aspects. The results reveal various measurable differences between human and AI-generated texts. Human texts exhibit more scattered sentence length distributions, more variety of vocabulary, a distinct use of dependency and constituent types, shorter constituents, and more optimized dependency distances. Humans tend to exhibit stronger negative emotions (such as fear and disgust) and less joy compared to text generated by LLMs, with the toxicity of these models increasing as their size grows. LLM outputs use more numbers, symbols and auxiliaries (suggesting objective language) than human texts, as well as more pronouns. The sexist bias prevalent in human text is also expressed by LLMs, and even magnified in all of them but one. Differences between LLMs and humans are larger than between LLMs.},
	number = {10},
	urldate = {2024-08-29},
	journal = {Artificial Intelligence Review},
	author = {Muñoz-Ortiz, Alberto and Gómez-Rodríguez, Carlos and Vilares, David},
	month = aug,
	year = {2024},
	note = {arXiv:2308.09067 [cs]},
	keywords = {Computer Science - Computation and Language, 68T50, I.2.7},
	pages = {265},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\GTTWMRHH\\Muñoz-Ortiz et al. - 2024 - Contrasting Linguistic Patterns in Human and LLM-G.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\RBPYZFW9\\2308.html:text/html},
}

@misc{wu_survey_2024,
	title = {A {Survey} on {LLM}-{Generated} {Text} {Detection}: {Necessity}, {Methods}, and {Future} {Directions}},
	shorttitle = {A {Survey} on {LLM}-{Generated} {Text} {Detection}},
	url = {http://arxiv.org/abs/2310.14724},
	doi = {10.48550/arXiv.2310.14724},
	abstract = {The powerful ability to understand, follow, and generate complex language emerging from large language models (LLMs) makes LLM-generated text flood many areas of our daily lives at an incredible speed and is widely accepted by humans. As LLMs continue to expand, there is an imperative need to develop detectors that can detect LLM-generated text. This is crucial to mitigate potential misuse of LLMs and safeguard realms like artistic expression and social networks from harmful influence of LLM-generated content. The LLM-generated text detection aims to discern if a piece of text was produced by an LLM, which is essentially a binary classification task. The detector techniques have witnessed notable advancements recently, propelled by innovations in watermarking techniques, statistics-based detectors, neural-base detectors, and human-assisted methods. In this survey, we collate recent research breakthroughs in this area and underscore the pressing need to bolster detector research. We also delve into prevalent datasets, elucidating their limitations and developmental requirements. Furthermore, we analyze various LLM-generated text detection paradigms, shedding light on challenges like out-of-distribution problems, potential attacks, real-world data issues and the lack of effective evaluation framework. Conclusively, we highlight interesting directions for future research in LLM-generated text detection to advance the implementation of responsible artificial intelligence (AI). Our aim with this survey is to provide a clear and comprehensive introduction for newcomers while also offering seasoned researchers a valuable update in the field of LLM-generated text detection. The useful resources are publicly available at: https://github.com/NLP2CT/LLM-generated-Text-Detection.},
	urldate = {2024-08-29},
	publisher = {arXiv},
	author = {Wu, Junchao and Yang, Shu and Zhan, Runzhe and Yuan, Yulin and Wong, Derek F. and Chao, Lidia S.},
	month = apr,
	year = {2024},
	note = {arXiv:2310.14724 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\9TLMMYXV\\Wu et al. - 2024 - A Survey on LLM-Generated Text Detection Necessit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\D9XFC987\\2310.html:text/html},
}

@misc{workshop_bloom_2023,
	title = {{BLOOM}: {A} {176B}-{Parameter} {Open}-{Access} {Multilingual} {Language} {Model}},
	shorttitle = {{BLOOM}},
	url = {http://arxiv.org/abs/2211.05100},
	doi = {10.48550/arXiv.2211.05100},
	abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
	urldate = {2024-08-29},
	publisher = {arXiv},
	author = {Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ilić, Suzana and Hesslow, Daniel and Castagné, Roman and Luccioni, Alexandra Sasha and Yvon, François and Gallé, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Benoît and Muennighoff, Niklas and del Moral, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and McMillan-Major, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Laurençon, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and van Strien, Daniel and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo González and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni, Francesco and Dupont, Gérard and Kruszewski, Germán and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and Gonzalez-Dios, Itziar and de la Rosa, Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, Jörg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon and Phan, Long and allal, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Muñoz, Manuel Romero and Masoud, Maraim and Grandury, María and Šaško, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and de Gibert, Ona and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and López, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Taşar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M. Saiful and Al-shaibani, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and Ben-David, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and von Platen, Patrick and Cornette, Pierre and Lavallée, Pierre François and Lacroix, Rémi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, Stéphane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and Névéol, Aurélie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and van der Wal, Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zdeněk and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Muñoz and McDuff, Daniel and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and Miranda-Escalada, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Clémentine and Periñán, Daniel León and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and de Bykhovetz, Madeleine Hahn and Takeuchi, Maiko and Pàmies, Marc and Castillo, Maria A. and Nezhurina, Marianna and Sänger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S. and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and Sang-aroonsiri, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Théo and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas},
	month = jun,
	year = {2023},
	note = {arXiv:2211.05100 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\4AHMV4LZ\\Workshop et al. - 2023 - BLOOM A 176B-Parameter Open-Access Multilingual L.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\TPKA6HHY\\2211.html:text/html},
}

@misc{oh_frequency_2024,
	title = {Frequency {Explains} the {Inverse} {Correlation} of {Large} {Language} {Models}' {Size}, {Training} {Data} {Amount}, and {Surprisal}'s {Fit} to {Reading} {Times}},
	url = {http://arxiv.org/abs/2402.02255},
	doi = {10.48550/arXiv.2402.02255},
	abstract = {Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades. The current work presents a series of analyses showing that word frequency is a key explanatory factor underlying these two trends. First, residual errors from four language model families on four corpora show that the inverse correlation between model size and fit to reading times is the strongest on the subset of least frequent words, which is driven by excessively accurate predictions of larger model variants. Additionally, training dynamics reveal that during later training steps, all model variants learn to predict rare words and that larger model variants do so more accurately, which explains the detrimental effect of both training data amount and model size on fit to reading times. Finally, a feature attribution analysis demonstrates that larger model variants are able to accurately predict rare words based on both an effectively longer context window size as well as stronger local associations compared to smaller model variants. Taken together, these results indicate that Transformer-based language models' surprisal estimates diverge from human-like expectations due to the superhumanly complex associations they learn for predicting rare words.},
	urldate = {2024-08-30},
	publisher = {arXiv},
	author = {Oh, Byung-Doh and Yue, Shisen and Schuler, William},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02255 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\7AZEP9YY\\Oh et al. - 2024 - Frequency Explains the Inverse Correlation of Larg.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\65VXXWVV\\2402.html:text/html},
}

@misc{tay2022efficienttransformerssurvey,
      title={Efficient Transformers: A Survey}, 
      author={Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},
      year={2022},
      eprint={2009.06732},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2009.06732}, 
}

@article{Korochkina_2024,
  title         = {The Children and Young People’s Books Lexicon (CYP-LEX): A large-scale lexical database of books read by children and young people in the United Kingdom},
  author        = {Korochkina, Maria and Marelli, Marco and Brysbaert, Marc and Rastle, Kathleen},
  year          = {2024},
  month         = mar,
  journal       = {Quarterly Journal of Experimental Psychology},
  publisher     = {SAGE Publications},
  doi           = {10.1177/17470218241229694},
  issn          = {1747-0226},
  url           = {http://dx.doi.org/10.1177/17470218241229694}
}
@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@article{Dentella_2024,
  title         = {Testing AI on language comprehension tasks reveals insensitivity to underlying meaning},
  author        = {Dentella, Vittoria and Günther, Fritz and Murphy, Elliot and Marcus, Gary and Leivada, Evelina},
  year          = {2024},
  month         = nov,
  journal       = {Scientific Reports},
  publisher     = {Springer Science and Business Media LLC},
  volume        = {14},
  number        = {1},
  doi           = {10.1038/s41598-024-79531-8},
  issn          = {2045-2322},
  url           = {http://dx.doi.org/10.1038/s41598-024-79531-8}
}



@article{keuleers_word_2015,
	title = {Word knowledge in the crowd: {Measuring} vocabulary size and word prevalence in a massive online experiment},
	volume = {0},
	issn = {1747-0218},
	shorttitle = {Word knowledge in the crowd},
	url = {http://dx.doi.org/10.1080/17470218.2015.1022560},
	doi = {10.1080/17470218.2015.1022560},
	abstract = {We use the results of a large online experiment on word knowledge in Dutch to investigate variables influencing vocabulary size in a large population and to examine the effect of word prevalence—the percentage of a population knowing a word—as a measure of word occurrence. Nearly 300,000 participants were presented with about 70 word stimuli (selected from a list of 53,000 words) in an adapted lexical decision task. We identify age, education, and multilingualism as the most important factors influencing vocabulary size. The results suggest that the accumulation of vocabulary throughout life and in multiple languages mirrors the logarithmic growth of number of types with number of tokens observed in text corpora (Herdan's law). Moreover, the vocabulary that multilinguals acquire in related languages seems to increase their first language (L1) vocabulary size and outweighs the loss caused by decreased exposure to L1. In addition, we show that corpus word frequency and prevalence are complementary measures of word occurrence covering a broad range of language experiences. Prevalence is shown to be the strongest independent predictor of word processing times in the Dutch Lexicon Project, making it an important variable for psycholinguistic research.},
	number = {0},
	urldate = {2015-05-04},
	journal = {The Quarterly Journal of Experimental Psychology},
	author = {Keuleers, Emmanuel and Stevens, Michaël and Mandera, Paweł and Brysbaert, Marc},
	month = feb,
	year = {2015},
	pmid = {25715025},
	note = {00001 },
	pages = {1--28},
	file = {Keuleers et al_2015_Word knowledge in the crowd.pdf:C\:\\Users\\jobsc\\Zotero\\storage\\NGX99MEC\\Keuleers et al_2015_Word knowledge in the crowd.pdf:application/pdf;Snapshot:C\:\\Users\\jobsc\\Zotero\\storage\\AV35E7VK\\17470218.2015.html:text/html},
}



@misc{chiang_chatbot_2024,
	title = {Chatbot {Arena}: {An} {Open} {Platform} for {Evaluating} {LLMs} by {Human} {Preference}},
	shorttitle = {Chatbot {Arena}},
	url = {http://arxiv.org/abs/2403.04132},
	doi = {10.48550/arXiv.2403.04132},
	abstract = {Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at {\textbackslash}url\{https://chat.lmsys.org\}.},
	urldate = {2025-03-24},
	publisher = {arXiv},
	author = {Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E. and Stoica, Ion},
	month = mar,
	year = {2024},
	note = {arXiv:2403.04132 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\V29ZKW3T\\Chiang et al. - 2024 - Chatbot Arena An Open Platform for Evaluating LLMs by Human Preference.pdf:application/pdf;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\UG9QEXD6\\2403.html:text/html},
}

@misc{grattafiori_llama_2024,
	title = {The {Llama} 3 {Herd} of {Models}},
	url = {http://arxiv.org/abs/2407.21783},
	doi = {10.48550/arXiv.2407.21783},
	abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	urldate = {2025-05-09},
	publisher = {arXiv},
	author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Wyatt, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Guzmán, Francisco and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Thattai, Govind and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Zhang, Jack and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and Linde, Jelmer van der and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Prasad, Karthik and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Lakhotia, Kushal and Rantala-Yeary, Lauren and Maaten, Laurens van der and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and Oliveira, Luke de and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Tsimpoukelli, Maria and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Zhang, Ning and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Maheswari, Rohan and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Albiero, Vítor and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Wang, Xiaofang and Tan, Xiaoqing Ellen and Xia, Xide and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Srivastava, Aayushi and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Teo, Amos and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Dong, Annie and Franco, Annie and Goyal, Anuj and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and Paola, Beto De and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Liu, Ce and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Gao, Cynthia and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Le, Eric-Tuan and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Kokkinos, Filippos and Ozgenel, Firat and Caggioni, Francesco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Inan, Hakan and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Zhan, Hongyuan and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Leontiadis, Ilias and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Lam, Janice and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Jagadeesh, Kiran and Huang, Kun and Chawla, Kunal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Liu, Miao and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Mehta, Nikhil and Laptev, Nikolay Pavlovich and Dong, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Parthasarathy, Rangaprabhu and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Howes, Russ and Rinott, Ruty and Mehta, Sachin and Siby, Sachin and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Mahajan, Saurabh and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Patil, Shishir and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Deng, Summer and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Koehler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wu, Xiaojian and Wang, Xiaolan and Wu, Xilun and Gao, Xinbo and Kleinman, Yaniv and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Zhao, Yu and Hao, Yuchen and Qian, Yundi and Li, Yunlu and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei and Ma, Zhiyu},
	month = nov,
	year = {2024},
	note = {arXiv:2407.21783 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\Job Schepens\\Zotero\\storage\\E5ZAQFQT\\Grattafiori et al. - 2024 - The Llama 3 Herd of Models.pdf:application/pdf;Snapshot:C\:\\Users\\Job Schepens\\Zotero\\storage\\Z88P65GM\\2407.html:text/html},
}

@article{liu_deepseek-v3_2024,
	title = {Deepseek-v3 technical report},
	url = {https://arxiv.org/abs/2412.19437},
	urldate = {2025-05-09},
	journal = {arXiv preprint arXiv:2412.19437},
	author = {Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong},
	year = {2024},
}

@misc{fourrier_open_2024,
	title = {Open {LLM} {Leaderboard} v2},
	url = {https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard},
	publisher = {Hugging Face},
	author = {Fourrier, Clémentine and Habib, Nathan and Lozovskaya, Alina and Szafer, Konrad and Wolf, Thomas},
	year = {2024},
}


@article{korochkina_morphology_2025,
	title = {Morphology in children’s books, and what it means for learning},
	volume = {10},
	url = {https://www.nature.com/articles/s41539-025-00313-6},
	number = {1},
	urldate = {2025-05-16},
	journal = {npj Science of Learning},
	author = {Korochkina, Maria and Rastle, Kathleen},
	year = {2025},
	note = {Publisher: Nature Publishing Group UK London},
	pages = {22},
	file = {Available Version (via Google Scholar):C\:\\Users\\jobsc\\Zotero\\storage\\ASDL7IQZ\\s41539-025-00313-6.html:text/html},
}

@article{evanson_language_2023,
  title={Language acquisition: do children and language models follow similar learning stages?},
  author={Evanson, Linnea and Lakretz, Yair and King, Jean-R{\'e}mi},
  journal={arXiv preprint arXiv:2306.03586},
  year={2023}
}



@article{lopes_rego_language_2024,
	title = {Language models outperform cloze predictability in a cognitive model of reading},
	volume = {20},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1012117},
	number = {9},
	urldate = {2025-06-03},
	journal = {PLOS Computational Biology},
	author = {Lopes Rego, Adrielli Tina and Snell, Joshua and Meeter, Martijn},
	year = {2024},
	note = {Publisher: Public Library of Science San Francisco, CA USA},
	pages = {e1012117},
	file = {Available Version (via Google Scholar):C\:\\Users\\jobsc\\Zotero\\storage\\ZANP4SPI\\Lopes Rego et al. - 2024 - Language models outperform cloze predictability in a cognitive model of reading.pdf:application/pdf},
}

@inproceedings{botch_humans_2024,
	title = {Humans diverge from language models when predicting spoken language},
	url = {https://openreview.net/forum?id=DqKjKQtyna},
	abstract = {Humans communicate through both spoken and written language, often switching between these modalities depending on their goals. The recent success of large language models (LLMs) has driven researchers to understand the extent to which these models align with human behavior and neural representations of language. While prior work has shown similarities in how humans and LLMs form predictions of written text, no work has investigated whether LLMs are representative of human predictions of spoken language. We investigated the alignment between LLMs and behavior of human participants (N=300) who predicted words within a story presented as either spoken language or written text. We found that LLM predictions were more similar to humans' predictions of written text compared to spoken language, though humans' predictions of spoken language were the most accurate. Then, by training encoding models to predict neural activity recorded with fMRI to the same auditory story, we showed that models based on human predictions of spoken language better aligned with observed brain activity during listening compared to models based on LLM predictions. These findings suggest that the structure of spoken language carries additional information relevant to human behavior and neural representations.},
	language = {en},
	urldate = {2025-06-03},
	author = {Botch, Thomas L. and Finn, Emily S.},
	month = mar,
	year = {2024},
	file = {Full Text PDF:C\:\\Users\\jobsc\\Zotero\\storage\\IVSRARKW\\Botch and Finn - 2024 - Humans diverge from language models when predicting spoken language.pdf:application/pdf},
}

@misc{boeve_systematic_2024,
	title = {A {Systematic} {Evaluation} of {Dutch} {Large} {Language} {Models}’ {Surprisal} {Estimates} in {Sentence}, {Paragraph}, and {Book} {Reading}},
	url = {https://osf.io/vqnw6_v1},
	doi = {10.31219/osf.io/vqnw6},
	abstract = {Studies using computational estimates of word predictability from neural language models have garnered strong evidence in favour of surprisal theory. Upon encountering a word, readers experience a processing difficulty that is a linear function of that word’s surprisal. Evidence for this effect has been established in the English language or using multilingual models to estimate surprisal across languages. At the same time, many language-specific models of unknown psychometric quality are made openly available. Here, we provide a systematic evaluation of the surprisal estimates of a collection of large language models, specifically designed for Dutch, examining how well they account for reading times. We compare their performance to a multilingual model (mGPT) and an N-gram model. Across three eye-tracking corpora, a Dutch model predicted reading times better than the multilingual model. Dutch large language models replicate the general inverse scaling trend observed for English, with the surprisal estimates of smaller models showing a better fit to reading times than those of the largest models, however, this effect depends partly on the corpus used to evaluate the model. Surprisingly, in contrast to the linear effect of surprisal on reading times observed in other corpora, for the GECO corpus a non-linear link fitted the data best. Overall, these results offer a psychometric leaderboard of Dutch large language models and challenge the notion of a ubiquitous linear effect of surprisal. The complete set of surprisal estimates derived from all neural language models across the three corpora, along with the code to extract the surprisal, is made publicly available (https://osf.io/wr4qf/).},
	language = {en-us},
	urldate = {2025-06-03},
	publisher = {OSF},
	author = {Boeve, Sam and Bogaerts, Louisa},
	month = dec,
	year = {2024},
	keywords = {Dutch, eye movements, large language models, prediction, reading},
	file = {OSF Preprint:C\:\\Users\\jobsc\\Zotero\\storage\\5XPDPAE5\\Boeve and Bogaerts - 2024 - A Systematic Evaluation of Dutch Large Language Models’ Surprisal Estimates in Sentence, Paragraph,.pdf:application/pdf},
}

@article{heilbron_lexical_2023,
	title = {Lexical {Processing} {Strongly} {Affects} {Reading} {Times} {But} {Not} {Skipping} {During} {Natural} {Reading}},
	volume = {7},
	issn = {2470-2986},
	url = {https://doi.org/10.1162/opmi_a_00099},
	doi = {10.1162/opmi_a_00099},
	abstract = {In a typical text, readers look much longer at some words than at others, even skipping many altogether. Historically, researchers explained this variation via low-level visual or oculomotor factors, but today it is primarily explained via factors determining a word’s lexical processing ease, such as how well word identity can be predicted from context or discerned from parafoveal preview. While the existence of these effects is well established in controlled experiments, the relative importance of prediction, preview and low-level factors in natural reading remains unclear. Here, we address this question in three large naturalistic reading corpora (n = 104, 1.5 million words), using deep neural networks and Bayesian ideal observers to model linguistic prediction and parafoveal preview from moment to moment in natural reading. Strikingly, neither prediction nor preview was important for explaining word skipping—the vast majority of explained variation was explained by a simple oculomotor model, using just fixation position and word length. For reading times, by contrast, we found strong but independent contributions of prediction and preview, with effect sizes matching those from controlled experiments. Together, these results challenge dominant models of eye movements in reading, and instead support alternative models that describe skipping (but not reading times) as largely autonomous from word identification, and mostly determined by low-level oculomotor information.},
	urldate = {2025-06-03},
	journal = {Open Mind},
	author = {Heilbron, Micha and van Haren, Jorie and Hagoort, Peter and de Lange, Floris P.},
	month = oct,
	year = {2023},
	pages = {757--783},
	file = {Full Text PDF:C\:\\Users\\jobsc\\Zotero\\storage\\F8SW9HBM\\Heilbron et al. - 2023 - Lexical Processing Strongly Affects Reading Times But Not Skipping During Natural Reading.pdf:application/pdf;Snapshot:C\:\\Users\\jobsc\\Zotero\\storage\\XA3XW5BJ\\117729.html:text/html},
}


@article{caucheteux_brains_2022,
	title = {Brains and algorithms partially converge in natural language processing},
	volume = {5},
	copyright = {2022 The Author(s)},
	issn = {2399-3642},
	url = {https://www.nature.com/articles/s42003-022-03036-1},
	doi = {10.1038/s42003-022-03036-1},
	abstract = {Deep learning algorithms trained to predict masked words from large amount of text have recently been shown to generate activations similar to those of the human brain. However, what drives this similarity remains currently unknown. Here, we systematically compare a variety of deep language models to identify the computational principles that lead them to generate brain-like representations of sentences. Specifically, we analyze the brain responses to 400 isolated sentences in a large cohort of 102 subjects, each recorded for two hours with functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG). We then test where and when each of these algorithms maps onto the brain responses. Finally, we estimate how the architecture, training, and performance of these models independently account for the generation of brain-like representations. Our analyses reveal two main findings. First, the similarity between the algorithms and the brain primarily depends on their ability to predict words from context. Second, this similarity reveals the rise and maintenance of perceptual, lexical, and compositional representations within each cortical region. Overall, this study shows that modern language algorithms partially converge towards brain-like solutions, and thus delineates a promising path to unravel the foundations of natural language processing.},
	language = {en},
	number = {1},
	urldate = {2025-06-03},
	journal = {Communications Biology},
	author = {Caucheteux, Charlotte and King, Jean-Rémi},
	month = feb,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Language, Network models, Neural encoding},
	pages = {1--10},
	file = {Full Text PDF:C\:\\Users\\jobsc\\Zotero\\storage\\PZGHZ6NF\\Caucheteux and King - 2022 - Brains and algorithms partially converge in natural language processing.pdf:application/pdf},
}

@article{desbordes_dimensionality_2023,
	title = {Dimensionality and ramping: {Signatures} of sentence integration in the dynamics of brains and deep language models},
	volume = {43},
	shorttitle = {Dimensionality and ramping},
	url = {https://www.jneurosci.org/content/43/29/5350.abstract},
	number = {29},
	urldate = {2025-06-03},
	journal = {Journal of Neuroscience},
	author = {Desbordes, Théo and Lakretz, Yair and Chanoine, Valérie and Oquab, Maxime and Badier, Jean-Michel and Trébuchon, Agnès and Carron, Romain and Bénar, Christian-G. and Dehaene, Stanislas and King, Jean-Rémi},
	year = {2023},
	note = {Publisher: Society for Neuroscience},
	pages = {5350--5364},
	file = {Available Version (via Google Scholar):C\:\\Users\\jobsc\\Zotero\\storage\\2PFAAAQ2\\Desbordes et al. - 2023 - Dimensionality and ramping Signatures of sentence integration in the dynamics of brains and deep la.pdf:application/pdf},
}

@inproceedings{fresen_language_2024,
	title = {Language {Models} {That} {Accurately} {Represent} {Syntactic} {Structure} {Exhibit} {Higher} {Representational} {Similarity} {To} {Brain} {Activity}},
	volume = {46},
	url = {https://escholarship.org/uc/item/1fp7m6nf},
	urldate = {2025-06-03},
	booktitle = {Proceedings of the {Annual} {Meeting} of the {Cognitive} {Science} {Society}},
	author = {Fresen, Abraham Jacob and Choenni, Rochelle and Heilbron, Micha and Zuidema, Willem and de Heer Kloots, Marianne},
	year = {2024},
	file = {Available Version (via Google Scholar):C\:\\Users\\jobsc\\Zotero\\storage\\P3RPWTLT\\Fresen et al. - 2024 - Language Models That Accurately Represent Syntactic Structure Exhibit Higher Representational Simila.pdf:application/pdf},
}

@article{oh_dissociable_2025,
	title = {Dissociable frequency effects attenuate as large language model surprisal predictors improve},
	volume = {143},
	issn = {0749-596X},
	url = {https://www.sciencedirect.com/science/article/pii/S0749596X25000385},
	doi = {10.1016/j.jml.2025.104645},
	abstract = {Recent psycholinguistic modeling work using surprisal from Transformer-based language models has reported separable effects of frequency and predictability on real-time processing difficulty. However, it has also been shown that as Transformer-based language models become larger and are trained on more data, they are able to predict low-frequency words more accurately, which has a deleterious effect on fit to reading times. This article examines the impact of this property of language models on the dissociability of frequency effects and predictability effects in naturalistic reading. Regression results show robust positive effects of language model size and training data amount on the ability of word frequency to explain variance in held-out reading times as the contribution due to surprisal declines, which suggests a strong compensatory relationship between frequency and language model surprisal. Additionally, an analysis of the learning trajectories of low-frequency tokens reveals that the influence of model size is strongest on the prediction of tokens that are not part of a bigram sequence observed earlier in the context that models can readily copy, which suggests that limitations in model size create pressures toward learning more general associations. Taken together, these results suggest that the observed frequency effects may be due to imperfect estimates of predictability, and may disappear entirely as better-fitting language models are discovered. This further highlights the importance of exploring additional language models as models of human sentence processing.},
	urldate = {2025-06-03},
	journal = {Journal of Memory and Language},
	author = {Oh, Byung-Doh and Schuler, William},
	month = aug,
	year = {2025},
	keywords = {Computational modeling, Frequency, Large language models, Sentence processing, Surprisal},
	pages = {104645},
	file = {ScienceDirect Snapshot:C\:\\Users\\jobsc\\Zotero\\storage\\TL2B6A5T\\S0749596X25000385.html:text/html},
}

@article{oh_why_2023,
	title = {Why does surprisal from larger transformer-based language models provide a poorer fit to human reading times?},
	volume = {11},
	url = {https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a_00548/115371},
	urldate = {2025-06-03},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Oh, Byung-Doh and Schuler, William},
	year = {2023},
	note = {Publisher: MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA …},
	pages = {336--350},
	file = {Available Version (via Google Scholar):C\:\\Users\\jobsc\\Zotero\\storage\\T2IDH2HB\\115371.html:text/html},
}

@article{martinez2024ai,
  title={AI-generated estimates of familiarity, concreteness, valence, and arousal for over 100,000 Spanish words},
  author={Mart{\'\i}nez, Gonzalo and Conde, Javier and Reviriego, Pedro and Brysbaert, Marc},
  journal={Quarterly Journal of Experimental Psychology},
  pages={17470218241306694},
  year={2024},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{binz2024centaur,
  title={Centaur: a foundation model of human cognition},
  author={Binz, Marcel and Akata, Elif and Bethge, Matthias and Br{\"a}ndle, Franziska and Callaway, Fred and Coda-Forno, Julian and Dayan, Peter and Demircan, Can and Eckstein, Maria K and {\'E}ltet{\H{o}}, No{\'e}mi and others},
  journal={arXiv preprint arXiv:2410.20268},
  year={2024}
}

@article{meixner2022perceptual,
  title={The perceptual span is dynamically adjusted in response to foveal load by beginning readers.},
  author={Meixner, Johannes M and Nixon, Jessie S and Laubrock, Jochen},
  journal={Journal of Experimental Psychology: General},
  volume={151},
  number={6},
  pages={1219},
  year={2022},
  publisher={American Psychological Association}
}


@misc{rystrøm2025multilingualmulticulturalevaluating,
      title={Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs}, 
      author={Jonathan Rystrøm and Hannah Rose Kirk and Scott Hale},
      year={2025},
      eprint={2502.16534},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.16534}, 
}