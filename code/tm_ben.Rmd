---
title: "Correlation between frequency lists"
output: html_document
---

```{r setup, include=FALSE}
# written and run in R 4.2.2
library(ggplot2)
my_theme = theme(
  title = element_text(size = rel(.75)),
	axis.title.x = element_text(size = rel(.9)),
  axis.title.y = element_text(size = rel(.9)),
  axis.text.x = element_text(size = rel(.7)),
  axis.text.y = element_text(size = rel(.7)),
  # legend.position = "none",
  panel.background = element_rect(fill = "white"),
  legend.background = element_rect(fill = "white"),
  panel.grid.minor = element_line(color = "#B8B8B8",
                                linewidth = 0.1,
                                linetype = 1), 
  panel.grid.major = element_line(color = "#B8B8B8",
                                linewidth = 0.3,
                                linetype = 1))

MIN_FREQUENCY = 0
MIN_FREQUENCY_JOINED = 0
MIN_FREQUENCY_LABEL = 17
MIN_FREQUENCY_LABEL = 5
MAX_FREQUENCY = 70001 # for plotting
knitr::opts_chunk$set(echo = TRUE)
# readRenviron("../RProfile.site")
# Sys.setenv(openai_secret_key = Sys.getenv("OPENAI_API_KEY"))
# install.packages("tm")
# install.packages("readtext")
# install.packages("SnowballC")
# install.packages("ggrepel")
# install.packages("textstem")
# install.packages("ggpointdensity")
# install.packages("ks")
# install.koRpus.lang("de")
# library("koRpus.lang.de")
# library(textstem)
library(ggrepel)
library(tm)
library(readtext)
library(SnowballC)
library(tidyverse)
library(scales)
library(lexicon)
library(viridis)
library(ggpointdensity)
library(ks)
```


## Load LLM frequency list

```{r}
getFreqMat <- function(df, opts = list()) {
	corpus <- Corpus(VectorSource(df$text))
	tdm <- as.matrix(TermDocumentMatrix(corpus, control = opts))
	feqmat <- data.frame(type = rownames(tdm), 
											 ai_freq = rowSums(tdm), 
											 row.names = NULL)
	# x <- lexicon::freq_first_names %>% slice_max(prop, n = 4000)
	# feqmat <- feqmat[!(feqmat$type %in% x$Name),]
}

opts <- list(removeNumbers = T,
						removePunctuation = T,
						# preserve_intra_word_contractions = TRUE,
						# preserve_intra_word_dashes = TRUE,
						stopwords = F,
						stemming = FALSE, 
						tolower=FALSE,
						wordLengths = c(2, Inf))

### LEMMA OR TYPES 
conditions = c("adlt", "adht", "chlt", "chht")
ftables <- list()

# start for loop
for(cond in conditions) {
print(cond)

# cond = "adlt"
# cond = "adht"
# cond = "chlt"
# cond = "chht"

wordtype = "types"
# wordtype = "lemma"

if (wordtype == "types") {
	# df_l <- readtext::readtext("data-original/gpt3_5_texts/gen_texts*/*.txt")
	# df_l <- readtext::readtext("data-original/exp2/adht/*.txt")
	# df_l <- readtext::readtext("data-original/exp2/adlt/*.txt")
	df_l <- readtext::readtext(paste0("data-original/exp2/", cond, "/*.txt"))
	# save(df_l, file = "data-processed/corpus-3.5-PUNC.RData") # used for udpipe
	# save(df_l, file = "data-processed/corpus-adlt.RData") # used for udpipe
	# save(df_l, file = "data-processed/corpus-adht.RData") # used for udpipe
	save(df_l, file = paste0("data-processed/corpus-", cond, ".RData")) # used for udpipe
	# load("data-processed/corpus-3.5.RData", verbose = T)
	freqmat_l <- getFreqMat(df_l, opts) # types
} else {
	# load("data-processed/tidycorpus.RData") # words on lines
	# load("data-processed/tidycorpus-text.RData", verbose = T) # texts on lines
	# load("data-processed/tidycorpus-text-3.5.RData", verbose = T) # texts on lines
	# load("data-processed/tidycorpus-text-3.5-ch.RData", verbose = T) # texts on lines (udpipe)
	# load("data-processed/tidycorpus-text-adlt.RData", verbose = T) # texts on lines (udpipe)
	# load("data-processed/tidycorpus-text-adht.RData", verbose = T) # texts on lines (udpipe)
	load(paste0("data-processed/tidycorpus-text-", cond, ".RData"), verbose = T) # texts on lines (udpipe)
	freqmat_l <- getFreqMat(text_df2, opts) # lemma
	# save(freqmat_l, file = "data-processed/freqmat_l_lemma.RData")
}

# apply normalisation like in childLex
# freqmat_l$ai_norm <- 1000000 * freqmat_l$ai_freq / (sum(freqmat_l$ai_freq))
freqmat_l$ai_normplus1 <- 1000000 * (freqmat_l$ai_freq + 1) / (sum(freqmat_l$ai_freq))

# freqmat_l$ai_norm_log <- log(freqmat_l$ai_norm)
freqmat_l$ai_norm_log <- log(freqmat_l$ai_normplus1)

# freqmat_l <- freqmat_l %>% filter(ai_norm > MIN_FREQUENCY)
freqmat_l <- freqmat_l %>% arrange(desc(ai_norm))

# save(freqmat_l, file = "data-processed/freqmat_l_lemma-3.5.RData") # used for table
# save(freqmat_l, file = "data-processed/freqmat_l_lemma-adlt.RData") # used for table
# save(freqmat_l, file = "data-processed/freqmat_l_lemma-adht.RData") # used for table

# save(freqmat_l, file = "data-processed/freqmat_l-types-3.5-PUNC.RData") # used for table
# save(freqmat_l, file = "data-processed/freqmat_l-types-adlt.RData") # used for table
# save(freqmat_l, file = "data-processed/freqmat_l-types-adht.RData") # used for table

save(freqmat_l, file = paste0("data-processed/freqmat_l-", wordtype, "-", cond, ".RData")) # used for table

# load(file = "data-processed/freqmat_l-types-3.5.RData", verbose = T)
# load(file = "data-processed/freqmat_l-types-3.5-PUNC.RData", verbose = T)
# load(file = "data-processed/freqmat_l-types-adht.RData", verbose = T)

# freqmat_l$cond <- cond
ftables[[cond]] <- freqmat_l # trees with head annotation

# end for loop
}

# word	count	frequency_per_million	log_frequency_per_million

ftables_with_source <- Map(function(tbl, cond) {
    tbl$Source <- cond  # Add a new column with the condition as the source identifier
    return(tbl)
}, ftables, conditions)

for(cond in conditions) {
ftables[[cond]] %>% mutate(ai_norm = round(ai_norm, digits = 2),
										ai_norm_log = round(ai_norm_log, digits = 4)) %>%
	rename(word = type,
										count = ai_freq,
										frequency_per_million = ai_norm,
										log_frequency_per_million = ai_norm_log) %>% 
	write.csv(paste0("data-processed/", cond,".csv"), row.names = F)
}

all_data <- do.call(rbind, ftables_with_source)
all_data %>% mutate(ai_norm = round(ai_norm, digits = 2),
										ai_norm_log = round(ai_norm_log, digits = 4)) %>%
	rename(word = type,
										count = ai_freq,
										frequency_per_million = ai_norm,
										log_frequency_per_million = ai_norm_log) %>% 
	write.csv("data-processed/all_data.csv", row.names = F)
```

```{r, eval=FALSE}
### TESTING ###
freqmat_l[freqmat_l$type == "So",]
freqmat_l[freqmat_l$type == "so",]
freqmat_l <- freqmat_l %>% 
	mutate(word_length = nchar(type)) %>%
	arrange(desc(word_length)) %>%
	filter(word_length == 3)
freqmat_l[freqmat_l$type == "Oma",]
library(lexicon)
freq_first_names <- lexicon::freq_first_names
lexicon::freq_first_names[lexicon::freq_first_names$Name == "Man"]
lexicon::freq_first_names[lexicon::freq_first_names$Name == "Art"]
head(freqmat_l[order(freqmat_l$ai_freq, decreasing = T),], 10)
head(freqmat_l[order(freqmat_l$ai_freq, decreasing = F),], 10)
```


## Merge and load human frequency list 

```{r}
library(readxl)
getChildLex <- function(sheet = 1){
	childLex <- read_xlsx(xlsx, sheet = sheet, range = cell_cols("A:I"))
	childLexAgg <- childLex %>% 
		summarise(.by = type,
		# summarise(.by = lemma,
							# childlex_norm = lemma.norm[1],					
							# childlex_norm = type.norm[1],
							childlex_norm = sum(atype.norm),
							n_pos = n())
	MIN_FREQUENCY = 0
	childLexAgg <- childLexAgg %>% filter(childlex_norm > MIN_FREQUENCY)
	# childLexAgg <- rename(childLexAgg, type = lemma)
}

getChildLexLemma <- function(sheet = 1){
	childLex <- read_xlsx(xlsx, sheet = sheet, range = cell_cols("A:I"))
	childLexAgg <- childLex %>% 
		# summarise(.by = type,
		summarise(.by = lemma,
							childlex_norm = lemma.norm[1],
							# childlex_norm = type.norm[1],
							# childlex_norm = sum(atype.norm),
							n_pos = n())
	childLexAgg <- childLexAgg %>% filter(childlex_norm > MIN_FREQUENCY)
	# childLexAgg <- rename(childLexAgg, type = lemma)
}

MIN_FREQUENCY = 0
xlsx = "data-original/childLex/childLex_0.17.01c_2018-12-24_schr.xlsx"
# read_xlsx(xlsx, sheet = 1, range = cell_cols("A:I"))
# https://drive.google.com/open?id=1OKdYErf1q3RzNXx_WyOnA4jq3uZqu6l_
wordtype = "type"
if (wordtype == "type") { # 164k in excel file and 182k in ms table?
	childLexAgg <- getChildLex(excel_sheets(xlsx)[1]) # 158,398
	childLexAgg6 <- getChildLex(excel_sheets(xlsx)[2])
	childLexAgg9 <- getChildLex(excel_sheets(xlsx)[3])
	childLexAgg11 <- getChildLex(excel_sheets(xlsx)[4])
} else {
	childLexAgg <- getChildLexLemma(excel_sheets(xlsx)[1]) # 96204
	childLexAgg6 <- getChildLexLemma(excel_sheets(xlsx)[2])
	childLexAgg9 <- getChildLexLemma(excel_sheets(xlsx)[3])
	childLexAgg11 <- getChildLexLemma(excel_sheets(xlsx)[4])
}
# save(childLexAgg, file = "data-processed/childLexAgg.RData")

# childLexAgg[childLexAgg$type == "hatte",]
# childLexAgg[childLexAgg$type == "Offenbar",]
# childLexAgg[childLexAgg$type == "offenbar",]


### LITKEY ###
# head(litkey2)
# litkey[litkey$target == "beginnen",]
# freqmat_l[freqmat_l$type == "beginnen",]
# merged_lit[merged_lit$target == "beginnen",]
# litkeya[litkeya$target == "beschlossen", ]
load("data-processed/litkey.RData", verbose = T)
litkey <- litkey %>% distinct()

# load(file = "data-processed/freqmat_l-types-3.5.RData", verbose = T)
# load(file = "data-processed/freqmat_l-types-3.5-PUNC.RData", verbose = T)

conditions = c("adlt", "adht", "chlt", "chht")
for(cond in conditions) {
	print(cond)
	load(file = paste0("data-processed/freqmat_l-types-", cond, ".RData"), verbose = T)
	
	wordtype = "type"
	if (wordtype == "type") {
		merged_lit <- inner_join(litkey, freqmat_l, join_by(target == type))
		merged_litx <- full_join(litkey, freqmat_l, join_by(target == type))
		merged_dfx <- full_join(childLexAgg, freqmat_l, join_by(type == type)) # used for plotting
		# merged_df <- inner_join(childLexAgg, freqmat_l, join_by(type == type))
		# merged_df_ch <- left_join(childLexAgg, freqmat_l, join_by(type == type))
		merged_df6 <- inner_join(childLexAgg6, freqmat_l, join_by(type == type))
		merged_df9 <- inner_join(childLexAgg9, freqmat_l, join_by(type == type))
		merged_df11 <- inner_join(childLexAgg11, freqmat_l, join_by(type == type))
		merged_lit_ch <- inner_join(merged_lit, childLexAgg, join_by(target == type))
		# save(merged_df,    file = "data-processed/merged_df-type-3.5.RData")
		# save(merged_df_ch, file = "data-processed/merged_df-type-3.5-ch.RData")
		# save(merged_dfx,   file = "data-processed/merged_df-type-3.5-full-NAMES.RData") # old file that experimented with excluding names
		# save(merged_dfx,   file = "data-processed/merged_df-type-3.5-full.RData") # used for plotting
		save(merged_dfx,   file = paste0("data-processed/merged_df-type-", cond, ".RData")) # used for plotting
		
		# save(merged_litx, file = "data-processed/merged_lit-type-3.5-full.RData")
		# save(merged_litx, file = "data-processed/merged_lit-type-3.5-full-NAMES.RData")
		# save(merged_lit,  file = "data-processed/merged_lit-type.RData")
		# save(merged_lit,  file = "data-processed/merged_lit-type-3.5.RData")
	} else {
		load("data-processed/litkey-lemma.RData", verbose = T)
		merged_lit <- inner_join(litkey2, freqmat_l, join_by(chl_lemma == type))
		merged_df <- inner_join(childLexAgg, freqmat_l, join_by(lemma == type))
		merged_df6 <- inner_join(childLexAgg6, freqmat_l, join_by(lemma == type))
		merged_df9 <- inner_join(childLexAgg9, freqmat_l, join_by(lemma == type))
		merged_df11 <- inner_join(childLexAgg11, freqmat_l, join_by(lemma == type))
		merged_lit_ch <- inner_join(merged_lit, childLexAgg, join_by(chl_lemma == lemma))
		# save(merged_df, file = "data-processed/merged_df-lemma.RData")
	}
}

# merged_lit <- inner_join(litkey2, freqmat_l, join_by(chl_lemma == type))
# merged_lit[merged_lit$target == "beschlossen",]
```


## Results

```{r}
# out.width # how big it is in html
# dpi # does not change html display but figure is double the size
# fig.width # how much space the figure uses

# as saved above 
load(file = "data-processed/merged_df-type-3.5-full.RData", verbose = T)

# data from cormat.R
load("data-processed/all_wider_again_subt_ch_plus1_prob.RData", verbose = T)

aap <- t(all_wider_againl %>% filter(merged_type == "namens"))

merged_df <- merged_dfx # 172747
summary(merged_df)

fromexp2 <- all_wider_againl %>% # 371415
	select(merged_type, 
				 value_ai_freq, value_chcount, 
				 norm_ai_freq, norm_chcount, 
				 norm_logcountplus1_ai_freq, norm_logcountplus1_chcount) %>%
	rename(
		childlex_norm = norm_chcount,
		ai_norm = norm_ai_freq)

merged_df <- fromexp2 %>%  # 172747
	filter(!rowSums(select(., starts_with("value_")) == 0) == length(select(., starts_with("value_"))))

fromexp2 <- all_wider_againl %>% # 371415
	select(merged_type, 
				 
				 # value_ai_freq, 
				 value_chcount, 
				 value_adht, value_adlt,
				 value_chht, value_chlt,
				 
				 norm_ai_freq, norm_chcount, 
				 
				 norm_logcountplus1_ai_freq, norm_logcountplus1_chcount,
				 norm_logcountplus1_adht, norm_logcountplus1_adlt,
				 norm_logcountplus1_chht, norm_logcountplus1_chlt) %>%
	rename(
		childlex_norm = norm_chcount,
		ai_norm = norm_ai_freq)

forexp2 <- fromexp2 %>%  # 172747
	filter(!rowSums(select(., starts_with("value_")) == 0) == length(select(., starts_with("value_"))))

save(merged_df, file = "basedonexp2.RData")
save(forexp2, file = "forexp2.RData")

MIN_FREQUENCY_COR <- 0

# childlex-llm
merged_df %>% 
	filter(childlex_norm > MIN_FREQUENCY_COR) %>%
	filter(ai_norm > MIN_FREQUENCY_COR) %>% 
	with(cor(childlex_norm, ai_norm, use = "complete.obs"))
# .88 in text

# litkey - childlex
# colnames(merged_lit)
merged_lit %>% 
	filter(litkey_norm_childlex > MIN_FREQUENCY_COR) %>%
	filter(litkey_norm_children > MIN_FREQUENCY_COR) %>% 
	with(cor(litkey_norm_childlex, litkey_norm_children, use = "complete.obs"))

# litkey-llm
merged_lit %>% 
	filter(litkey_norm_children > MIN_FREQUENCY_COR) %>%
	filter(ai_norm > MIN_FREQUENCY_COR) %>% 
	with(cor(litkey_norm_children, ai_norm, use = "complete.obs"))

# childlex-llm
merged_df %>% 
	filter(childlex_norm > MIN_FREQUENCY_COR) %>%
	filter(ai_norm > MIN_FREQUENCY_COR) %>% 
	with(cor(childlex_norm, ai_norm, use = "complete.obs"))


# load("data-processed/merged_df-type-3.5-full-NAMES.RData", verbose = T)
# load("data-processed/merged_df-type-3.5-full.RData", verbose = T)
# merged_df <- merged_dfx

# merged_df[merged_df$type == "Oma",]

# merged_df$ai_norm[is.na(merged_df$ai_norm)] = .1
# merged_df$childlex_norm[is.na(merged_df$childlex_norm)] = .1
cor.test(merged_df$childlex_norm, merged_df$ai_norm) # .88
# cor.test(log(merged_df$childlex_norm), log(merged_df$ai_norm)) # .59 172747 rows WITH NAMES AND WITH PUNC
# cor.test(log(merged_df$childlex_norm), log(merged_df$ai_norm)) # .59 173253 rows NO NAMES WITH PUNC
# cor.test(log(merged_df$childlex_norm), log(merged_df$ai_norm)) # .33 221267 rows NO NAMES NO PUNC
cor.test(merged_df$norm_logcountplus1_chcount, merged_df$norm_logcountplus1_ai_freq) # .59

# load("data-processed/merged_df-type-3.5.RData", verbose = T)

cor.test(merged_df$childlex_norm, merged_df$ai_norm) # .88
# cor.test(log(merged_df$childlex_norm), log(merged_df$ai_norm)) #.67, 31410 rows
# x <- read_csv("data-processed/job_cor_comp_df.csv") # 1152 rows
```

```{r}
# Correlations depending on subsets of frequency list

# subset_sizes <- seq(100, nrow(merged_df), by = 10)
subset_sizes <- seq(100, nrow(merged_df), length.out = 400)
length(subset_sizes)
subset_sizes <- round(subset_sizes, digits = 0)
cor_coeffs <- map_dbl(subset_sizes, function(x) {
  merged_df %>%
    sample_n(x, replace = F) %>% 
    with(cor(childlex_norm, ai_norm, use = "complete.obs"))
})
p <- data.frame(subset_sizes, cor_coeffs) %>%
  ggplot(aes(x = subset_sizes, y = cor_coeffs)) +
	geom_line(linewidth = .1, alpha = .8) +
	geom_smooth(method = "loess", span = 0.1, se = F, linewidth = .5) + 
	scale_x_continuous(name = "Number of types") +
	scale_y_continuous(name = "Correlation Coefficient", 
		# breaks = c(1, 5, 10, 50, 100, 500, 1000, 5000), 
		#	labels = scales::label_number(accuracy = 1), 
		limits = c(0, 1), # word lemmas
		# limits = c(.7, 1), # word types
		expand = c(0, 0)) + 
	my_theme
p
# ggsave(p, filename = "figures/sample-cor-F-3.5.pdf", scale = 1, width = 4, height = 3)
# ggsave(p, filename = "figures/sample-cor-F-lemma.pdf", scale = 1, width = 4, height = 3)
```

```{r}
# Correlations depending on subsets of texts

# run cor_sim.R
# load("data-processed/cors_sim_many.RData", verbose = T)
# load("data-processed/cors_sim_many-3.5.RData", verbose = T)
# load("data-processed/cors_sim_many-3.5-logs.RData", verbose = T)
load("data-processed/cors_sim_many-3.5-logs-full.RData", verbose = T)
ggplot(cors_sim, aes(x = subset_sizes, y = cor_coeffs)) +
	geom_line(linewidth = .1, alpha = .8) +
	geom_smooth(method = "loess", span = 0.1, se = F, linewidth = .5) + 
	xlab("Number of texts") +
	scale_y_continuous(name = "Correlation Coefficient", 
		limits = c(0, 1), # word lemmas
		expand = c(0, 0)) + 
	my_theme
# ggsave(filename = "figures/sample-cor-T-3.5-logs.pdf", scale = 1, width = 4, height = 3)
# ggsave(filename = "figures/sample-cor-T-3.5.pdf", scale = 1, width = 4, height = 3)
# ggsave(filename = "figures/sample-cor-T-lemma.pdf", scale = 1, width = 4, height = 3)
```


```{r, out.width = "100%", fig.width=7, dpi = 300, echo=T, eval = T}
x_label <- "ChildLex-based (6-12 y/o children) type frequency (ppm)"
y_label <- "LLM-based type frequency (ppm)"

source("r-code/scatterPlots.R")
# load("data-processed/merged_df-type-3.5-full.RData", verbose = T)
# load(file = "all_wider_again.csv", verbose = T)
# load("basedonexp2.RData", verbose = T)
load("forexp2.RData", verbose = T)

colnames(forexp2)
all_longer_again <- forexp2 %>% 
	select(c(norm_logcountplus1_chcount, 
					 norm_logcountplus1_adht,
					 norm_logcountplus1_adlt, 
					 norm_logcountplus1_chht, 
					 norm_logcountplus1_chlt, 
					 merged_type)) %>%
	pivot_longer(cols = c(norm_logcountplus1_adht,
												norm_logcountplus1_adlt, 
												norm_logcountplus1_chht, 
												norm_logcountplus1_chlt), 
							names_to = "corpus", values_to = "norm_logcountplus1_ai")
# all_longer_again
a <- all_longer_again %>% rename("type" = merged_type, "x" = norm_logcountplus1_chcount, "y" = norm_logcountplus1_ai)

# a <- a[1:1000,]
# summary(a)
p <- scatterPlotFacets(a, x_label, y_label, datap = 50000)
# ggsave(p, filename = paste("figures/scatterplotfacets.pdf", sep = ""), 
			 # scale = 1, width = 3.5, height = 4)
ggsave(p, filename = paste("figures/scatterplotfacets.pdf", sep = ""), 
			 scale = 1, width = 9, height = 9) 

b <- a %>% filter(x > 1000 & y > 1000)
b <- a %>% filter(x > 5 & y > 5)
p <- scatterPlotFacetsZoom(b , x_label, y_label, datap = 50000)
# ggsave(p, filename = paste("figures/scatterplotfacets.pdf", sep = ""), 
			 # scale = 1, width = 3.5, height = 4) # final
ggsave(p, filename = paste("figures/scatterplotfacetszoom.pdf", sep = ""), 
			 scale = 1, width = 9, height = 9) # final

load("basedonexp2.RData", verbose = T)
tibble(merged_df)

a <- merged_df %>% dplyr::select(merged_type, norm_logcountplus1_chcount, norm_logcountplus1_ai_freq)
p <- scatterPlot(a %>% rename("type" = merged_type, 
															"x" = norm_logcountplus1_chcount, 
															"y" = norm_logcountplus1_ai_freq), 
								 x_label, y_label, datap = 50000)
p
ggsave(p, filename = paste("figures/scatterplotexp1.pdf", sep = ""), scale = 1, width = 3.5, height = 4)
# ggsave(p, filename = paste("figures/for 6-12ChildLexLLM.pdf", sep = ""), scale = 1, width = 4, height = 4)
# ggsave(p, filename = paste("figures/for 6-12ChildLexLLMlemma.pdf", sep = ""), scale = 1, width = 4, height = 4)
# ggsave(p, filename = paste("figures/for 6-12ChildLexLLM-3.5.pdf", sep = ""), scale = 1, width = 4, height = 4)
# ggsave(p, filename = paste("figures/for 6-12ChildLexLLM-3.5-ch.pdf", sep = ""), scale = 1, width = 4, height = 4)
# ggsave(p, filename = paste("figures/for 6-12ChildLexLLM-3.5-ch-test-3.pdf", sep = ""), scale = 1, width = 3.5, height = 4) # final
 
# scatterPlot(merged_df6 %>% select(type, childlex_norm, ai_norm) %>% 
# 							rename("type" = 1, "x" = 2, "y" = 3), 
# 						"ChildLex-based (6-8 y/o children) type frequency",
# 						"LLM-based type frequency (ppm)")
# scatterPlot(merged_df9 %>% select(type, childlex_norm, ai_norm) %>% 
# 							rename("type" = 1, "x" = 2, "y" = 3), 
# 						"ChildLex-based (9-10 y/o children) type frequency", 
# 						"LLM-based type frequency (ppm)")
# scatterPlot(merged_df11 %>% select(type, childlex_norm, ai_norm) %>% 
# 							rename("type" = 1, "x" = 2, "y" = 3), 
# 						"ChildLex-based (11-12 y/o children) type frequency", 
# 						"LLM-based type frequency (ppm)")

# scatterPlot(merged_lit %>% select(target, litkey_norm_childlex, litkey_norm_children) %>% 
# 							rename("type" = 1, "x" = 2, "y" = 3), 
# 						"ChildLex-based (6-12 y/o children) type frequency",  
# 						"Litkey-based type frequency")
```






```{r}
# litkey
# load("data-processed/merged_lit-type.RData", verbose = T)
# load("data-processed/merged_lit-type-3.5.RData", verbose = T)
# load("data-processed/merged_lit-type-3.5-full-NAMES.RData", verbose = T)
load("data-processed/merged_lit-type-3.5-full.RData", verbose = T)

merged_df <- merged_litx
tibble(merged_df)
summary(merged_df)
sum(merged_df$litkey_abs, na.rm = T)
sum(merged_df$ai_freq, na.rm = T)

a <- merged_df %>% 
	mutate(across(c(litkey_abs, ai_freq), ~replace_na(., 0))) %>%
	mutate(normplus1_litkey = 1000000 * (litkey_abs + 1) / (sum(litkey_abs))) %>%
	mutate(norm_logcountplus1_litkey = log(normplus1_litkey)) %>% 
	mutate(normplus1_ai_freq = 1000000 * (ai_freq + 1) / (sum(ai_freq))) %>%
	mutate(norm_logcountplus1_ai_freq = log(normplus1_ai_freq))
summary(a)

p <- scatterPlot(a %>% 
								 	dplyr::select(target, norm_logcountplus1_litkey, norm_logcountplus1_ai_freq) %>%
							rename("type" = target, "x" = norm_logcountplus1_litkey, "y" = norm_logcountplus1_ai_freq), 
							"Litkey-based type frequency (ppm)", 
							"LLM-based type frequency (ppm)",
							datap = 50) # for scale fill viridis
p
# ggsave(filename = paste("figures/litkeyLLM.pdf", sep = ""), scale = 1, width = 4, height = 4)
# ggsave(filename = paste("figures/litkeyLLM.pdf", sep = ""), scale = 1, width = 4, height = 4)
# ggsave(filename = paste("figures/litkeyLLMlemma.pdf", sep = ""), scale = 1, width = 4, height = 4)
# ggsave(filename = paste("figures/litkeyLLM-3.5.pdf", sep = ""), scale = 1, width = 4, height = 4)
# ggsave(filename = paste("figures/litkeyLLM-3.5-test-3.pdf", sep = ""), scale = 1, width = 3.5, height = 4) # final
ggsave(p, filename = paste("figures/litkey.pdf", sep = ""), scale = 1, width = 3.5, height = 4) # final
```



# todo 

```{r, include=FALSE, eval = F}
# 1 WASTE (Jurish & WÃ¼rzner, 2013) 
# 2 TAGH (Geyken & Hanneforth, 2006) 
# 3 PoS tagger moot (Jurish, 2003
dic <- make_lemma_dictionary(corpus,
												 engine = "treetagger", # hunspell, lexicon
												 path = p,
												 debug=TRUE,
												 lang = "de")
library(tm)
library(textstem) # using the lemmas in the lexicon package
corpus <- tm_map(corpus, textstem::lemmatize_strings)

p <- "C:/Users/Job Schepens/Downloads/tree-tagger-windows-3.2.3/TreeTagger"

# load("hallu.RData")
# y <- hallu %>% slice_max(ai_norm, n = 100)
# corpus <- tm_map(corpus, removeWords, y$type)
```
